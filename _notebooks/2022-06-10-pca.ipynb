{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20623fed",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "> Unsupervised feature extraction:주성분분석\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: pinkocto\n",
    "- categories: [python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2d737",
   "metadata": {},
   "source": [
    "## 고차원 데이터\n",
    "\n",
    "<img src = \"./my_icons/high_dim_data.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ced7f",
   "metadata": {},
   "source": [
    "- 변수의 수가 많은 $\\to$ 불필요한 변수 존재\n",
    "- 시각적으로 표현하기 어려움\n",
    "- 계산 복잡도 증가 $\\to$ 모델링 비효율적\n",
    "- 중요한 변수만을 선택 $\\to$ 차원축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ec9dc",
   "metadata": {},
   "source": [
    "## 변수 선택 / 추출을 통한 차원축소\n",
    "\n",
    "### 변수선택 (selection) : \n",
    "> 분석 목적에 부합하는 소수의 예측변수만을 선택\n",
    "- 장점: 선택한 변수 해석 용이\n",
    "- 단점: 변수간 상관관계 고려 어려움\n",
    "\n",
    "### 변수추출 (extraction) : \n",
    "> 예측변수의 변환을 통해 새로운 변수 추출\n",
    "- 장점: 변수 간 상관관계 고려, 일반적으로 변수의 개수를 많이 줄일 수 있음\n",
    "- 단점: 추출된 변수의 해석이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3098ca8",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/feature_select.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58831b1d",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Supervised</font> <font color='red'>feature selection</font>: \n",
    "Information gain, Stepwise regression, LASSO, Genetic Algorithm, $\\dots$\n",
    "#### <font color='blue'>Supervised</font> <font color='green'>feature extraction</font>:\n",
    "Partial least squares (PLS)\n",
    "#### <font color='purple'>Unsupervised</font> <font color='red'>feature selection</font>: \n",
    "PCA loading\n",
    "#### <font color='purple'>Unsupervised</font> <font color='green'>feature extraction:</font> \n",
    "$Y$를 이용하지 않고 $X$들의 결합으로 변수를 추출하는 방법<br>\n",
    "Pincipal Component Analysis (PCA), Wavelets transformms, Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689df3d",
   "metadata": {},
   "source": [
    "## PCA 개요\n",
    "- 고차원 데이터를 효과적으로 분석하기 위한 대표적 분석 기법\n",
    "- 차원축소, 시각화, 군집화, 압축\n",
    "- PCA는 $n$개의 관측치와 $p$개의 변수로 구성된 데이터를 상관관계가 없는 $k$개의 변수로 구성된 데이터 ($n$개의 관측치)로 요약하는 방식으로, 이 때 요약된 변수는 기존 변수의 선형조합으로 생성됨.\n",
    "\n",
    "- 원래 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영 ( Projection) 시키는 기법\n",
    "\n",
    "- 주요 목적\n",
    "    - 데이터 차원 축소 ($n \\times p \\to n\\times k, \\space where \\space k << p)$\n",
    "    - 데이터 시각화 및 해석\n",
    "    \n",
    "- 일반적으로 PCA는 전체 분석 과정 충 초기에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600575d6",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/pca_outline.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4936b",
   "metadata": {},
   "source": [
    "$Z_1,Z_2, Z_3$은 기존 변수인 $X_1, X_2, \\dots X_p$의 선형 조합으로 새롭게 생성된 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c201fa",
   "metadata": {},
   "source": [
    "$Z$ is linear combination (선형결합) of the original $p$ variables in $X$\n",
    "\n",
    "$$Z_1 = a_1^TX = a_{11}X_1 + a_{12}X_2 + \\dots a_{1p}X_p$$\n",
    "$$Z_2 = a_2^TX = a_{21}X_1 + a_{22}X_2 + \\dots a_{2p}X_p$$\n",
    "$$\\vdots$$\n",
    "\n",
    "$$Z_p = a_p^TX = a_{p1}X_1 + a_{p2}X_2 + \\dots a_{pp}X_p$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71113345",
   "metadata": {},
   "source": [
    "- $X_1, X_2, \\dots, X_p$: 원래 변수 (original variable)\n",
    "- $a_i = [a_{i1}, a_{i2}, \\dots, a_{ip}]$, $i$번째 기저(basis) 또는 계수(Loading)\n",
    "- $Z_1, Z_2, \\dots, Z_p$ 각 기저로 사영 변환 후 변수 (주성분 : Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc359e",
   "metadata": {},
   "source": [
    "### 주성분 분석\n",
    "아래 2차원 데이터를 좌측과 우측 두 개의 축에 사영시킬 경우 우측 기저(basis)가 좌측 기저에 비해 손실되는 정보의 양(분산의 크기)이 적으므로 상대적으로 선호되는 기저라고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c4e33",
   "metadata": {},
   "source": [
    "<img src=\"./my_icons/var-img.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4504d05",
   "metadata": {},
   "source": [
    "- 1번축과 2번축에 이 데이터를 사영시킨 후에 그 데이터의 분산을 봤을 때 1번분산이 클까? 2번분산이 클까?\n",
    "\n",
    "**답 : 2번이 분산이 더 크다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e01220",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/var.PNG\">\n",
    "<img src = './my_icons/var2.PNG'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327338a7",
   "metadata": {},
   "source": [
    "- 위의 그림의 경우 1번의 분산이 더 크다.\n",
    "- 주성분 분석의 관점에서 1번 축이 더 좋다. (원래 데이터의 분산을 최대화하는 1번 축이 더 좋다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cbafc",
   "metadata": {},
   "source": [
    "## PCA 수리적 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051ced2",
   "metadata": {},
   "source": [
    "$\\bar{X} = \\begin{bmatrix}\n",
    "\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\dots \\\\ \\bar{x}_p\n",
    "\\end{bmatrix}, \\quad \\text{Mean Vector}$\n",
    "\n",
    "\n",
    "$C_n = \\begin{bmatrix}\n",
    "S_{11} & \\dots & S_{1p} \\\\ \\vdots  & \\ddots & \\vdots\\\\ \n",
    "S_{p1} & \\dots & S_{pp}\n",
    "\\end{bmatrix}, \\quad \\text{Covariance Matrix}$\n",
    "\n",
    "\n",
    "\n",
    "$R = \\begin{bmatrix}\n",
    "1  & r_{12} & \\dots  & r_{1p} \\\\ \n",
    "r_{21} & 1 & \\dots & r_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "r_{p1} & r_{p2} & \\dots & 1\n",
    "\\end{bmatrix}, \\quad \\text{Correlation Matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af6e90",
   "metadata": {},
   "source": [
    "### 공분산(Covariance)의 성질\n",
    "- $\\bf{X}$를 $p$개의 변수와 $n$개의 개체로 구성된 $n \\times p$행렬로 정의할 떄 $\\bf{X}$의 공분산 행렬은 다음과 같음\n",
    "\n",
    "$$Cov(\\bf{X}) = \\frac{1}{n}(X-\\bar{X})(X-\\bar{X})^\\top$$\n",
    "\n",
    "- 공분산 행렬의 대각 성분은 각 변수의 분산과 같으며, 비대각행렬은 대응하는 두 변수의 공분산과 같음 (변수 개수 : $p$)\n",
    "\n",
    "$$ C_x = Var[x] = \\begin{bmatrix} \n",
    "Var[x_1] & Cov[x_1,x_2] & \\dots & Cov[x_1, x_p] \\\\\n",
    "Cov[x_2, x_1] & Var[x_2] & \\dots & Cov[x_1,x_p] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "Cov[x_p,x_1] & Cov[x_p,x_2] & \\dots & Var[x_p]\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix} \\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n",
    "\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{p1} & \\sigma_{p2} & \\dots& \\sigma_{pp}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} \\sigma_1^2 & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n",
    "\\sigma_{21} & \\sigma_{2}^2 & \\dots & \\sigma_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{p1} & \\sigma_{p2} & \\dots& \\sigma_{p}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- 데이터의 총분산은 공분산행렬의 대각성분들의 합으로 표현됨.\n",
    "\n",
    "\n",
    "$$tr[Cov(X)] = Cov(X)_{11} + Cov(X)_{22} + Cov(X)_{33} + \\dots Cov(X)_{pp}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d4a33",
   "metadata": {},
   "source": [
    "### 고유값 및 고유벡터\n",
    "- 어떤 행렬 $\\bf A$에 대해 상수 $\\lambda$와 벡터 $\\bf{x}$가 다음 식을 만족할 때, $\\lambda$와 $\\bf{x}$를 각각 행렬 $\\bf{A}$의 고유값 및 고유벡터라고 함.\n",
    "\n",
    "$$ \\bf{A}\\bf{x} = \\lambda \\bf{x} \\to (\\bf{A} - \\lambda I)\\bf{x} = 0$$\n",
    "\n",
    "- 벡터에 행렬을 곱한다는 것은 해당 벡터를 선형변환 (linear transformation)한다는 의미 $\\to$ 고유벡터는 이 변환에 의해 방향이 변하지 않는 벡터를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c2567",
   "metadata": {},
   "source": [
    "##  PCA 알고리즘 - 주성분 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0354d86",
   "metadata": {},
   "source": [
    "<img src = './my_icons/var_origin.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f859aea",
   "metadata": {},
   "source": [
    "- Assume that we have the centered data ($\\text{i.e.}, \\bar{X}_i=0,\\space i=1,\\dots,p$)\n",
    "- Let $\\bf{X}$ be an p-dimensional random vector with the covariance matrix $\\Sigma$\n",
    "- Let $\\alpha$ be an p-dimensional vector of length one ($\\text{i.e.}, \\alpha^\\top \\alpha = 1)$\n",
    "- Let $Z=\\alpha^\\top \\bf{X}$ be the projection of $\\bf{X}$ onto the direction $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3b197",
   "metadata": {},
   "source": [
    "**<center>The main purpose in PCA is</center>**\n",
    "\n",
    "**<center><font color='red'>to find $\\alpha$ that produces the largest variance of $Z$</font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec113ca",
   "metadata": {},
   "source": [
    "$$\\text{Max}\\space Var(Z) = Var(\\alpha^\\top\\bf{X}) = \\alpha^\\top Var(\\bf{X})\\alpha = \\alpha^\\top\\Sigma\\alpha$$\n",
    "\n",
    "$$\\text{s.t.}\\space||\\alpha||=\\alpha^\\top\\alpha=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0ea9d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d742d",
   "metadata": {},
   "source": [
    "다시 전체적인 식을 정리해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8f265",
   "metadata": {},
   "source": [
    "$\\text{Max} \\space \\alpha^\\top\\Sigma\\alpha = \\alpha^\\top E\\Lambda E^\\top\\alpha, \\quad \\Sigma_{m\\times m} = E\\Lambda E^\\top$\n",
    "\n",
    "$\\text{s.t.} ||\\alpha|| = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c6e23",
   "metadata": {},
   "source": [
    "$\\text{Max} \\space \\beta^\\top\\Lambda\\beta \\quad \\text{where}\\space \\beta=E^\\top\\alpha$\n",
    "\n",
    "$\\text{s.t.} ||\\beta||=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703a0d1",
   "metadata": {},
   "source": [
    "$\\text{Max}\\space \\lambda_1\\beta_1^2+\\lambda_2\\beta_2^2 + \\dots + \\lambda_m\\beta_m^2$\n",
    "\n",
    "$\\text{s.t.} \\space \\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2 = 1$\n",
    "\n",
    "$\\qquad\\lambda_1 > \\lambda_2 > \\dots > \\lambda_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6c451",
   "metadata": {},
   "source": [
    "<center>Thus, the optimal value is $\\lambda_1$ and $\\alpha=e_1$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240efd4c",
   "metadata": {},
   "source": [
    "### PCA - 예제"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2971d4",
   "metadata": {},
   "source": [
    "<img src='./my_icons/pca_ex1.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a64b43",
   "metadata": {},
   "source": [
    "$$\\text{normalize } \\bf{X} \\text{ to } E(X_i)=0, Var(X_i)=1$$\n",
    "\n",
    "$$\\lambda_3 > \\lambda_2 > \\lambda_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a104156",
   "metadata": {},
   "source": [
    "$Z_1 = e_1^\\top\\bf{X} = 0.5699X_1 + 0.5765X_2 -0.5855X_3 $\n",
    "\n",
    "$\\quad=0.5699\\cdot\\begin{bmatrix}-1.1930 \\\\ -0.0370 \\\\ -0.5919 \\\\ 0.3792 \\\\ 1.4427\\end{bmatrix} \n",
    "+ 0.5765 \\cdot\\begin{bmatrix}-1.0300 \\\\ -0.7647 \\\\ -0.3257 \\\\ 1.0739 \\\\ 1.0464\\end{bmatrix}\n",
    "-0.5855 \\cdot\\begin{bmatrix}1.5012 \\\\ 0.3540 \\\\ -0.0910 \\\\ -0.7140 \\\\ -1.0502\\end{bmatrix}\n",
    "= \\begin{bmatrix}-2.1527 \\\\ -0.6692 \\\\ -0.4718 \\\\ 1.2533 \\\\ 2.0404\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd05f3",
   "metadata": {},
   "source": [
    "$Z_2 = e_2^\\top\\bf{X} = \\begin{bmatrix}-0.0615\\\\0.4912\\\\-0.2798\\\\-0.4703\\\\0.3204\\end{bmatrix}$\n",
    "\n",
    "$Z_3 = e_3^\\top\\bf{X} = \\begin{bmatrix}0.3160\\\\-0.1493\\\\-0.4047\\\\0.1223\\\\0.1157\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce709a9",
   "metadata": {},
   "source": [
    "$$\\therefore \\bf{Z} =  \n",
    "\\begin{bmatrix}-2.1527 & −0.0615 & 0.3160\\\\\n",
    "-0.6692 & 0.4912& −0.1493\\\\\n",
    "-0.4718 & −0.2798 & −0.4047\\\\\n",
    "1.2533 & −0.4703 & 0.1223\\\\\n",
    "2.0404 & 0.3204& 0.1157\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7079508",
   "metadata": {},
   "source": [
    "$Z_1,Z_2,Z_3$는 오리지널 $X$들의 선형결합으로 얻어진 새로운 축(변수), 새롭게 추출된 변수라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed957f",
   "metadata": {},
   "source": [
    "$$Cov(Z) = \\begin{bmatrix}2.7596 & 0 & 0 \\\\\n",
    "0 & 0.1618 & 0\\\\\n",
    "0 & 0 & 0.0786\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca672263",
   "metadata": {},
   "source": [
    "$$Cov(Z_1, Z_2) = 0$$\n",
    "$$Cov(Z_2, Z_3) = 0$$\n",
    "$$Cov(Z_3, Z_1) = 0$$\n",
    "$$Cov(Z_1, Z_1) = 2.7596$$\n",
    "\n",
    "**<center><font color='blue'>$\\Rightarrow$주성분(Z)들은 서로 독립!!!</center></font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0baa05",
   "metadata": {},
   "source": [
    "### `#` 몇 개의 주성분을 사용해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49163049",
   "metadata": {},
   "source": [
    "<img src='./my_icons/pca_number.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e558ad",
   "metadata": {},
   "source": [
    "$$\\text{Eigenvalues of the covariance matrix}=\\text{Variances of each principal component (각 주성분의 분산)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b8640",
   "metadata": {},
   "source": [
    "<img src='my_icons/pca_pic.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d3e2de",
   "metadata": {},
   "source": [
    "$$\\text{Eigenvalues of the covariance matrix} (\\lambda_1,\\lambda_2,\\lambda_3)$$\n",
    "\n",
    "$$=\\text{Variances of each principal component (각 주성분의 분산)}$$\n",
    "\n",
    "$$Cov(Z) = \\begin{bmatrix}2.7596 & 0 & 0 \\\\\n",
    "0 & 0.1618 & 0\\\\\n",
    "0 & 0 & 0.0786\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f97901",
   "metadata": {},
   "source": [
    "\n",
    "$Var(Z_1) = 2.7596 = \\lambda_3 (\\text{Largest eigenvalues})$\n",
    "\n",
    "$Var(Z_2) = 0.1618 = \\lambda_2$\n",
    "\n",
    "$Var(Z_3) = 0.0786 = \\lambda_1$\n",
    "\n",
    "$$\\text{[Proportion of total population variance due to the 1st principal component]} = \\frac{\\lambda_3}{\\lambda_1+ \\lambda_2 + \\lambda_3}=\\frac{2.7596}{0.0786+0.1618+2.7596} = 0.920$$\n",
    "\n",
    "첫번째 주성분만 사용하면 원래 데이터의 약 $92\\%$를 설명할 수 있다는 것!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9883687",
   "metadata": {},
   "source": [
    "굳이 두번째, 3번째 주성분을 사용하지 않아도 될 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ec0a4",
   "metadata": {},
   "source": [
    "#### `1.` 선택방식 1 : 고유값 감소율이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 수를 선택\n",
    "\n",
    "#### `2.` 선택방식 2: 일정 수준 이상의 분산비를 보존하는 최소의 주성분을 선택 (보통 70% 이상)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f8a65",
   "metadata": {},
   "source": [
    "## PCA Loading Plot -  예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4aebf8",
   "metadata": {},
   "source": [
    "PCA Loading : 실제 변수가 주성분 결정에 얼마나 많은 영향을 미쳤는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d84e7",
   "metadata": {},
   "source": [
    " ## PCA 알고리즘 - 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f020b",
   "metadata": {},
   "source": [
    "### `step1.` 데이터 정규화 (mean centering)\n",
    "### `step2.` 기존 변수의 covariance (correlation) matrix 계산\n",
    "### `step3.` Covariance (correlation) matrix로부터 eigenvalue 및 이에 해당되는 eigenvector를 계산\n",
    "### `step4.` Eigenvalue 및 해당되는 eigenvectors를 순서대로 나열\n",
    "- $\\lambda(1) > \\lambda(2) > \\lambda(3) > \\lambda(4) > \\lambda(5)$\n",
    "- $e(1) > e(2) > e(3) > e(4) > e(5), \\quad i=1,\\dots,5 \\text{ is a vector}$\n",
    "\n",
    "\n",
    "\n",
    "### `step5.` 정렬된 eigenvector를 토대로 기존 변수를 변환\n",
    "\n",
    "\n",
    "- $Z_1 = e(1)\\bf{X} = e_{11}\\cdot X_1 + e_{12}\\cdot X_2 + \\dots + e_{15}\\cdot X_5$\n",
    "- $Z_2 = e(1)\\bf{X} = e_{21}\\cdot X_1 + e_{22}\\cdot X_2 + \\dots + e_{25}\\cdot X_5$\n",
    "- $\\dots$\n",
    "- $Z_5 = e(5)\\bf{X} = e_{51}\\cdot X_1 + e_{52}\\cdot X_2 + \\dots + e_{55}\\cdot X_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2957492",
   "metadata": {},
   "source": [
    "## PCA 한계\n",
    "\n",
    "### 주성분 분석의 특징\n",
    "공분산 행렬의 고유벡터를 사용하므로 단일 가우시안(unimodal) 분포로 추정할 수 있는 데이터에 대해 서로 독립적인 축을 찾는데 사용할 수 있음\n",
    "\n",
    "### 한계점1\n",
    "- 데이터의 분포가 가우시안이 아니거나 다중 가우시안(multimodal) 자료들에 대해서는 적용하기가 어려움\n",
    "\n",
    "- 대안: 커널PCA, LLE (Locally Linear Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a483f8e",
   "metadata": {},
   "source": [
    "<img src = './my_icons/pca_limit.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da87d6",
   "metadata": {},
   "source": [
    "### 한계점2\n",
    "- 분류/예측 문제에 대해서 데이터의 범주 정보를 고려하지 않기 때문에 범주간 구분이 잘 되도록 변환을 해주는 것은 아님\n",
    "    - 주성분분석은 단순히 변환된 축이 최대 분산방향과 정렬되도록 좌표회전을 수행함\n",
    "    - 대안: Partial Least Square (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25314a",
   "metadata": {},
   "source": [
    "<img src = './my_icons/pca_limit2.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9384de",
   "metadata": {},
   "source": [
    "추출된 X변수들이 분류가 잘 되거나 예측이 잘되는 그런 방향으로 추출된 것은 아니다. 분산이 최대화 되는 방향으로 추출된 것. 그렇기 때문에 항상 분류문제와 예측문제에 우리가 추출된  변수를 사용하더라도 잘 된다는 보장은 없다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

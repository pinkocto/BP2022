{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61f9e4b",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "> Unsupervised feature extraction:주성분분석\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: pinkocto\n",
    "- categories: [python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6534f7",
   "metadata": {},
   "source": [
    "## 고차원 데이터\n",
    "\n",
    "<img src = \"./my_icons/high_dim_data.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa1445",
   "metadata": {},
   "source": [
    "- 변수의 수가 많은 $\\to$ 불필요한 변수 존재\n",
    "- 시각적으로 표현하기 어려움\n",
    "- 계산 복잡도 증가 $\\to$ 모델링 비효율적\n",
    "- 중요한 변수만을 선택 $\\to$ 차원축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f0d0c",
   "metadata": {},
   "source": [
    "## 변수 선택 / 추출을 통한 차원축소\n",
    "\n",
    "### 변수선택 (selection) : \n",
    "> 분석 목적에 부합하는 소수의 예측변수만을 선택\n",
    "- 장점: 선택한 변수 해석 용이\n",
    "- 단점: 변수간 상관관계 고려 어려움\n",
    "\n",
    "### 변수추출 (extraction) : \n",
    "> 예측변수의 변환을 통해 새로운 변수 추출\n",
    "- 장점: 변수 간 상관관계 고려, 일반적으로 변수의 개수를 많이 줄일 수 있음\n",
    "- 단점: 추출된 변수의 해석이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f024149",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/feature_select.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42fd43",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Supervised</font> <font color='red'>feature selection</font>: \n",
    "Information gain, Stepwise regression, LASSO, Genetic Algorithm, $\\dots$\n",
    "#### <font color='blue'>Supervised</font> <font color='green'>feature extraction</font>:\n",
    "Partial least squares (PLS)\n",
    "#### <font color='purple'>Unsupervised</font> <font color='red'>feature selection</font>: \n",
    "PCA loading\n",
    "#### <font color='purple'>Unsupervised</font> <font color='green'>feature extraction:</font> \n",
    "$Y$를 이용하지 않고 $X$들의 결합으로 변수를 추출하는 방법<br>\n",
    "Pincipal Component Analysis (PCA), Wavelets transformms, Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a262b6",
   "metadata": {},
   "source": [
    "## PCA 개요\n",
    "- 고차원 데이터를 효과적으로 분석하기 위한 대표적 분석 기법\n",
    "- 차원축소, 시각화, 군집화, 압축\n",
    "- PCA는 $n$개의 관측치와 $p$개의 변수로 구성된 데이터를 상관관계가 없는 $k$개의 변수로 구성된 데이터 ($n$개의 관측치)로 요약하는 방식으로, 이 때 요약된 변수는 기존 변수의 선형조합으로 생성됨.\n",
    "\n",
    "- 원래 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영 ( Projection) 시키는 기법\n",
    "\n",
    "- 주요 목적\n",
    "    - 데이터 차원 축소 ($n \\times p \\to n\\times k, \\space where \\space k << p)$\n",
    "    - 데이터 시각화 및 해석\n",
    "    \n",
    "- 일반적으로 PCA는 전체 분석 과정 충 초기에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036872c6",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/pca_outline.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fabe53",
   "metadata": {},
   "source": [
    "$Z_1,Z_2, Z_3$은 기존 변수인 $X_1, X_2, \\dots X_p$의 선형 조합으로 새롭게 생성된 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6579ebe",
   "metadata": {},
   "source": [
    "$Z$ is linear combination (선형결합) of the original $p$ variables in $X$\n",
    "\n",
    "$$Z_1 = a_1^TX = a_{11}X_1 + a_{12}X_2 + \\dots a_{1p}X_p$$\n",
    "$$Z_2 = a_2^TX = a_{21}X_1 + a_{22}X_2 + \\dots a_{2p}X_p$$\n",
    "$$\\vdots$$\n",
    "\n",
    "$$Z_p = a_p^TX = a_{p1}X_1 + a_{p2}X_2 + \\dots a_{pp}X_p$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37eccf",
   "metadata": {},
   "source": [
    "- $X_1, X_2, \\dots, X_p$: 원래 변수 (original variable)\n",
    "- $a_i = [a_{i1}, a_{i2}, \\dots, a_{ip}]$, $i$번째 기저(basis) 또는 계수(Loading)\n",
    "- $Z_1, Z_2, \\dots, Z_p$ 각 기저로 사영 변환 후 변수 (주성분 : Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32e66",
   "metadata": {},
   "source": [
    "### 주성분 분석\n",
    "아래 2차원 데이터를 좌측과 우측 두 개의 축에 사영시킬 경우 우측 기저(basis)가 좌측 기저에 비해 손실되는 정보의 양(분산의 크기)이 적으므로 상대적으로 선호되는 기저라고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c560ad2",
   "metadata": {},
   "source": [
    "<img src=\"./my_icons/var-img.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6b47b",
   "metadata": {},
   "source": [
    "- 1번축과 2번축에 이 데이터를 사영시킨 후에 그 데이터의 분산을 봤을 때 1번분산이 클까? 2번분산이 클까?\n",
    "\n",
    "**답 : 2번이 분산이 더 크다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d126f9d",
   "metadata": {},
   "source": [
    "<img src = \"./my_icons/var.PNG\">\n",
    "<img src = './my_icons/var2.PNG'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f08c6e2",
   "metadata": {},
   "source": [
    "- 위의 그림의 경우 1번의 분산이 더 크다.\n",
    "- 주성분 분석의 관점에서 1번 축이 더 좋다. (원래 데이터의 분산을 최대화하는 1번 축이 더 좋다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb410",
   "metadata": {},
   "source": [
    "## PCA 수리적 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c191062",
   "metadata": {},
   "source": [
    "$\\bar{X} = \\begin{bmatrix}\n",
    "\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\dots \\\\ \\bar{x}_p\n",
    "\\end{bmatrix}, \\quad \\text{Mean Vector}$\n",
    "\n",
    "\n",
    "$C_n = \\begin{bmatrix}\n",
    "S_{11} & \\dots & S_{1p} \\\\ \\vdots  & \\ddots & \\vdots\\\\ \n",
    "S_{p1} & \\dots & S_{pp}\n",
    "\\end{bmatrix}, \\quad \\text{Covariance Matrix}$\n",
    "\n",
    "\n",
    "\n",
    "$R = \\begin{bmatrix}\n",
    "1  & r_{12} & \\dots  & r_{1p} \\\\ \n",
    "r_{21} & 1 & \\dots & r_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "r_{p1} & r_{p2} & \\dots & 1\n",
    "\\end{bmatrix}, \\quad \\text{Correlation Matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99112ac4",
   "metadata": {},
   "source": [
    "### 공분산(Covariance)의 성질\n",
    "- $\\bf{X}$를 $p$개의 변수와 $n$개의 개체로 구성된 $n \\times p$행렬로 정의할 떄 $\\bf{X}$의 공분산 행렬은 다음과 같음\n",
    "\n",
    "$$Cov(\\bf{X}) = \\frac{1}{n}(X-\\bar{X})(X-\\bar{X})^\\top$$\n",
    "\n",
    "- 공분산 행렬의 대각 성분은 각 변수의 분산과 같으며, 비대각행렬은 대응하는 두 변수의 공분산과 같음 (변수 개수 : $p$)\n",
    "\n",
    "$$ C_x = Var[x] = \\begin{bmatrix} \n",
    "Var[x_1] & Cov[x_1,x_2] & \\dots & Cov[x_1, x_p] \\\\\n",
    "Cov[x_2, x_1] & Var[x_2] & \\dots & Cov[x_1,x_p] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "Cov[x_p,x_1] & Cov[x_p,x_2] & \\dots & Var[x_p]\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix} \\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n",
    "\\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{p1} & \\sigma_{p2} & \\dots& \\sigma_{pp}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} \\sigma_1^2 & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n",
    "\\sigma_{21} & \\sigma_{2}^2 & \\dots & \\sigma_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{p1} & \\sigma_{p2} & \\dots& \\sigma_{p}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- 데이터의 총분산은 공분산행렬의 대각성분들의 합으로 표현됨.\n",
    "\n",
    "\n",
    "$$tr[Cov(X)] = Cov(X)_{11} + Cov(X)_{22} + Cov(X)_{33} + \\dots Cov(X)_{pp}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fcfa8",
   "metadata": {},
   "source": [
    "### 고유값 및 고유벡터\n",
    "- 어떤 행렬 $\\bf A$에 대해 상수 $\\lambda$와 벡터 $\\bf{x}$가 다음 식을 만족할 때, $\\lambda$와 $\\bf{x}$를 각각 행렬 $\\bf{A}$의 고유값 및 고유벡터라고 함.\n",
    "\n",
    "$$ \\bf{A}\\bf{x} = \\lambda \\bf{x} \\to (\\bf{A} - \\lambda I)\\bf{x} = 0$$\n",
    "\n",
    "- 벡터에 행렬을 곱한다는 것은 해당 벡터를 선형변환 (linear transformation)한다는 의미 $\\to$ 고유벡터는 이 변환에 의해 방향이 변하지 않는 벡터를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f5d2e",
   "metadata": {},
   "source": [
    "##  PCA 알고리즘 - 주성분 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a977",
   "metadata": {},
   "source": [
    "<img src = './my_icons/var_origin.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8c0cf",
   "metadata": {},
   "source": [
    "- Assume that we have the centered data ($\\text{i.e.}, \\bar{X}_i=0,\\space i=1,\\dots,p$)\n",
    "- Let $\\bf{X}$ be an p-dimensional random vector with the covariance matrix $\\Sigma$\n",
    "- Let $\\alpha$ be an p-dimensional vector of length one ($\\text{i.e.}, \\alpha^\\top \\alpha = 1)$\n",
    "- Let $Z=\\alpha^\\top \\bf{X}$ be the projection of $\\bf{X}$ onto the direction $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ef67c",
   "metadata": {},
   "source": [
    "**<center>The main purpose in PCA is</center>**\n",
    "\n",
    "**<center><font color='red'>to find $\\alpha$ that produces the largest variance of $Z$</font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba75b1",
   "metadata": {},
   "source": [
    "$$\\text{Max}\\space Var(Z) = Var(\\alpha^\\top\\bf{X}) = \\alpha^\\top Var(\\bf{X})\\alpha = \\alpha^\\top\\Sigma\\alpha$$\n",
    "\n",
    "$$\\text{s.t.}\\space||\\alpha||=\\alpha^\\top\\alpha=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bdb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f93d1c",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "> 편향 분산 트레이드 오프에 대해 이해해보자.\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: dinonene\n",
    "- categories: [python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9993f1",
   "metadata": {},
   "source": [
    "<img src='./my_icons/bias_variance_tradeoff.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b18d10",
   "metadata": {},
   "source": [
    "위의 그림을 가지고 생각해보자.\n",
    "\n",
    "만약 내가 데이터를 10만개를 가지고 있다고 가정해보자. 모델링을 할 때 이 데이터 전체에 대해서 하는 것이 아니라 데이터의 부분집합 6개의 subset을 만들어서 (각각 다른 조합의 데이터들이 6개의 subset이 된다.) 이 6개의 subset에 대해서 트레이닝을 하고 예측을 해본다라고 하자.\n",
    "\n",
    "우리가 복잡한 모델과 덜 복잡한 모델을 사용했을 때 어떻게 달라지는가를 알아보자.\n",
    "\n",
    "Decision Tree를 예를들어 봅시다. 트리가 Adaboost처럼 딱 한단계만 들어있으면 단순한 모델이 되고, 트리의 깊이가 20단계까지 내려가면 훨씬 복잡한 모델이 되겠죠!\n",
    "\n",
    "내가 가진 데이터의 독립변수의 개수가 20개라고 했을 때 이 얕은 트리에서는 20개의 변수 중에 가장 중요하다고 생각하는 변수를 하나밖에 사용을 안할것이고, 깊은 트리에서는 20개가 다 쓰일지 안쓰일지 모르겠으나 훨씬 많은 독립변수들이 이 트리를 만드는데 개입을 할 거에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb9f6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2699b82",
   "metadata": {},
   "source": [
    "단순한 모델의 경우 예측했을 때 값이 굉장히 비슷해 질 것이다. 반대로 깊이가 깊은 모델을 가지고 예측을 하면 각각의 training set(6개의 subset)에 대해서 결과값들이 들쭉날쭉한 그런 값들이 나올 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d5d8e",
   "metadata": {},
   "source": [
    "상식적으로 너무 단순하게 예측을 했기 때문에 실제값과 차이가 클 수 밖에 없어요. 그래서 BIas가 높은 것 입니다. 반대로, 이 결과값은 상당히 동일하게 유지가 되죠. 그래서 Variance가 낮은거에요. Variance가 낮다는 것은 다양한 Subset(data)을 가지고 돌려봤을 때 이걸 할때마다 예측값이 얼마나 다양하게 나오는지 아니면 동일하게 나오는지입니다.\n",
    "\n",
    "그래서 단순한 모델의 경우 Bias가 높고, Variance가 낮다.\n",
    "\n",
    "\n",
    "반대로 모델이 복잡한 경우 (트리가 깊은 경우)는 각가의 트레이닝 데이터셋을 최대한 잘 예측하게끔 모델링이 되기 때문에 Bias가 낮고, training set에 대해서 결과가 바뀌기 때문에 여기선 Variance가 높다라고 할 수 있습니다.\n",
    "\n",
    "\n",
    "***그래서 모델이 얼마나 복잡하냐 복잡하지 않냐에 따라서 Bias와 Variance가 이런식으로 달라지는 거죠***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef3122",
   "metadata": {},
   "source": [
    "[그림1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d307b",
   "metadata": {},
   "source": [
    "- High Bias / Low Variance : 즉, 20개의 subset을 예측했을 때 항상 비슷한 결과값을 보여주지만 실제 값이랑은 꽤 거리가 있다. 하지만 한 곳에 잘 뭉쳐있으니까 Variance는 낮다.\n",
    "\n",
    "- 20개의 subset을 가지고 모델링 했을 때 매번 다른 결과값을 보여준다. 즉, 트레이닝 데이터 셋의 특성에 따라 나오는 결과값도 항상 다르기 때문에 Variance가 높은거고, 대신 bias는 training set에 대해서 좀 더 잘 예측을 하도록 모델링이 되기 때문에 bias는 낮게 예측이 되는거죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a124e8",
   "metadata": {},
   "source": [
    "[그림2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9fba1",
   "metadata": {},
   "source": [
    "모델이 복잡할수록(오른쪽) 다양한 subset에 대해서 예측을 했을 때 매번 결과는 달라지니까 Variance가 높고, 대신 매번 그 subset에 대한 예측은 잘 하기 때문에 bias는 낮은 거에요. 그래서 이 경우는 오버피팅이 걱정이 되는것이고\n",
    "\n",
    "반대로 모델의 복잡도가 낮은 경우는 여러개의 subset을 통해서 예측을 하더라도 그 결과값이 거의 동일하게 나오기 때문에 Variance는 굉장히 낮으나, 애초에 그 예측력이 떨어져서 bias가 높다. 그래서 언더피팅이 우려가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2edc3e",
   "metadata": {},
   "source": [
    ">Note: 바이어스는 에러라고 보면되고, Variance는 여러개의 subset data를 training set으로 썼을 때 매번 할때마다 얼마나 다양한 결과값이 나오느냐를 의미한다고 보면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15866d8e",
   "metadata": {},
   "source": [
    "> Tip: 즉, 복잡한 모델일수록 에러(바이어스)는 낮고, 결과값은 다이나믹하게 달라질 수 있기 때문에 Variance는 높다. 모델이 단순하면 그 반대다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

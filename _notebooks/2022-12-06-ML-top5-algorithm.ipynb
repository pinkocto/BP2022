{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2489b4c",
   "metadata": {},
   "source": [
    "# ML Top5 Algorithm\n",
    "> 머신러닝 입문자들을 위한 Top5 알고리즘\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: dinonene\n",
    "- categories: [python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f775295",
   "metadata": {},
   "source": [
    "### 01. Linear Regression\n",
    "\n",
    "1. 가장 기본적인 기법\n",
    "\n",
    "2. 해석하기 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10de90",
   "metadata": {},
   "source": [
    "### 02. Logistic Regression\n",
    "1. Binary Classification을 위한 가장 기본적인 기법\n",
    "2. 해석하기 쉬움\n",
    "3. Deep Learning의 Activation으로도 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e33c5",
   "metadata": {},
   "source": [
    "### 03. Decision Tree\n",
    "**1. 활용 범주가 넓음**\n",
    "\n",
    " - Linear Regression 같은 경우는 연속형변수를 예측하는데 사용하였고, Logistic Regression은 yes or no와 같은 Binary Classification에 쓴다.\n",
    "\n",
    " - Decision Tree는 이 둘 다 활용이 가능하고 거기에 더해서 다중 클래스 classification 모델에서도 사용할 수 있다.\n",
    "   \n",
    "   \n",
    "   \n",
    "**2. Visulaization**\n",
    "\n",
    "**3. 최신 알고리즘 기반 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87744e25",
   "metadata": {},
   "source": [
    "### 04. Random Forest\n",
    "\n",
    "1. 활용 범주가 넓음\n",
    "\n",
    "2. 앙상블 모델의 기본\n",
    "\n",
    "3. 비교적 좋은 예측 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ce170",
   "metadata": {},
   "source": [
    "### 05. Kmeans Clustering\n",
    "\n",
    "1. Unsupervised Learning\n",
    "2. Segmentation이 용이\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "어떤 식으로 segment가 뭉칠지 돌리는 사람도 모릅니다.\n",
    "\n",
    "클러스터링을 하고나서 나눠진 걸 보고 아 이런 애들끼리 뭉쳤구나..! \n",
    "\n",
    "클러스터링이 끝난 뒤에 알 수 있는게 바로 Unsupervised Learning의 특징\n",
    "\n",
    "이처럼 이 알고리즘은 돌리는 사람이 미리 답을 정해 놓지 않는다는 겁니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7247b2c",
   "metadata": {},
   "source": [
    "데이터는 일단 주어져 있고 알고리즘을 돌리는 사람은 이걸 3개의 세그먼트(클러스터)로 나눌지 5개로 나눌지 딱 그것만 정해주면 이 알고리즘은 데이터를 가장 비슷한 애들끼리 딱 그 숫자, 지정된 클러스터의 숫자만큼 나오도록 알아서 묶어주는 거에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d30b6e",
   "metadata": {},
   "source": [
    "3개로 묶는 것도 여러가지 방법으로 묶을 수 있겠지만, 컴퓨터가 판단하기에 이렇게 묶었을 때 이 그룹간의 특징이 가장 잘 설명된다는 거죠. \n",
    "\n",
    "그룹내에 있는 애들끼리는 최대한 유사하고, 그룹 간의 성격은 최대한 좀 떨어져있는\n",
    "\n",
    "그래서 Kmeans 클러스터링을 돌리면 어떤 식으로 결과가 나올지 예측이 불가능하고 그걸 돌리고 난 뒤에 클러스터 3개를 얻어 봤더니 A라는 특징은 이런 특징이 있고, B는 이렇고, C는 이렇다.이런식으로 해석 할 수 있는 거에요.\n",
    "\n",
    "그러기 때문에 이건 완전히 Unsupervised Learning입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafab87a",
   "metadata": {},
   "source": [
    "### And. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a84ee",
   "metadata": {},
   "source": [
    "XGBoost는 앞에서 설명한 Decision Tree, Random Forest 같은 트리 모델 중의 하나이고, 나름 최신기법 중 하나입니다. 실제로 나온 시점은 수년이 지났지만, 여전히 많이 활발하게 사용되는 알고리즘이고 캐클 컴페티션에서 이미지 같은 거 아니고 그냥 Binary Classification 같은 문제면 거의 XGBoost 이것만 있으면 어느정도는 다룰 수 있어요.\n",
    "\n",
    "\n",
    "최신 기법이라고 할만한 트리 모델은 xgboost, LightGBM, 그리고 Cat Boost이렇게 3개가 있는데 xgboost가 가장 오래됐고, 그 다음으로 핫하게 뜨고 있는게 LightGBM, Cat Boost는 가장 마지막으로 나온 정말 최신 모델."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

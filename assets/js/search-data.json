{
  
    
        "post0": {
            "title": "Crack",
            "content": "import matplotlib.pyplot as plt import seaborn as sns import keras from keras.models import Sequential from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import Adam, RMSprop, Adagrad from keras.layers import BatchNormalization from sklearn.metrics import classification_report,confusion_matrix import tensorflow as tf import cv2 import os import time import numpy as np import warnings warnings.filterwarnings(&#39;ignore&#39;) . print(keras.__version__) print(cv2.__version__) print(sns.__version__) print(np.__version__) . 2.10.0 4.6.0 0.12.0 1.23.4 . os.getcwd() # 현재 작업 폴더 . &#39;C: Users hanka Desktop dino BP2022 _notebooks&#39; . labels = [&#39;Negative_500&#39;, &#39;Positive_500&#39;] img_size = 120 # 어디 폴더의 Nagative, Postive를 불러오겠다. def read_images(data_dir): data = [] for label in labels: path = os.path.join(data_dir, label) class_num = labels.index(label) for img in os.listdir(path): try: img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE) # 이미지 변경 resized_arr = cv2.resize(img_arr, (img_size, img_size)) data.append([resized_arr, class_num]) except Exception as e: print(e) return np.array(data) Dataset = read_images(&#39;./data/Surface_Crack_Detection_small&#39;) . print( Dataset.shape ) print( Dataset[0][0], Dataset[0][1]) # 피처와 Target . (1000, 2) [[172 161 149 ... 183 182 184] [174 166 147 ... 176 175 177] [176 171 170 ... 180 176 176] ... [163 166 170 ... 175 175 173] [158 155 164 ... 172 173 171] [161 153 154 ... 170 172 170]] 0 . data_dir = &#39;./data/Surface_Crack_Detection_small&#39; path_negative = os.path.join(data_dir, &quot;Negative_500&quot;) path_positive = os.path.join(data_dir, &quot;Positive_500&quot;) # 파일 및 폴더 내용 확인 print( len(os.listdir(path_negative) )) print( len(os.listdir(path_positive) )) num_n = len(os.listdir(path_negative) ) num_p = len(os.listdir(path_positive) ) num = [num_n, num_p] . 500 500 . &#54028;&#51068; &#44060;&#49688; &#54869;&#51064; . Im = [&#39;Negative&#39;, &#39;Positive&#39;] num = [num_n, num_p] plt.figure(figsize=(10, 6)) x = np.arange(2) plt.bar(Im, num) . &lt;BarContainer object of 2 artists&gt; . Dataset[0] . array([array([[172, 161, 149, ..., 183, 182, 184], [174, 166, 147, ..., 176, 175, 177], [176, 171, 170, ..., 180, 176, 176], ..., [163, 166, 170, ..., 175, 175, 173], [158, 155, 164, ..., 172, 173, 171], [161, 153, 154, ..., 170, 172, 170]], dtype=uint8), 0], dtype=object) . print(Dataset.shape) . (1000, 2) . Dataset[0][0] # 픽셀 데이터 . array([[172, 161, 149, ..., 183, 182, 184], [174, 166, 147, ..., 176, 175, 177], [176, 171, 170, ..., 180, 176, 176], ..., [163, 166, 170, ..., 175, 175, 173], [158, 155, 164, ..., 172, 173, 171], [161, 153, 154, ..., 170, 172, 170]], dtype=uint8) . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . x = [] y = [] for feature, label in Dataset: x.append(feature) y.append(label) x = np.array(x).reshape(-1, img_size, img_size, 1) x = x / 255 y = np.array(y) print(x.shape, y.shape) . (1000, 120, 120, 1) (1000,) . plt.subplot(1, 2, 1) plt.imshow(x[300].reshape(img_size, img_size), cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) . (-0.5, 119.5, 119.5, -0.5) . plt.subplot(1, 2, 2) plt.imshow(x[500].reshape(img_size, img_size), cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) . (-0.5, 119.5, 119.5, -0.5) . model = Sequential() model.add(Conv2D(64, 3,padding=&quot;same&quot;, activation=&quot;relu&quot;, input_shape = x.shape[1:])) model.add(MaxPool2D()) model.add(Conv2D(64, 3, padding=&quot;same&quot;, activation=&quot;relu&quot;)) model.add(MaxPool2D()) model.add(Conv2D(128, 3, padding=&quot;same&quot;, activation=&quot;relu&quot;)) model.add(MaxPool2D()) model.add(Flatten()) model.add(Dense(256,activation=&quot;relu&quot;)) model.add(Dropout(0.5)) model.add(BatchNormalization()) model.add(Dense(1, activation=&quot;sigmoid&quot;)) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 120, 120, 64) 640 max_pooling2d (MaxPooling2D (None, 60, 60, 64) 0 ) conv2d_1 (Conv2D) (None, 60, 60, 64) 36928 max_pooling2d_1 (MaxPooling (None, 30, 30, 64) 0 2D) conv2d_2 (Conv2D) (None, 30, 30, 128) 73856 max_pooling2d_2 (MaxPooling (None, 15, 15, 128) 0 2D) flatten (Flatten) (None, 28800) 0 dense (Dense) (None, 256) 7373056 dropout (Dropout) (None, 256) 0 batch_normalization (BatchN (None, 256) 1024 ormalization) dense_1 (Dense) (None, 1) 257 ================================================================= Total params: 7,485,761 Trainable params: 7,485,249 Non-trainable params: 512 _________________________________________________________________ . start = time.time() opt = Adam(lr=1e-5) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;]) history = model.fit(x, y, epochs = 15, batch_size = 128, validation_split = 0.25, verbose=1) print(&quot;소요시간&quot;, time.time() - start ) . Epoch 1/15 6/6 [==============================] - 11s 2s/step - loss: 0.6236 - accuracy: 0.6627 - val_loss: 0.7916 - val_accuracy: 0.0000e+00 Epoch 2/15 6/6 [==============================] - 10s 2s/step - loss: 0.5544 - accuracy: 0.7147 - val_loss: 0.8501 - val_accuracy: 0.0000e+00 Epoch 3/15 6/6 [==============================] - 10s 2s/step - loss: 0.5201 - accuracy: 0.7573 - val_loss: 0.8742 - val_accuracy: 0.0000e+00 Epoch 4/15 6/6 [==============================] - 11s 2s/step - loss: 0.4800 - accuracy: 0.7827 - val_loss: 0.8732 - val_accuracy: 0.0000e+00 Epoch 5/15 6/6 [==============================] - 11s 2s/step - loss: 0.4774 - accuracy: 0.7960 - val_loss: 0.8605 - val_accuracy: 0.0000e+00 Epoch 6/15 6/6 [==============================] - 11s 2s/step - loss: 0.4546 - accuracy: 0.8027 - val_loss: 0.8374 - val_accuracy: 0.0000e+00 Epoch 7/15 6/6 [==============================] - 10s 2s/step - loss: 0.4381 - accuracy: 0.8173 - val_loss: 0.8102 - val_accuracy: 0.0000e+00 Epoch 8/15 6/6 [==============================] - 11s 2s/step - loss: 0.4358 - accuracy: 0.8133 - val_loss: 0.7829 - val_accuracy: 0.0000e+00 Epoch 9/15 6/6 [==============================] - 11s 2s/step - loss: 0.4164 - accuracy: 0.8227 - val_loss: 0.7499 - val_accuracy: 0.0080 Epoch 10/15 6/6 [==============================] - 12s 2s/step - loss: 0.3856 - accuracy: 0.8493 - val_loss: 0.7181 - val_accuracy: 0.2400 Epoch 11/15 6/6 [==============================] - 10s 2s/step - loss: 0.3797 - accuracy: 0.8533 - val_loss: 0.6935 - val_accuracy: 0.5760 Epoch 12/15 6/6 [==============================] - 11s 2s/step - loss: 0.3373 - accuracy: 0.8827 - val_loss: 0.6736 - val_accuracy: 0.7200 Epoch 13/15 6/6 [==============================] - 11s 2s/step - loss: 0.3323 - accuracy: 0.8747 - val_loss: 0.6644 - val_accuracy: 0.7480 Epoch 14/15 6/6 [==============================] - 11s 2s/step - loss: 0.3101 - accuracy: 0.8920 - val_loss: 0.6438 - val_accuracy: 0.8880 Epoch 15/15 6/6 [==============================] - 11s 2s/step - loss: 0.3218 - accuracy: 0.8747 - val_loss: 0.6266 - val_accuracy: 0.9560 소요시간 162.9058222770691 . plt.figure(figsize=(12, 12)) plt.style.use(&#39;ggplot&#39;) plt.subplot(2,2,1) plt.plot(history.history[&#39;accuracy&#39;]) plt.plot(history.history[&#39;val_accuracy&#39;]) plt.title(&#39;Accuracy of the Model&#39;) plt.ylabel(&#39;Accuracy&#39;, fontsize=12) plt.xlabel(&#39;Epoch&#39;, fontsize=12) plt.legend([&#39;train accuracy&#39;, &#39;validation accuracy&#39;], loc=&#39;lower right&#39;, prop={&#39;size&#39;: 12}) plt.subplot(2,2,2) plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss of the Model&#39;) . Text(0.5, 1.0, &#39;Loss of the Model&#39;) . 한 에폭 당 6번밖에 학습을 못하니까 8에폭까지 성능변화가 없다가 그 이후에 올라가는 것을 볼 수 있다. | . &#49884;&#46020; 1. barch size&#47484; &#51460;&#50668;&#49436; &#54620; &#50640;&#54253; &#45817; &#54617;&#49845;&#54943;&#49688;&#47484; &#45720;&#47536;&#45796;. . start = time.time() opt = Adam(lr=1e-5) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;]) history = model.fit(x, y, epochs = 15, batch_size = 16, validation_split = 0.25, verbose=1) print(&quot;소요시간&quot;, time.time() - start ) . Epoch 1/15 47/47 [==============================] - 13s 256ms/step - loss: 0.2841 - accuracy: 0.8987 - val_loss: 0.5707 - val_accuracy: 0.9680 Epoch 2/15 47/47 [==============================] - 13s 279ms/step - loss: 0.2494 - accuracy: 0.9227 - val_loss: 0.5101 - val_accuracy: 0.9760 Epoch 3/15 47/47 [==============================] - 13s 273ms/step - loss: 0.2229 - accuracy: 0.9387 - val_loss: 0.4406 - val_accuracy: 0.9880 Epoch 4/15 47/47 [==============================] - 12s 264ms/step - loss: 0.1932 - accuracy: 0.9427 - val_loss: 0.3940 - val_accuracy: 0.9680 Epoch 5/15 47/47 [==============================] - 12s 260ms/step - loss: 0.1846 - accuracy: 0.9480 - val_loss: 0.2834 - val_accuracy: 0.9920 Epoch 6/15 47/47 [==============================] - 13s 269ms/step - loss: 0.1569 - accuracy: 0.9573 - val_loss: 0.2352 - val_accuracy: 0.9840 Epoch 7/15 47/47 [==============================] - 12s 261ms/step - loss: 0.1490 - accuracy: 0.9600 - val_loss: 0.1286 - val_accuracy: 0.9960 Epoch 8/15 47/47 [==============================] - 13s 271ms/step - loss: 0.1649 - accuracy: 0.9587 - val_loss: 0.1572 - val_accuracy: 0.9560 Epoch 9/15 47/47 [==============================] - 12s 259ms/step - loss: 0.1393 - accuracy: 0.9680 - val_loss: 0.0991 - val_accuracy: 0.9760 Epoch 10/15 47/47 [==============================] - 12s 259ms/step - loss: 0.1247 - accuracy: 0.9680 - val_loss: 0.0710 - val_accuracy: 0.9720 Epoch 11/15 47/47 [==============================] - 12s 259ms/step - loss: 0.1239 - accuracy: 0.9680 - val_loss: 0.0948 - val_accuracy: 0.9560 Epoch 12/15 47/47 [==============================] - 12s 258ms/step - loss: 0.1100 - accuracy: 0.9773 - val_loss: 0.0291 - val_accuracy: 0.9920 Epoch 13/15 47/47 [==============================] - 12s 258ms/step - loss: 0.1277 - accuracy: 0.9720 - val_loss: 0.0438 - val_accuracy: 0.9800 Epoch 14/15 47/47 [==============================] - 12s 258ms/step - loss: 0.1024 - accuracy: 0.9760 - val_loss: 0.0409 - val_accuracy: 0.9840 Epoch 15/15 47/47 [==============================] - 12s 258ms/step - loss: 0.0855 - accuracy: 0.9853 - val_loss: 0.0356 - val_accuracy: 0.9880 소요시간 185.70813751220703 . plt.figure(figsize=(12, 12)) plt.style.use(&#39;ggplot&#39;) plt.subplot(2,2,1) plt.plot(history.history[&#39;accuracy&#39;]) plt.plot(history.history[&#39;val_accuracy&#39;]) plt.title(&#39;Accuracy of the Model&#39;) plt.ylabel(&#39;Accuracy&#39;, fontsize=12) plt.xlabel(&#39;Epoch&#39;, fontsize=12) plt.legend([&#39;train accuracy&#39;, &#39;validation accuracy&#39;], loc=&#39;lower right&#39;, prop={&#39;size&#39;: 12}) plt.subplot(2,2,2) plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss of the Model&#39;) . Text(0.5, 1.0, &#39;Loss of the Model&#39;) . &#49884;&#46020; 2. &#50640;&#54253;&#51012; &#45720;&#47536;&#45796;. . start = time.time() opt = Adam(lr=1e-5) model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;]) history = model.fit(x, y, epochs = 30, batch_size = 128, validation_split = 0.25, verbose=1) print(&quot;소요시간&quot;, time.time() - start ) . Epoch 1/30 6/6 [==============================] - 9s 1s/step - loss: 0.0658 - accuracy: 0.9813 - val_loss: 0.0301 - val_accuracy: 0.9920 Epoch 2/30 6/6 [==============================] - 9s 1s/step - loss: 0.0662 - accuracy: 0.9853 - val_loss: 0.0309 - val_accuracy: 0.9920 Epoch 3/30 6/6 [==============================] - 9s 2s/step - loss: 0.0661 - accuracy: 0.9867 - val_loss: 0.0340 - val_accuracy: 0.9880 Epoch 4/30 6/6 [==============================] - 9s 1s/step - loss: 0.0650 - accuracy: 0.9840 - val_loss: 0.0298 - val_accuracy: 0.9920 Epoch 5/30 6/6 [==============================] - 9s 1s/step - loss: 0.0633 - accuracy: 0.9867 - val_loss: 0.0222 - val_accuracy: 0.9960 Epoch 6/30 6/6 [==============================] - 9s 2s/step - loss: 0.0618 - accuracy: 0.9867 - val_loss: 0.0239 - val_accuracy: 0.9960 Epoch 7/30 6/6 [==============================] - 9s 2s/step - loss: 0.0608 - accuracy: 0.9880 - val_loss: 0.0283 - val_accuracy: 0.9920 Epoch 8/30 6/6 [==============================] - 9s 1s/step - loss: 0.0638 - accuracy: 0.9880 - val_loss: 0.0290 - val_accuracy: 0.9920 Epoch 9/30 6/6 [==============================] - 9s 1s/step - loss: 0.0539 - accuracy: 0.9893 - val_loss: 0.0252 - val_accuracy: 0.9920 Epoch 10/30 6/6 [==============================] - 9s 1s/step - loss: 0.0631 - accuracy: 0.9853 - val_loss: 0.0396 - val_accuracy: 0.9800 Epoch 11/30 6/6 [==============================] - 9s 1s/step - loss: 0.0568 - accuracy: 0.9867 - val_loss: 0.0223 - val_accuracy: 0.9960 Epoch 12/30 6/6 [==============================] - 9s 1s/step - loss: 0.0562 - accuracy: 0.9853 - val_loss: 0.0232 - val_accuracy: 0.9960 Epoch 13/30 6/6 [==============================] - 9s 1s/step - loss: 0.0558 - accuracy: 0.9853 - val_loss: 0.0290 - val_accuracy: 0.9880 Epoch 14/30 6/6 [==============================] - 9s 1s/step - loss: 0.0568 - accuracy: 0.9867 - val_loss: 0.0269 - val_accuracy: 0.9920 Epoch 15/30 6/6 [==============================] - 9s 1s/step - loss: 0.0501 - accuracy: 0.9880 - val_loss: 0.0386 - val_accuracy: 0.9800 Epoch 16/30 6/6 [==============================] - 9s 1s/step - loss: 0.0542 - accuracy: 0.9853 - val_loss: 0.0350 - val_accuracy: 0.9800 Epoch 17/30 6/6 [==============================] - 9s 1s/step - loss: 0.0478 - accuracy: 0.9893 - val_loss: 0.0237 - val_accuracy: 0.9960 Epoch 18/30 6/6 [==============================] - 9s 1s/step - loss: 0.0520 - accuracy: 0.9893 - val_loss: 0.0174 - val_accuracy: 1.0000 Epoch 19/30 6/6 [==============================] - 9s 1s/step - loss: 0.0501 - accuracy: 0.9867 - val_loss: 0.0186 - val_accuracy: 0.9960 Epoch 20/30 6/6 [==============================] - 9s 1s/step - loss: 0.0520 - accuracy: 0.9893 - val_loss: 0.0270 - val_accuracy: 0.9840 Epoch 21/30 6/6 [==============================] - 9s 1s/step - loss: 0.0528 - accuracy: 0.9880 - val_loss: 0.0177 - val_accuracy: 1.0000 Epoch 22/30 6/6 [==============================] - 9s 1s/step - loss: 0.0532 - accuracy: 0.9880 - val_loss: 0.0278 - val_accuracy: 0.9840 Epoch 23/30 6/6 [==============================] - 9s 1s/step - loss: 0.0449 - accuracy: 0.9893 - val_loss: 0.0273 - val_accuracy: 0.9840 Epoch 24/30 6/6 [==============================] - 9s 1s/step - loss: 0.0509 - accuracy: 0.9893 - val_loss: 0.0188 - val_accuracy: 1.0000 Epoch 25/30 6/6 [==============================] - 9s 1s/step - loss: 0.0472 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 0.9960 Epoch 26/30 6/6 [==============================] - 9s 1s/step - loss: 0.0466 - accuracy: 0.9893 - val_loss: 0.0254 - val_accuracy: 0.9880 Epoch 27/30 6/6 [==============================] - 9s 1s/step - loss: 0.0426 - accuracy: 0.9920 - val_loss: 0.0262 - val_accuracy: 0.9840 Epoch 28/30 6/6 [==============================] - 9s 1s/step - loss: 0.0437 - accuracy: 0.9907 - val_loss: 0.0321 - val_accuracy: 0.9800 Epoch 29/30 6/6 [==============================] - 9s 1s/step - loss: 0.0473 - accuracy: 0.9893 - val_loss: 0.0176 - val_accuracy: 1.0000 Epoch 30/30 6/6 [==============================] - 9s 1s/step - loss: 0.0432 - accuracy: 0.9907 - val_loss: 0.0303 - val_accuracy: 0.9840 소요시간 260.3884177207947 . plt.figure(figsize=(12, 12)) plt.style.use(&#39;ggplot&#39;) plt.subplot(2,2,1) plt.plot(history.history[&#39;accuracy&#39;]) plt.plot(history.history[&#39;val_accuracy&#39;]) plt.title(&#39;Accuracy of the Model&#39;) plt.ylabel(&#39;Accuracy&#39;, fontsize=12) plt.xlabel(&#39;Epoch&#39;, fontsize=12) plt.legend([&#39;train accuracy&#39;, &#39;validation accuracy&#39;], loc=&#39;lower right&#39;, prop={&#39;size&#39;: 12}) plt.subplot(2,2,2) plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.title(&#39;Loss of the Model&#39;) . Text(0.5, 1.0, &#39;Loss of the Model&#39;) .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/25/crack.html",
            "relUrl": "/python/2022/10/25/crack.html",
            "date": " • Oct 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import numpy as np . 1/(1+np.exp(-1.67))*0.093 . 0.07826655136741911 .",
            "url": "https://pinkocto.github.io/BP2022/2022/10/25/Untitled.html",
            "relUrl": "/2022/10/25/Untitled.html",
            "date": " • Oct 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(221025) MNIST",
            "content": "- CNN의 기본 이해 - CNN을 실습을 통해 알아보기 . from IPython.display import display, Image import os, warnings warnings.filterwarnings(action=&#39;ignore&#39;) . import tensorflow as tf from tensorflow.keras import models from tensorflow.keras import layers # from tensorflow.keras import models import Sequential print(tf.__version__) . 2.10.0 . 이미지 - Conv - Pooling - Conv - Polling - FCL (filter)3x3 2x2 (f)3x3 2x2 32개 64개 . Conv: 3x3 필터, 32개의 필터개수, 입력 이미지(28,28,,1) | Maxpooling (2,2) | Conv: 3x3 필터, 64개의 필터개수 | Maxpooling (2,2) | Fully Conneted Layer | . model = models.Sequential() model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation=&#39;relu&#39;, input_shape=(28,28,1) )) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation=&#39;relu&#39; )) model.add(layers.MaxPooling2D((2, 2))) model.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 max_pooling2d_1 (MaxPooling (None, 13, 13, 32) 0 2D) conv2d_2 (Conv2D) (None, 11, 11, 64) 18496 max_pooling2d_2 (MaxPooling (None, 5, 5, 64) 0 2D) ================================================================= Total params: 18,816 Trainable params: 18,816 Non-trainable params: 0 _________________________________________________________________ . model.add( layers.Flatten() ) model.add( layers.Dense(64, activation=&#39;relu&#39;)) model.add( layers.Dense(10, activation=&#39;softmax&#39;)) . model.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 26, 26, 32) 320 max_pooling2d_1 (MaxPooling (None, 13, 13, 32) 0 2D) conv2d_2 (Conv2D) (None, 11, 11, 64) 18496 max_pooling2d_2 (MaxPooling (None, 5, 5, 64) 0 2D) flatten (Flatten) (None, 1600) 0 dense (Dense) (None, 64) 102464 dense_1 (Dense) (None, 10) 650 ================================================================= Total params: 121,930 Trainable params: 121,930 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.datasets import mnist from tensorflow.keras.utils import to_categorical (train_images, train_labels), (test_images, test_labels) = mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11490434/11490434 [==============================] - 1s 0us/step . train_images = train_images.reshape((60000, 28, 28, 1)) train_images = train_images.astype(&#39;float32&#39;) / 255 test_images = test_images.reshape((10000, 28, 28, 1)) test_images = test_images.astype(&#39;float32&#39;) / 255 . train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) . print(&quot;입력층 데이터(X) : &quot;,train_images.shape, test_images.shape ) print(&quot;출력층 데이터(y) : &quot;,train_labels.shape, test_labels.shape ) . 입력층 데이터(X) : (60000, 28, 28, 1) (10000, 28, 28, 1) 출력층 데이터(y) : (60000, 10) (10000, 10) . %%time model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=5, batch_size=64) . Epoch 1/5 938/938 [==============================] - 19s 20ms/step - loss: 0.1647 - accuracy: 0.9501 - val_loss: 0.0481 - val_accuracy: 0.9834 Epoch 2/5 938/938 [==============================] - 19s 21ms/step - loss: 0.0504 - accuracy: 0.9846 - val_loss: 0.0359 - val_accuracy: 0.9879 Epoch 3/5 938/938 [==============================] - 20s 21ms/step - loss: 0.0334 - accuracy: 0.9895 - val_loss: 0.0369 - val_accuracy: 0.9881 Epoch 4/5 938/938 [==============================] - 19s 20ms/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.0255 - val_accuracy: 0.9912 Epoch 5/5 938/938 [==============================] - 19s 20ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.0236 - val_accuracy: 0.9924 CPU times: total: 7min 15s Wall time: 1min 36s . &lt;keras.callbacks.History at 0x20e4747ed00&gt; . test_loss, test_acc = model.evaluate(test_images, test_labels) print(test_acc) . 313/313 [==============================] - 1s 4ms/step - loss: 0.0236 - accuracy: 0.9924 0.9923999905586243 . Conv &#44228;&#52789; &#52628;&#44032;&#54644;&#48372;&#44592; . model = models.Sequential() model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation=&#39;relu&#39;, input_shape=(28,28,1) )) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation=&#39;relu&#39; )) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation=&#39;relu&#39; )) model.add(layers.MaxPooling2D((2, 2))) model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_3 (Conv2D) (None, 26, 26, 32) 320 max_pooling2d_3 (MaxPooling (None, 13, 13, 32) 0 2D) conv2d_4 (Conv2D) (None, 11, 11, 64) 18496 max_pooling2d_4 (MaxPooling (None, 5, 5, 64) 0 2D) conv2d_5 (Conv2D) (None, 3, 3, 64) 36928 max_pooling2d_5 (MaxPooling (None, 1, 1, 64) 0 2D) ================================================================= Total params: 55,744 Trainable params: 55,744 Non-trainable params: 0 _________________________________________________________________ . model.add( layers.Flatten() ) model.add( layers.Dense(64, activation=&#39;relu&#39;)) model.add( layers.Dense(10, activation=&#39;softmax&#39;)) . model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_3 (Conv2D) (None, 26, 26, 32) 320 max_pooling2d_3 (MaxPooling (None, 13, 13, 32) 0 2D) conv2d_4 (Conv2D) (None, 11, 11, 64) 18496 max_pooling2d_4 (MaxPooling (None, 5, 5, 64) 0 2D) conv2d_5 (Conv2D) (None, 3, 3, 64) 36928 max_pooling2d_5 (MaxPooling (None, 1, 1, 64) 0 2D) flatten_1 (Flatten) (None, 64) 0 dense_2 (Dense) (None, 64) 4160 dense_3 (Dense) (None, 10) 650 ================================================================= Total params: 60,554 Trainable params: 60,554 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.datasets import mnist from tensorflow.keras.utils import to_categorical (train_images, train_labels), (test_images, test_labels) = mnist.load_data() . train_images = train_images.reshape((60000, 28, 28, 1)) train_images = train_images.astype(&#39;float32&#39;) / 255 test_images = test_images.reshape((10000, 28, 28, 1)) test_images = test_images.astype(&#39;float32&#39;) / 255 . train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) . print(&quot;입력층 데이터(X) : &quot;,train_images.shape, test_images.shape ) print(&quot;출력층 데이터(y) : &quot;,train_labels.shape, test_labels.shape ) . 입력층 데이터(X) : (60000, 28, 28, 1) (10000, 28, 28, 1) 출력층 데이터(y) : (60000, 10) (10000, 10) . %%time model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=5, batch_size=64) . Epoch 1/5 938/938 [==============================] - 20s 21ms/step - loss: 0.2627 - accuracy: 0.9182 - val_loss: 0.0972 - val_accuracy: 0.9697 Epoch 2/5 938/938 [==============================] - 21s 22ms/step - loss: 0.0785 - accuracy: 0.9762 - val_loss: 0.0790 - val_accuracy: 0.9775 Epoch 3/5 938/938 [==============================] - 22s 23ms/step - loss: 0.0541 - accuracy: 0.9832 - val_loss: 0.0549 - val_accuracy: 0.9833 Epoch 4/5 938/938 [==============================] - 22s 23ms/step - loss: 0.0432 - accuracy: 0.9871 - val_loss: 0.0490 - val_accuracy: 0.9853 Epoch 5/5 938/938 [==============================] - 21s 23ms/step - loss: 0.0343 - accuracy: 0.9896 - val_loss: 0.0620 - val_accuracy: 0.9818 CPU times: total: 7min 49s Wall time: 1min 45s . &lt;keras.callbacks.History at 0x20e4879fe20&gt; . test_loss, test_acc = model.evaluate(test_images, test_labels) print(test_acc) . 313/313 [==============================] - 1s 4ms/step - loss: 0.0620 - accuracy: 0.9818 0.9818000197410583 . 추가 전 : 0.9923999905586243 | 추가 후 : 0.9818000197410583 | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/25/DNN.html",
            "relUrl": "/python/2022/10/25/DNN.html",
            "date": " • Oct 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(22/10/24) 😎 Cross Validation",
            "content": "import time import gc import os, warnings import numpy as np # 경고 메시지 무시하거나 숨길때(ignore), 다시보이게(default) # warnings.filterwarnings(action=&#39;default&#39;) warnings.filterwarnings(action=&#39;ignore&#39;) . import mglearn mglearn.plots.plot_cross_validation() . from sklearn.datasets import load_iris from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score . 00. Base . iris = load_iris() logreg = LogisticRegression() . scores = cross_val_score(logreg, iris.data, iris.target) print(scores) . [0.96666667 1. 0.93333333 0.96666667 1. ] . 01. CV=10 . scores_cv = cross_val_score(logreg, iris.data, iris.target, cv=10) print(scores_cv) . [1. 0.93333333 1. 1. 0.93333333 0.93333333 0.93333333 1. 1. 1. ] . 02. Kfold (n_splits = 3) . iris.target . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . from sklearn.model_selection import KFold # 객체를 사용해서 넣어줄 수도 있다. kfold = KFold(n_splits = 3) print(&#39;교차 검증 점수 : n{}&#39;.format(cross_val_score(logreg, iris.data, iris.target, cv=kfold))) . 교차 검증 점수 : [0. 0. 0.] . 문제 발생! | shuffle=True 옵션을 지정해주지 않으면 앞에서부터 3등분 나눠진다. | 그렇게되면 첫번째 fold는 0만, 두번째 fold는 1만, 세번째 fold는 2만 있게되는데 | . kfold_random = KFold(n_splits = 3, shuffle=True, random_state=0) print(&#39;교차 검증 점수 : n{}&#39;.format(cross_val_score(logreg, iris.data, iris.target, cv=kfold_random))) . 교차 검증 점수 : [0.98 0.96 0.96] . 문제 해결! | . 03. Boston Houst Price . from sklearn.model_selection import cross_val_score from sklearn.datasets import load_boston from sklearn.linear_model import LinearRegression import sklearn import pandas as pd import mglearn print(sklearn.__version__) . 1.1.2 . boston = load_boston() df = pd.DataFrame(boston.data, columns=boston.feature_names) df[&#39;price&#39;] = boston.target print(df.shape) . (506, 14) . X = df.drop([&#39;price&#39;], axis=1) y = df[&#39;price&#39;] . cv=5 . lr_model = LinearRegression() msescores = cross_val_score(lr_model, X, y, scoring=&#39;neg_mean_squared_error&#39;, cv=5) . rmse = np.sqrt(-1 * msescores) print(rmse) . [3.52991509 5.10378498 5.75101191 8.9867887 5.77179405] . print(&#39;평균 RMSE : {0:.3f}&#39;.format(np.mean(rmse))) . 평균 RMSE : 5.829 . cv=10 . lr_model = LinearRegression() msescores = cross_val_score(lr_model, X, y, scoring=&#39;neg_mean_squared_error&#39;, cv=10) . rmse = np.sqrt(-1 * msescores) print(rmse) . [ 3.04744921 3.76181913 3.75148053 5.93354231 5.64669077 4.45374875 3.15392917 12.9759539 5.77319193 3.3106511 ] . print(&#39;평균 RMSE : {0:.3f}&#39;.format(np.mean(rmse))) . 평균 RMSE : 5.181 . cv를 $5 to10$으로 변경후 RMSE의 평균을 구한 결과 | 점수가 $5.829 to 5.181$로 떨어졌다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/24/Cross_Validation_Class_For_Amex.html",
            "relUrl": "/python/2022/10/24/Cross_Validation_Class_For_Amex.html",
            "date": " • Oct 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(OpenCV - Chap6) 화소(pixel)처리",
            "content": "&#54868;&#49548;&#51032; &#44060;&#45392; . 화소란 화면(영상)을 구성하는 가장 기본이 되는 단위를 말한다. 일반적으로 영상처리 입문에서 가장 먼저 다루는 내용이 화소값 기반 처리이다. 이것은 영상 구조에 대해 알기 위해 가장 먼저 이해해야 하는 것이 화소에 대한 기본 개념이기 때문이다. . 디지털 영상은 이 화소들의 집합을 의미하며, 이 화소들에 대해 다양한 연산을 하는 것이 영상처리이다. . 6.1 &#50689;&#49345;&#54868;&#49548;&#51032; &#51217;&#44540; . 영상처리를 아주 간단하게 말해보면, 2차원 데이터에 대한 행렬 연산이라고 할 수 있다. 따라서 영상을 다루려면 기본적으로 영상의 화소에 접근하고, 그 값을 수정하거나 새로 만들 수 있어야 한다. . 과거 OpenCV와 같은 대중적인 영상처리 API가 없었을 때, 영상 데이터를 처리하고 저장하는 것이 쉽지만은 않은 일이었다. 하지만 파이썬에서는 행렬 데이터 처리에 유용한 넘파이(Numpy) 라이브러리를 지원하고 있으며, OpenCV API도 numpy.ndarray 객체를 기반으로 영상 데이터를 처리한다. 6.1.1 &#54868;&#49548;(&#54665;&#47148; &#50896;&#49548;) &#51217;&#44540; . 다음은 모든 원소를 순회하여 원소값을 2배로 변경하는 예제이다. . - 방법1 . 행렬의 원소를 순회하며 직접 원소값을 가져와서 계산 . import numpy as np def mat_access1(mat): for i in range(mat.shape[0]): for j in range(mat.shape[1]): k = mat[i, j] mat[i, j] = k * 2 . mat1 = np.arange(10).reshape(2,5) mat1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . print(&#39;원소 처리 전: n%s n&#39; % mat1) mat_access1(mat1) print(&#39;원소 처리 후: n%s n&#39; % mat1) . 원소 처리 전: [[ 0 2 4 6 8] [10 12 14 16 18]] 원소 처리 후: [[ 0 4 8 12 16] [20 24 28 32 36]] . - 방법2 . 행렬 원소를 순회하며, ndarray 클래스의 내부 메서드인 item() 함수와 itemset() 함수로 가져와서 값을 변경 . def mat_access2(mat): for i in range(mat.shape[0]): for j in range(mat.shape[1]): k = mat.item(i, j) # mat.itemset((i, j), k*2) . mat2 = np.arange(10).reshape(2, 5) mat2 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . print(&#39;원소 처리 전: n%s n&#39; % mat2) mat_access2(mat2) print(&#39;원소 처리 후: n%s n&#39; % mat2) . 원소 처리 전: [[0 1 2 3 4] [5 6 7 8 9]] 원소 처리 후: [[ 0 2 4 6 8] [10 12 14 16 18]] . 6.1.2 &#50689;&#49345; &#48152;&#51204;&#51012; &#49688;&#54665;&#54616;&#45716; &#45796;&#50577;&#54620; &#48169;&#48277;&#46308; . 행렬을 처리하여 영상의 반전을 수행하는 다양한 방법들을 함수로 만들고, 각 방법의 수행속도를 계산해보자. . import numpy as np, cv2, time ## 화소 직접접근 def pixel_access1(image): image1 = np.zeros(image.shape[:2], image.dtype) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i,j] # 화소접근 image1[i, j] = 255 - pixel # 화소할당 return image1 . def pixel_access2(image): # item() 함수 접근 방법 image2 = np.zeros(image.shape[:2], image.dtype) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image.item(i, j) # 화소접근 image2.itemset((i, j), 255 - pixel) # 화소할당 return image2 . def pixel_access3(image): lut = [255 - i for i in range(256)] lut = np.array(lut, np.uint8) image3 = lut[image] return image3 . def pixel_access4(image): image4 = cv2.subtract(255, image) return image4 . def pixel_access5(image): image5 = 255 - image return image5 . image = cv2.imread(&#39;./ghtop_images/chap06_images/bright.jpg&#39;, cv2.IMREAD_GRAYSCALE) . image.shape . (450, 360) . def time_check(func, msg): start_time = time.perf_counter() ret_img = func(image) elapsed = (time.perf_counter() - start_time) * 1000 print(msg, &quot;수행시간 : %0.2f ms&quot; % elapsed ) return ret_img . image1 = time_check(pixel_access1, &quot;[방법1] 직접 접근 방식&quot;) image2 = time_check(pixel_access2, &quot;[방법2] item() 접근 방식&quot;) image3 = time_check(pixel_access3, &quot;[방법3] 룩업테이블 방식&quot;) image4 = time_check(pixel_access4, &quot;[방법4] OpenCV 함수 방식&quot;) image5 = time_check(pixel_access5, &quot;[방법5] ndarray 방식&quot;) . [방법1] 직접 접근 방식 수행시간 : 550.22 ms [방법2] item() 접근 방식 수행시간 : 108.73 ms [방법3] 룩업테이블 방식 수행시간 : 0.92 ms [방법4] OpenCV 함수 방식 수행시간 : 0.08 ms [방법5] ndarray 방식 수행시간 : 0.19 ms . 실행결과를 보면, OpenCV 또는 ndarray 방식으로 화소에 접근하는 경우 속도가 빠른 것을 확인할 수 있었다. . 따라서 화소 직접 접근 방법보다는 OpenCV에서 제공하는 함수들을 조합하거나 ndarray 객체의 원소간 연산으로 구현 내용을 만드는 것이 좋다. . 6.2 &#54868;&#49548; &#48157;&#44592; &#48320;&#54872; . 6.2.1 &#44536;&#47112;&#51060; &#49828;&#52992;&#51068; (&#47749;&#50516;&#46020;) &#50689;&#49345; . 일반적으로 이해하는 컬러가 아닌 영상을 우리는 흑백영상이라고 쉽게 부르지만, 엄밀한 의미에서 흑백 영상이라는 것은 검은색과 흰색으로 구성된 영상을 의미하기 때문에 단일채널 영상에 이 이름을 붙이는 것이 맞지 않을 수도 있다. . 디지털 영상처리에서 보통 단일채널의 영상을 그레이 스케일(gray-scale)영상 혹은 명암도 영상이라고 한다. . 그레이 스케일 영상 . 0~255의 값을 가지는 화소들이 모여서 구성된 영상 | 0은 검은색, 255는 흰색을 의미 | 0~255 사이 값들은 진한 회색에서 연한 회색까지를 나타냄 | . | . import numpy as np import cv2 . image1 = np.zeros((50,512), np.uint8) # 50x512 영상 생성 image2 = np.zeros((50,512), np.uint8) rows, cols = image1.shape[:2] for i in range(rows): for j in range(cols): image1.itemset((i,j), j//2) # 화소값 점진적 증가 image2.itemset((i,j), j // 20*10) # 계단 현상 증가 . cv2.imshow(&quot;image1&quot;, image1) cv2.imshow(&quot;image2&quot;, image2) cv2.waitKey(0) cv2.imwrite(&#39;./prac_image/image1_226.png&#39;, image1) # 이미지 저장 cv2.imwrite(&#39;./prac_image/image2_226.png&#39;, image2) # 이미지 저장 cv2.destroyAllWindows() . . - 실행결과 . image1 . 나눗셈 몫 연산자로 2로 나눈 몫을 저장하는 것은 가로 인덱스의 절반 값으로 j열 원소의 화소값을 설정한 것이다. 따라서 화소값은 왼쪽에서 오른쪽으로 0에서 255의 값까지 점진적으로 증가한다. | . image2 . (j // 20 * 10) 은 몫 연산자로 인해서 계산 값의 소수 부분은 날라간다. 따라서 20화소씩 ㅣ같은 값을 갖게 되어 계단 현상을 나타내며 증가한다. | . 6.2.2 &#50689;&#49345;&#51032; &#54868;&#49548; &#54364;&#54788; . 영상파일을 읽어 들여 그 영상의 특정 부분의 화소들을 확인해보자. 영상파일을 행렬에 저장하고, 관심 영역을 지정해서 출력하면 간단히 영상 데이터인 화소들의 값을 출력할 수 있다. . # 영상 화소값 확인 (pixel_value) import cv2 image = cv2.imread(&#39;./ghtop_images/chap06_images/pixel.jpg&#39;, cv2.IMREAD_GRAYSCALE) (x, y), (w, h) = (180, 37), (15, 10) roi_img = image[y:y+h, x:x+w] # 행은 시작 y좌표에서 y+h까지, 열은 시작 x좌표에서 x+w까지 #print(&quot;[roi img] = n&quot;, roi_img) . $(x, y)$는 사각형의 시작좌표 | $(w, h)$는 사각형의 크기 | 즉, 사각형의 시작좌표와 크기로 관심영역을 지정한다. | . print(&quot;[roi_img] =&quot;) for row in roi_img: for p in row: print(&quot;%4d&quot; % p, end=&quot;&quot;) print() . [roi_img] = 56 51 59 66 84 104 154 206 220 208 203 207 205 204 204 75 57 53 53 72 71 100 152 195 214 212 201 209 207 205 88 76 65 53 51 60 73 96 143 200 219 200 206 204 202 91 92 80 63 53 59 59 61 89 144 195 222 205 200 205 89 94 90 82 63 54 51 56 65 92 149 203 223 209 196 89 91 90 89 84 64 54 55 51 56 94 140 208 223 203 91 86 84 85 97 86 72 59 50 53 66 81 148 211 216 92 86 85 88 92 95 88 70 55 53 59 64 89 155 211 88 85 86 90 87 87 89 86 72 56 50 53 59 88 175 87 85 86 88 87 84 86 90 86 70 53 44 51 56 111 . cv2.rectangle(image, (x,y,w,h) , 255, 1) # 관심 영역에 사각형 표시 cv2.imshow(&quot;image&quot;, image) cv2.waitKey(0) cv2.imwrite(&#39;./prac_image/image_227.png&#39;, image) # 이미지 저장 cv2.destroyAllWindows() . . 실행 결과를 보면, 영상의 우상단에 흰색의 작은 사각형이 그려져 있다. 이 사각형이 관심 영역이며, 이 영역의 화소값과 비교해보자. | . print(&quot;[roi img] = n&quot;, roi_img) . [roi img] = [[ 56 51 59 66 84 104 154 206 220 208 203 207 205 204 204] [ 75 57 53 53 72 71 100 152 195 214 212 201 209 207 205] [ 88 76 65 53 51 60 73 96 143 200 219 200 206 204 202] [ 91 92 80 63 53 59 59 61 89 144 195 222 205 200 205] [ 89 94 90 82 63 54 51 56 65 92 149 203 223 209 196] [ 89 91 90 89 84 64 54 55 51 56 94 140 208 223 203] [ 91 86 84 85 97 86 72 59 50 53 66 81 148 211 216] [ 92 86 85 88 92 95 88 70 55 53 59 64 89 155 211] [ 88 85 86 90 87 87 89 86 72 56 50 53 59 88 175] [ 87 85 86 88 87 84 86 90 86 70 53 44 51 56 111]] . 관심영역 즉, 흰색 사각형이 그려져 있는 부분을 보면 주대각선 윗 부분은 흰색(밝은색)이고 아랫부분은 진한회색(어두운색)임을 알 수 있다. | 화소 값을 보면 주대각선 기준 윗부분은 화소값은 대략 $200 sim225$범위의 값을 나타내고, 그 아래부분은 대략 $50 sim80$범위의 값임을 확인 | 즉, 흰색부분은 화소값이 255와 가깝고, 어두운 부분은 0에 가까운 값을 갖는다. | . 6.2.3 &#50689;&#49345; &#48157;&#44592;&#51032; &#44032;&#44048;&#50689;&#49345; . 화소값이 영상의 밝기를 나타내기 때문에 이 화소값을 변경하면 영상의 밝기를 바꿀 수 있다. . 예를 들어 영상의 화소에 특정한 상숫값을 더하면 영상이 밝아지고, 상숫값을 빼면 영상이 어두워진다. | 또한, 화소가 가질 수 있는 최댓값(예로 255)에서 그 화소의 값을 빼면 반전 영상이 만들어진다. | . 6.2.4 &#54665;&#47148; &#45927;&#49480; &#48143; &#44273;&#49480;&#51012; &#51060;&#50857;&#54620; &#50689;&#49345; &#54633;&#49457; . 영상에 상수를 더하거나 빼는 연산을 확장하면 두 개의 영상을 더하거나 빼는 연산을 생각해 볼 수 있다. 두 영상을 합하면 영상 합성이 되며, 두 영상을 빼면 차영상(difference image)이 된다. . 다음은 알렉산더 대왕 동상 영상($A$)과 사도의 건물 영상($B$), 두 영상을 합성한 영상($A+B$)을 구하는 예제이다. . 🤔 문제발생&lt;/p&gt; 두 개의 행렬을 합하게 되면, saturation 연산으로 인해 255가 넘어가는 화소들은 흰색으로 나타나서 영상의 합성이 제대로 수행되지 않는다. . | 참고로 행렬의 덧셈과 뺄셈에서 OpenCV는 saturation 방식을 사용하고, numpy는 modulo 방식을 사용한다. . &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 😎 해결방법 1. $dst(y,x) = image1(y,x)*0.5 + image2(y,x)*0.5$ 2. $dst(y,x) = image1(y,x)* alpha + image2(y,x)*(1- alpha)$ 3. $dst(y,x) = image1(y,x)* alpha + image2(y,x)* beta$ &#54665;&#47148; &#54633;&#44284; &#44273; &#50672;&#49328;&#51012; &#53685;&#54620; &#50689;&#49345; &#54633;&#49457; . import numpy as np, cv2 image1 = cv2.imread(&#39;./ghtop_images/chap06_images/add1.jpg&#39;, cv2.IMREAD_GRAYSCALE) # 영상 읽기 image2 = cv2.imread(&#39;./ghtop_images/chap06_images/add2.jpg&#39;, cv2.IMREAD_GRAYSCALE) . alpha, beta = 0.6, 0.7 # 곱셈 비율 add_img1 = cv2.add(image1, image2) # 두 영상 단순 더하기 add_img2 = cv2.add(image1 * alpha, image2 * beta) # 두 영상 비율에 따른 더하기 add_img2 = np.clip(add_img2, 0, 255).astype(&#39;uint8&#39;) # saturation 처리 add_img3 = cv2.addWeighted(image1, alpha, image2, beta, 0) # 두 영상 비율에 따른 더하기 titles = [&#39;image1&#39;, &#39;image2&#39;,&#39;add_img1&#39;,&#39;add_img2&#39;,&#39;add_img3&#39;] # 윈도우 이름 for t in titles: cv2.imshow(t, eval(t)) # 영상 표시 cv2.waitKey(0) cv2.destroyAllWindows() . titles = [&#39;image1&#39;, &#39;image2&#39;,&#39;add_img1&#39;,&#39;add_img2&#39;,&#39;add_img3&#39;] eval(titles[1]) . array([[110, 122, 118, ..., 165, 166, 166], [143, 159, 168, ..., 165, 166, 166], [115, 117, 140, ..., 165, 166, 166], ..., [ 32, 41, 45, ..., 34, 32, 30], [ 27, 35, 40, ..., 110, 109, 108], [ 41, 36, 31, ..., 146, 148, 149]], dtype=uint8) . 파이썬 내장함수 eval()함수를 사용하면 리스트 원소의 문자열을 행렬 변수로 사용하여 행렬을 윈도우에 표시한다. | . &lt;/div&gt; | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/15/opencv.html",
            "relUrl": "/python/2022/10/15/opencv.html",
            "date": " • Oct 15, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(OpenCV - Chap5) 10월 13일(2)",
            "content": "5.3.1 &#49324;&#52825; &#50672;&#49328; (&#54665;&#47148; &#49328;&#49696; &#50672;&#49328;) . 사칙연산을 위한 OpenCV함수에 대해 알아보자. . cv2.add(src1, src2[, mask[, dtype]]]) -&gt; dst . 두 개의 배열 혹은 배열과 스칼라의 각 원소 간 합을 계산한다. 입력인수 src1, src2 중 하나는 스칼라값일 수 있다. . $dst(i) = saturate(src1(i) + src2(i)) quad text{if } mask(i) neq 0$ | $dst(i) = saturate(src1 + src2(i)) quad text{if } mask(i) neq 0$ | $dst(i) = saturate(src1(i) + src2) quad text{if } mask(i) neq 0$ | . | . | . cv2.addWeighted(src1, alpha1, src2, beta, gamma[,[dst[,dtype]]) -&gt; dst 두 배열의 각 원소에 가중치를 곱한 후에 각 원소 간 합 즉, 가중된(weighted) 합을 계산한다. | 수식: $dst(i) = saturate(src1(i) cdot alpha + src2(i) cdot beta + gamma)$ | . | . [ 참고 ] OpenCV에서 **saturate()** 는 0이하는 0으로, 255이상은 255로 범위를 한정시키는 연산이다. import numpy as np, cv2 m1 = np.full((3,6), 10, np.uint8) # 단일채널 생성 및 초기화 m2 = np.full((3,6), 50, np.uint8) m_mask = np.zeros(m1.shape, np.uint8) # 마스크 생성 m_mask[:,3:] = 1 # 관심 영역(모든행, 3열부터)을 지정한 후, 1을 할당 . . m1 . array([[10, 10, 10, 10, 10, 10], [10, 10, 10, 10, 10, 10], [10, 10, 10, 10, 10, 10]], dtype=uint8) . m2 . array([[50, 50, 50, 50, 50, 50], [50, 50, 50, 50, 50, 50], [50, 50, 50, 50, 50, 50]], dtype=uint8) . m_mask . array([[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1]], dtype=uint8) . - 행렬 덧셈 . m_add1 = cv2.add(m1, m2) m_add1 . array([[60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60]], dtype=uint8) . m_add2 = cv2.add(m1, m2, mask=m_mask) m_add2 . array([[ 0, 0, 0, 60, 60, 60], [ 0, 0, 0, 60, 60, 60], [ 0, 0, 0, 60, 60, 60]], dtype=uint8) . 마스크 영역 (관심영역)만 덧셈 연산이 된 것을 확인! | . - 행렬 나눗셈 . m_div1 = cv2.divide(m1, m2) m_div1 . array([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], dtype=uint8) . 전부 0으로 나온다? 0.2가 나와야 하는데?? (소수 부분이 상실되었다.) | . - 소수부분 소실 방지 . 행렬 원소 자료형을 32비트 실수형(np.float32)로 변환 . m1 = m1.astype(np.float32) # 소수부분 보존위해 행변환 m2 = np.float32(m2) # 형 변환 방법2 m_div2 = cv2.divide(m1, m2) m_div2 . array([[0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]], dtype=float32) . 소수부분 소실 문제가 잘 해결되었다. | . 5.3.2 &#51648;&#49688;, &#47196;&#44536;, &#51228;&#44273;&#44540; &#44288;&#47144; &#54632;&#49688; . import numpy as np, cv2 ## ndarray 생성 v1 = np.array([1,2,3], np.float32) # 1차원 리스트로 행렬 생성 v2 = np.array([[1],[2],[3]], np.float32) # 2차원 리스트 (3행, 1열) - 열벡터 v3 = np.array([[1,2,3]],np.float32) # 2차원 리스트 (1행, 3열) - 행벡터 . . v1 # 1차원 리스트로 행렬 생성 . array([1., 2., 3.], dtype=float32) . v2 # 2차원 리스트 (3행, 1열) - 열벡터 . array([[1.], [2.], [3.]], dtype=float32) . v3 # 2차원 리스트 (1행, 3열) - 행벡터 . array([[1., 2., 3.]], dtype=float32) . v_exp = cv2.exp(v1) # 1차원 행렬에 대한 지수 m_exp = cv2.exp(v2) # 행벡터 (1*3)에 대한 지수계산 m_exp = cv2.exp(v3) # 열벡터 (3*1)에 대한 지수 계산 v_log = cv2.log(v1) # 로그 계산 m_sqrt = cv2.sqrt(v2) # 제곱근 계산 m_pow = cv2.pow(v3, 3) # 3의 거듭제곱 계산 . . v_exp . array([[ 2.718282 ], [ 7.3890557], [20.085539 ]], dtype=float32) . np.array([[np.exp(1)], [np.exp(2)],[np.exp(3)]]) . array([[ 2.71828183], [ 7.3890561 ], [20.08553692]]) . 잘 계산되는구만! | . m_exp . array([[ 2.718282 , 7.3890557, 20.085539 ]], dtype=float32) . v_log . array([[0. ], [0.6931472], [1.0986123]], dtype=float32) . m_sqrt . array([[1. ], [1.4142135], [1.7320508]], dtype=float32) . m_pow . array([[ 1., 8., 27.]], dtype=float32) . &#54665;&#48289;&#53552;&#47484; &#50676;&#48289;&#53552;&#47196;, &#50676;&#48289;&#53552;&#47484; &#54665;&#48289;&#53552;&#47196; (Transpose) . print( v_log.T) . [[0. 0.6931472 1.0986123]] . print(m_sqrt.T) . [[1. 1.4142135 1.7320508]] . print(m_pow.T) . [[ 1.] [ 8.] [27.]] . 열벡터는 행벡터로, 행벡터는 열벡터로 변환하였다! | . 2&#52264;&#50896; &#54665;&#47148;&#51012; &#48289;&#53552;(1&#52264;&#50896;)&#47196; &#48320;&#54872; . np.ravel | .flatten() | . print(m_sqrt) . [[1. ] [1.4142135] [1.7320508]] . print(np.ravel(m_sqrt)) . [1. 1.4142135 1.7320508] . Numpy 모듈의 ravel() 함수를 이용해서 2차원 행렬을 벡터(1차원)으로 변환. | ravel() 함수는 리스트나 넘파이 배열뿐만 아니라 모든 다차원 배열을 벡터(1차원)로 변환할 수 있다. | . print(m_pow, type(m_pow)) print(m_pow.flatten(), type(m_pow.flatten())) . [[ 1. 8. 27.]] &lt;class &#39;numpy.ndarray&#39;&gt; [ 1. 8. 27.] &lt;class &#39;numpy.ndarray&#39;&gt; . ndarray 클래스의 내부 메소드인 flatten() 함수를 이용해서 벡터로 변환한다. | . &#54665;&#47148; &#53356;&#44592; &#48143; &#50948;&#49345; &#50672;&#49328; . 다음은 cv2.magnitude()와 cv2.phase() 함수의 예시이다. OpenCV 함수에서 연산의 결과가 실수값을 갖는 경우 대부분 입력 행렬 원소의 자료형도 np.float32형이여야 한다. . import numpy as np, cv2 . . x = np.array([1,2,3,5,10], np.float32) # 리스트로 ndarray 객체 생성 y = np.array([2,5,7,2,9]).astype(&#39;float32&#39;) # 행렬 생성 후 실수형 변환 . x . array([ 1., 2., 3., 5., 10.], dtype=float32) . y . array([2., 5., 7., 2., 9.], dtype=float32) . - 크기 계산 . mag = cv2.magnitude(x, y) mag . array([[ 2.236068 ], [ 5.3851647], [ 7.615773 ], [ 5.3851647], [13.453624 ]], dtype=float32) . - 각도(방향) 계산 . ang = cv2.phase(x, y) ang . array([[1.1071129], [1.1902124], [1.1658309], [0.3805839], [0.7329612]], dtype=float32) .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/14/operations_func.html",
            "relUrl": "/python/2022/10/14/operations_func.html",
            "date": " • Oct 14, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(OpenCV - Chap5) 10월 13일",
            "content": "05. &#44592;&#48376; &#48176;&#50676; &#50672;&#49328; &#54632;&#49688; . OpenCV는 수학과 과학 연산을 위한 파이썬 패키지인 넘파이(numpy)와 연동해 배열을 생성할 수 있으며, 이런 배열을 처리할 수 있는 다양한 연산함수를 지원한다. . 파이썬에서는 배열을 처리하기 위한 자료형으로 리스트, 튜플, 사전 등의 열거형(sequence) 객체가 있다. 리스트는 다차원의 배열을 만들고 원소를 수정할 수 있으며, 튜플은 다차원의 배열을 만들 수 있지만, 수정이 불가능한 자료형이다. OpenCV 모듈의 함수들은 넘파이 모듈의 배열(ndarray) 객체를 기반으로 입력 배열과 출력 배열을 사용한다. . 이 장에서는 OpenCV에서 지원하는 여러 배열 처리 함수들을 살펴본다. . 5.1 &#44592;&#48376; &#48176;&#50676; (Array) &#54632;&#49688; . OpenCV에서는 배열을 옵션에 따라 여러 방향으로 뒤집거나 여러 번 반복하는 등 배열 자체를 처리하는 함수를 제공하고 있다. . 다음 예제는 영상파일을 읽은 후, cv2.flip(), cv2.repeat, cv2.transpose() 함수를 활용해서 상하좌우로 뒤집는 예시이다. . import cv2 image = cv2.imread(&#39;./ghtop_images/chap05_images/flip_test.jpg&#39;, cv2.IMREAD_COLOR) if image is None: raise Exception(&quot;영상파일 읽기 오류 발생&quot;) # 예외 처리 . . - &#50896;&#48376; &#51060;&#48120;&#51648; . image = cv2.imshow(&#39;image&#39;, image) cv2.waitKey(0) cv2.destroyAllWindows() . . 원본 이미지 . . - x&#52629; &#44592;&#51456;&#51004;&#47196; &#46244;&#51665;&#51008; &#51060;&#48120;&#51648; . x_axis = cv2.flip(image, 0) # x축 기준 상하 뒤집기 cv2.imshow(&#39;x_axis&#39;, x_axis) cv2.waitKey(0) cv2.destroyAllWindows() . . x-axis flip . . - y&#52629; &#44592;&#51456;&#51004;&#47196; &#46244;&#51665;&#51008; &#51060;&#48120;&#51648; . y_axis = cv2.flip(image, 1) # y축 기준 좌우 뒤집기 cv2.imshow(&#39;y_axis&#39;, y_axis) cv2.waitKey(0) cv2.destroyAllWindows() . . y_axis flip . . - x, y&#52629; &#44592;&#51456; &#49345;&#54616;&#51340;&#50864; &#46244;&#51665;&#44592; . xy_axis = cv2.flip(image, -1) # 양축(x,y축) 기준 상하좌우 뒤집기 cv2.imshow(&#39;xy_axis&#39;, xy_axis) cv2.waitKey(0) cv2.destroyAllWindows() . . xy_axis flip . . - &#48373;&#49324;&#48376; &#47564;&#46308;&#44592; . cv.repeat(src, ny, nx[,dst[) -&gt; dst . src, dst : 입력, 출력 배열 . | ny, nx : 수직, 수평방향 반복 횟수 . | . | . 입력 배열의 반복된 복사본으로 출력 배열을 채운다. | . rep_image = cv2.repeat(image, 1, 2) # 반복 복사 cv2.imshow(&#39;rep_image&#39;, rep_image) cv2.waitKey(0) cv2.destroyAllWindows() . . repeat_image . . - &#51204;&#52824; &#51060;&#48120;&#51648; . 입력 행렬의 전치 행렬을 출력으로 반환한다. | . trans_image = cv2.transpose(image) # 행렬 전치 cv2.imshow(&#39;trans_image&#39;, trans_image) cv2.waitKey(0) cv2.destroyAllWindows() . . trans_image . . image.shape . . (267, 360, 3) . trans_image.shape . . (360, 267, 3) . $267 times 360 times 3 to 360 times 267 times 3$으로 전치된 것 확인! . 5.2 &#52292;&#45328; &#52376;&#47532; &#54632;&#49688; . 컬러 영상은 파란색(B), 녹색(G), 빨간색(R)의 각기 독립적인 2차원 정보를 합쳐 놓은 배열이라고 정의할 수 있다. 요즈음 영상처리 API에서는 컬러 영상을 표현하기 위해 채널(Channel)이라는 개념을 도입한다. 즉, 빨간색, 녹색, 파란색의 독립적인 2차원 정보는 각각 Blue채널, Green채널, Red 채널이라는 이름으로 표현된다. . 다음은 단일채널 행렬을 여러 개 합치거나, 다채널을 분리하는 등 채널을 처리하는 함수에 대한 설명이다. . 간단한 예제로 채널에 대한 개념을 알아보자. . 1. 단일채널 행렬 3개를 생성 | 2. 3개의 채널을 합쳐 하나의 다채널 행렬로 생성 | 3. 그 후 합쳐진 다채널 행렬을 다시 단일채널로 분리 | . import numpy as np import cv2 . . ## numpy.ndarray를 이용해 행렬 생성 및 초기화 방법 ch0 = np.zeros((2,4), np.uint8) + 10 # 0원소 행렬 선언 후 10 더하기 ch1 = np.ones((2,4), np.uint8) * 20 # 1원소 행렬 선언 후 20 곱하기 ch2 = np.full((2,4), 30, np.uint8) # 행렬을 생성하며 30으로 초기화 list_bgr = [ch0, ch1, ch2] # 단일채널 행렬들을 모아 리스트 구성 merge_bgr = cv2.merge(list_bgr) # 채널 합성 split_bgr = cv2.split(merge_bgr) # 채널 분리 : 컬러영상 &gt; 3채널 분리 . . ch0 . . array([[10, 10, 10, 10], [10, 10, 10, 10]], dtype=uint8) . ch1 . . array([[20, 20, 20, 20], [20, 20, 20, 20]], dtype=uint8) . ch2 . . array([[30, 30, 30, 30], [30, 30, 30, 30]], dtype=uint8) . # 단일 채널 행렬 3개 (ch0, ch1, ch2) list_bgr . . [array([[10, 10, 10, 10], [10, 10, 10, 10]], dtype=uint8), array([[20, 20, 20, 20], [20, 20, 20, 20]], dtype=uint8), array([[30, 30, 30, 30], [30, 30, 30, 30]], dtype=uint8)] . print(&#39;merge_bgr 행렬 형태: &#39;, merge_bgr.shape) print(&#39; &#39;) print(merge_bgr) . . merge_bgr 행렬 형태: (2, 4, 3) [[[10 20 30] [10 20 30] [10 20 30] [10 20 30]] [[10 20 30] [10 20 30] [10 20 30] [10 20 30]]] . print(&#39;split_bar 행렬 형태: &#39;, np.array(split_bgr).shape) # numpy의 shape() 함수를 적용하기 위해 ndarray객체로 변경하여 행렬 형태로 출력 print(&#39; &#39;) print(split_bgr[0]) print(&#39; &#39;) print(split_bgr[1]) print(&#39; &#39;) print(split_bgr[2]) . . split_bar 행렬 형태: (3, 2, 4) [[10 10 10 10] [10 10 10 10]] [[20 20 20 20] [20 20 20 20]] [[30 30 30 30] [30 30 30 30]] . 2열 3행 깊이가 4 | . split_bgr . . (array([[10, 10, 10, 10], [10, 10, 10, 10]], dtype=uint8), array([[20, 20, 20, 20], [20, 20, 20, 20]], dtype=uint8), array([[30, 30, 30, 30], [30, 30, 30, 30]], dtype=uint8)) . np.array(split_bgr) . . array([[[10, 10, 10, 10], [10, 10, 10, 10]], [[20, 20, 20, 20], [20, 20, 20, 20]], [[30, 30, 30, 30], [30, 30, 30, 30]]], dtype=uint8) . &#50696;&#51228; &#49892;&#49845; . import cv2 . . image = cv2.imread(&#39;./ghtop_images/chap05_images/color.jpg&#39;, cv2.IMREAD_COLOR) # 영상 읽기 if image is None: raise Exception(&#39;영상파일 읽기 오류&#39;) # 예외처리 if image.ndim != 3: raise Exception(&quot;컬러 영상 아님&quot;) # 예외 처리 - 컬러 영상 확인 . bgr = cv2.split(image) # blue, green, red = cv2.split(image) ## 3개 변수로 반환받기 가능! print(&#39;bgr 자료형:&#39;,type(bgr), type(bgr[0]), type(bgr[0][0][0])) . bgr 자료형: &lt;class &#39;tuple&#39;&gt; &lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;numpy.uint8&#39;&gt; . ## 각 채널을 윈도우에 띄우기 cv2.imshow(&#39;image&#39;, image) cv2.imshow(&#39;Blue chnnel&#39;, bgr[0]) cv2.imshow(&#39;Green chnnel&#39;, bgr[1]) cv2.imshow(&#39;Red chnnel&#39;, bgr[2]) cv2.waitKey(0) cv2.destroyAllWindows() . . original image vs. Blue channel . Blue Channel 아래쪽 파란색 마크 부분이 Blue channel에서는 밝게 나타난다. | . original image vs. Green channel . 원본이미지 아래쪽에 녹색을 띄는 진열대 부분이 Green Channel에서 밝게 나타난다. | . original image vs. red channel . 원본이미지의 왼쪽 냉장 전시물의 붉은쪽 문 부분이 Red Chnnel에서 밝게 나타난다. | . 5.3.0 &#49328;&#49696; &#50672;&#49328; &#54632;&#49688; . 행렬연산은 주로 첫 번째 배열의 i번째 원소와 두 번째 배열의 i번째 원소 간에 연산을 수행해서 결과 배열의 i번째 원소에 저장하는 방식을 취한다. 이러한 방식을 원소간 (per-element, element-wise) 연산이라 한다. . 5.3.1 &#49324;&#52825;&#50672;&#49328; . OpenCV에서 배열에 대한 사칙 연산은 두 배열의 원소간(per-element) 연산을 수행한다. . 5.3.2 &#51648;&#49688;, &#47196;&#44536;, &#51228;&#44273;&#44540; &#44288;&#47144; &#54632;&#49688; . OpenCV는 배열 원소의 지수와 로그 및 제곱근 관련 함수를 지원한다. .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/10/13/operatios_func.html",
            "relUrl": "/python/2022/10/13/operatios_func.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "OpenCV intro",
            "content": "ref: https://www.youtube.com/watch?v=F2FRpmh9sQo . &#51060;&#48120;&#51648; &#51069;&#50612;&#49436; &#49332;&#54196;&#48372;&#44592; . 01. cv2.imread(file_name, flag) . cv2.imread(file_name, flag) : 이미지를 읽어 Numpy 객체로 만드는 함수 file_name : 읽고자 하는 이미지 파일 | flag : 이미지를 읽는 방법 설정 . IMREAD_COLOR : 이미지를 Color로 읽고, 투명한 부분은 무시 | IMREAD_GRAYSCALE : 이미지를 Grayscale로 읽기 | IMREAD_UNCHANGED : 이미지를 Color로 읽고, 투명한 부분도 읽기 (Alpha) | . | 반환 값 : Numpy 객체 (행, 열, 색상: 기본 BGR) . | . 02. cv2. imshow(title, image) . cv2.imshow(title, image) : 특정한 이미지를 화면에 출력 title: 윈도우 창의 제목 | image : 출력할 이미지 객체 | . 03. cv2.imwrite(file_name, image) . cv2.imwrite(file_name, image) : 특정한 이미지를 파일로 저장하는 함수 file_name : 저장할 이미지 파일 이름 | image : 저장할 이미지 객체 | . 04. cv2.waitKey(time) . cv2.waitKey(time) : 키보드 입력을 처리하는 함수 time : 입력 대기 시간 (무한대기: 0) . | 반환 값: 사용자가 입력한 Ascii Code (ESC: 27) . | . 05. cv2.destroyAllWindows() . cv2.destroyAllWindows() : 화면의 모든 윈도우를 닫는 함수 import cv2 img_basic = cv2.imread(&#39;./ghtop_images/chap05_images/color.jpg&#39;, cv2.IMREAD_COLOR) cv2.imshow(&#39;Image Basic&#39;, img_basic) cv2.waitKey(0) cv2.imwrite(&#39;./prac_image/img_basic.png&#39;, img_basic) # 이미지 저장 cv2.destroyAllWindows() . . img_gray = cv2.cvtColor(img_basic, cv2.COLOR_BGR2GRAY) cv2.imshow(&quot;Image Gray&quot;, img_gray) cv2.waitKey(0) cv2.imwrite(&#39;./prac_image/img_gray.png&#39;, img_gray) # 이미지 저장 cv2.destroyAllWindows() . .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/09/01/opencv-intro.html",
            "relUrl": "/python/2022/09/01/opencv-intro.html",
            "date": " • Sep 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(1주차 ML2) 4월 28일",
            "content": "",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/28/ML.html",
            "relUrl": "/python/2022/04/28/ML.html",
            "date": " • Apr 28, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "OpenCV",
            "content": "https://www.youtube.com/watch?v=XK3eU9egll8 . &#54872;&#44221; &#49444;&#51221; . Anaconda prompt에서 다음 명령 수행 . pip install opencv-pythoon . import cv2 cv2.__version__ . &#39;4.5.5&#39; . OpenCV (Computer Vision) . 다양한 영상 (이미지) / 동영상 처리에 사용되는 오픈소스 라이브러리 . 1. &#51060;&#48120;&#51648; &#52636;&#47141; . import cv2 img = cv2.imread(&#39;./my_icons/img.jpg&#39;) # 해당 경로의 파일 읽어오기 cv2.imshow(&#39;img&#39;, img) # img 라는 이름의 창에 img를 표시 cv2.waitKey(5000) # 지정된 시간(ms) 동안 사용자 키 입력 대기 print(key) cv2.destroyAllWindows() # 모든 창 닫기 . 98 . &#51069;&#44592; &#50741;&#49496; . cv2.IMREAD_COLOR : 컬러 이미지,. 투명 영역은 무시 (기본값) | cv2.IMREAD_GRAYSCALE : 흑백이미지 | cv2.IMREAD_UNCHANGED : 투명 영엳까지 포함 | import cv2 img_color = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_COLOR) img_gray = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_GRAYSCALE) img_unchanged = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_UNCHANGED) cv2.imshow(&#39;img_color&#39;, img_color) cv2.imshow(&#39;img_gray&#39;, img_gray) cv2.imshow(&#39;img_unchanged&#39;, img_unchanged) cv2.waitKey(0) cv2.destroyAllWindows() . Shape . 이미지의 height, width, channel 정보 . import cv2 img = cv2.imread(&#39;./my_icons/img.jpg&#39;) img.shape # 세로, 가로, Channel . (390, 640, 3) . 2. &#46041;&#50689;&#49345; &#52636;&#47141; . &#46041;&#50689;&#49345; &#54028;&#51068; &#52636;&#47141; . import cv2 cap = cv2.VideoCapture(&#39;./my_icons/video.mp4&#39;) while cap.isOpened(): # 동영상 파일이 올바로 열렸는지? ret, frame = cap.read() # ret : 성공 여부, frame : 받아온 이미지 (프레임) if not ret: print(&#39;더 이상 가져올 프레임이 없어요&#39;) break cv2.imshow(&#39;video&#39;, frame) if cv2.waitKey(1) == ord(&#39;q&#39;): print(&#39;사용자 입력에 의해 종료합니다&#39;) break cap.release() # 자원 해제 cv2.destroyAllWindows() # 모든 창 닫기 . 더 이상 가져올 프레임이 없어요 . &#52852;&#47700;&#46972; &#52636;&#47141; . import cv2 cap = cv2.VideoCapture(0) # 0번째 카메라 장치 (Device ID) if not cap.isOpened(): # 카메라가 잘 열리지 않은 경우 exit() # 프로그램 종료 while True: ret, frame = cap.read() if not ret: break cv2.imshow(&#39;camera&#39;, frame) if cv2.waitKey(1) == ord(&#39;q&#39;): # 사용자가 q를 입력하면 break cap.release() cv2.destroyAllWindows() . 3. &#46020;&#54805; &#44536;&#47532;&#44592; . &#48712; &#49828;&#52992;&#52824;&#48513; &#47564;&#46308;&#44592; . import cv2 import numpy as np # 세로 480 X 가로 640, 3 Channel (RGB) 에 해당하는 스케치북 만들기 img = np.zeros((480, 640, 3), dtype = np.uint8) # img[:] = (255, 255, 255) # 전체 공간을 흰색으로 채우기 (B,G,R) # print(img) cv2.imshow(&#39;img&#39;, img) cv2.waitKey(0) cv2.destroyAllWindows() .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/26/opencv.html",
            "relUrl": "/python/2022/04/26/opencv.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "(1주차 ML2) 4월 26일",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import matplotlib.pyplot as plt ## 파이썬 내에서 그래프 출력시 디테일한 옵션 import matplotlib as mpl ## 한글폰트 설정, 글씨체 흐릿한 것을 선명하게 (전체적인 큰 틀의 옵션) . mpl.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) ## 맑은 고딕 . 1. Data . 위스콘신 유방암 데이터(Wisconsin Breast Cancer data)를 분석해보자. . 분석의 목적은 30개의 설명변수를 사용해 진단값이 악성인지 양성인지 예측하는 것입니다. . 1.1 Data Load . from sklearn.datasets import load_breast_cancer . cancer = load_breast_cancer() . cancer[&quot;feature_names&quot;] . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . Variable name Description . radius | 반지름 | . texture | 그레이스케일 값의 표준편차 | . perimeter | 둘레 | . area | 면적 | . smoothness | 반지름의 국소적 변화정도(local variation) | . compactness | $ frac{ text{perimeter}^2}{area}-1.0$ | . concavity | 오목한 정도(severity of concave portions of the contour) | . concave_points | 오목한 점들의 개수(number of concave portions of contour) | . symmetry | 대칭도 | . fractal dimension | 프랙탈 차원($ text{&quot;coastline approximation&quot;} - 1$) | . cancer[&quot;target_names&quot;] . array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;) . malignant : 악성 ($0$) | benign : 양성 ($1$) | . - 데이터와 정답을 확인해보자. . data, target = cancer[&quot;data&quot;], cancer[&quot;target&quot;] . data . array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01, 1.189e-01], [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01, 8.902e-02], [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01, 8.758e-02], ..., [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01, 7.820e-02], [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01, 1.240e-01], [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01, 7.039e-02]]) . target . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]) . 1.2 EDA . df = pd.DataFrame(data, columns=cancer[&quot;feature_names&quot;]) df.describe() . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . count 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | ... | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | . mean 14.127292 | 19.289649 | 91.969033 | 654.889104 | 0.096360 | 0.104341 | 0.088799 | 0.048919 | 0.181162 | 0.062798 | ... | 16.269190 | 25.677223 | 107.261213 | 880.583128 | 0.132369 | 0.254265 | 0.272188 | 0.114606 | 0.290076 | 0.083946 | . std 3.524049 | 4.301036 | 24.298981 | 351.914129 | 0.014064 | 0.052813 | 0.079720 | 0.038803 | 0.027414 | 0.007060 | ... | 4.833242 | 6.146258 | 33.602542 | 569.356993 | 0.022832 | 0.157336 | 0.208624 | 0.065732 | 0.061867 | 0.018061 | . min 6.981000 | 9.710000 | 43.790000 | 143.500000 | 0.052630 | 0.019380 | 0.000000 | 0.000000 | 0.106000 | 0.049960 | ... | 7.930000 | 12.020000 | 50.410000 | 185.200000 | 0.071170 | 0.027290 | 0.000000 | 0.000000 | 0.156500 | 0.055040 | . 25% 11.700000 | 16.170000 | 75.170000 | 420.300000 | 0.086370 | 0.064920 | 0.029560 | 0.020310 | 0.161900 | 0.057700 | ... | 13.010000 | 21.080000 | 84.110000 | 515.300000 | 0.116600 | 0.147200 | 0.114500 | 0.064930 | 0.250400 | 0.071460 | . 50% 13.370000 | 18.840000 | 86.240000 | 551.100000 | 0.095870 | 0.092630 | 0.061540 | 0.033500 | 0.179200 | 0.061540 | ... | 14.970000 | 25.410000 | 97.660000 | 686.500000 | 0.131300 | 0.211900 | 0.226700 | 0.099930 | 0.282200 | 0.080040 | . 75% 15.780000 | 21.800000 | 104.100000 | 782.700000 | 0.105300 | 0.130400 | 0.130700 | 0.074000 | 0.195700 | 0.066120 | ... | 18.790000 | 29.720000 | 125.400000 | 1084.000000 | 0.146000 | 0.339100 | 0.382900 | 0.161400 | 0.317900 | 0.092080 | . max 28.110000 | 39.280000 | 188.500000 | 2501.000000 | 0.163400 | 0.345400 | 0.426800 | 0.201200 | 0.304000 | 0.097440 | ... | 36.040000 | 49.540000 | 251.200000 | 4254.000000 | 0.222600 | 1.058000 | 1.252000 | 0.291000 | 0.663800 | 0.207500 | . 8 rows × 30 columns . cancer[&quot;feature_names&quot;] . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . 양성과 악성의 비율은 다음과 같다. . pd.Series(cancer[&quot;target&quot;]).value_counts() . 1 357 0 212 dtype: int64 . 종양진단 결과를 나타내는 class 변수의 도수분포로부터 357 명의 관측치가 양성(benign, class=0)에 해당하고, 이보다 적은 212명의 관측치가 악성(malign, class=1)에 해당함을 알 수 있습니다. . sns.countplot(x=target) plt.title(&quot;종양진단 class별 도수분포&quot;) plt.xlabel(&quot;class&quot;) plt.ylim([0, 450]) plt.text(-0.1, 220, &quot;악성&quot;) plt.text(.9, 370, &quot;양성&quot;) plt.show() . sns.boxplot(x=target, y=df[&quot;mean concave points&quot;]) plt.xlabel(&quot;class&quot;) . Text(0.5, 0, &#39;class&#39;) . 악성 종양세포에서 mean_concave_points 값이 훨씬 높은 편임을 알 수 있습니다. | . sns.boxplot(x=target, y=df[&quot;mean radius&quot;]) . &lt;AxesSubplot:ylabel=&#39;mean radius&#39;&gt; . 악성 종양세포에서 mean_radius 값이 훨씬 높은 편임을 알 수 있습니다. | . plt.scatter(x=df[&quot;mean concave points&quot;], y=df[&quot;mean radius&quot;], alpha=.5) plt.xlabel(&quot;mean_concave_points&quot;) plt.ylabel(&quot;mean_radius&quot;) . Text(0, 0.5, &#39;mean_radius&#39;) . 위의 그림은 mean_concave_points와 mean_radius 변수 사이의 강한 양의 상관관계가 있음을 보여줍니다. | . 1.3 Data Split . 데이터를 $7:3$의 비율로 train/test set으로 나누자 . from sklearn.model_selection import train_test_split train_data, test_data, train_target, test_target = train_test_split( data, target, train_size= 0.7, random_state=1001, ) . print(&quot;train data 개수:&quot;, len(train_data)) print(&quot;test data 개수:&quot;, len(test_data)) . train data 개수: 398 test data 개수: 171 . 2. Linear Regression and Categorical Label . Logistic Regression을 학습하기에 앞서 Linear Regression으로 학습할 경우 어떻게 되는지 보자. . from sklearn.linear_model import LinearRegression linear_regressor = LinearRegression() . 2.1 &#54617;&#49845; . linear_regressor.fit(train_data, train_target) . LinearRegression() . 2.2 &#50696;&#52769; . train_pred = linear_regressor.predict(train_data) test_pred = linear_regressor.predict(test_data) . 예측 결과를 보면 $0 sim 1$ 사이를 벗어난 예측값이 보이는데..? | 일단 넘어가자.. | . 2.3 &#49884;&#44033;&#54868; . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5)) preds = [ (&quot;Train&quot;, train_data, train_pred), (&quot;Test&quot;, test_data, test_pred), ] for idx, (name, d, pred) in enumerate(preds): ax = axes[idx] ax.scatter(x=d[:,0], y=pred) ax.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.set_xlabel(&#39;mean_radius&#39;) ax.set_ylabel(&#39;predict&#39;) ax.set_title(f&quot;{name} Data&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 2.4 &#54217;&#44032;&#54616;&#44592; . Linear Regression의 성능을 측정하기 위해서는 우선 예측값을 0과 1로 변환시켜줘야 합니다. . Youden&#39;s Index를 이용해 Best Threshold를 찾은 후, 0과 1로 변화시킨 후 정확도를 비교해보자. . ($ star$)Youden&#39;s J statistic . 참고링크: https://en.wikipedia.org/wiki/Youden%27s_J_statistic . $$ J = text{sensitivity} + text{specificity} - 1$$ . $$ J = frac{ text{True positives}}{ text{True positives}+ text{False negatives}} + frac{ text{True negatives}}{ text{True negatives}+ text{False positives}} - 1$$ . . from sklearn.metrics import auc, roc_curve fpr, tpr, threshold = roc_curve(train_target, train_pred) auroc = auc(fpr, tpr) . fpr . array([0. , 0. , 0. , 0.00699301, 0.00699301, 0.01398601, 0.01398601, 0.02097902, 0.02097902, 0.02797203, 0.02797203, 0.04195804, 0.04195804, 0.06993007, 0.06993007, 0.12587413, 0.12587413, 1. ]) . tpr . array([0. , 0.00392157, 0.70980392, 0.70980392, 0.90980392, 0.90980392, 0.93333333, 0.93333333, 0.96862745, 0.96862745, 0.98431373, 0.98431373, 0.99215686, 0.99215686, 0.99607843, 0.99607843, 1. , 1. ]) . threshold . array([ 2.35725299, 1.35725299, 0.82521489, 0.82466702, 0.70119883, 0.69924511, 0.67546196, 0.67513605, 0.63707749, 0.63401065, 0.61560836, 0.59241528, 0.58134679, 0.50969629, 0.50796669, 0.43013177, 0.42764525, -0.53936311]) . AUROC를 그려보자. . plt.plot(fpr, tpr) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . AUROC 값을 계산하면 다음과 같습니다. . print(f&quot;AUROC : {auroc: .4f}&quot;) . AUROC : 0.9960 . 이제 Best Threshold를 계산해보자. . np.argmax(tpr - fpr) . 10 . 10인 index 에서 Best Threshold를 갖는다는 의미 | . J = tpr - fpr idx = np.argmax(J) best_thresh = threshold[idx] print(f&quot;Best Threshold is {best_thresh:.4f}&quot;) print(f&quot;Best Threshold&#39;s sensitivity is {tpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s specificity is {1-fpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s J is {J[idx]:.4f}&quot;) . Best Threshold is 0.6156 Best Threshold&#39;s sensitivity is 0.9843 Best Threshold&#39;s specificity is 0.9720 Best Threshold&#39;s J is 0.9563 . Best Threshold는 AUROC 그래프에서 직선이 가장 긴 곳입니다. . plt.plot(fpr, tpr) plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10)) plt.plot((fpr[idx], fpr[idx]), (fpr[idx], tpr[idx]), color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) plt.show() . 예측값에서의 Best threshold 의 위치를 그려보자 . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) preds = [ (&quot;Train&quot;, train_data, train_pred), (&quot;Test&quot;, test_data, test_pred), ] for idx, (name, d, pred) in enumerate(preds): ax = axes[idx] ax.scatter(x=d[:,0], y=pred) ax.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.set_xlabel(&quot;mean_radius&quot;) ax.set_ylabel(&quot;predict&quot;) ax.set_title(f&quot;{name} Data&quot;) ax.axhline(best_thresh, color=&quot;blue&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 이제 Threshold로 예측값을 $0,1$로 변환 후 정확도를 보자 . train_pred_label = list(map(int, (train_pred &gt; best_thresh))) test_pred_label = list(map(int, (test_pred &gt; best_thresh))) . from sklearn.metrics import accuracy_score linear_train_accuracy = accuracy_score(train_target, train_pred_label) linear_test_accuracy = accuracy_score(test_target, test_pred_label) . print(f&quot;Train accuracy is : {linear_train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {linear_test_accuracy:.2f}&quot;) . Train accuracy is : 0.98 Test accuracy is : 0.96 . 3. Logistic Regression . 이번에는 Logistic Regression을 이용하여 예측해 보자. . 3.1 Scaling . Logistic Regression은 학습하기에 앞서 학습시킬 데이터를 정규화해야 합니다. . Logistic Regressiond에는 exp가 있는데, exp는 값이 클 경우 overflow가 일어날 수 있기 때문입니다. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() . 정규화는 항상 train data를 이용하여 학습하고 valid, test 데이터를 변환해야 합니다. . 모든 데이터를 한번에 학습할 경우 본적이 없는 valiation data의 평균과 분산이 반영되고 이는 overfitting을 일으키는 원인이 됩니다. . scaler.fit(train_data) . StandardScaler() . 학습된 Scaler로 train/test 데이터를 변환합니다. . scaled_train_data = scaler.transform(train_data) scaled_test_data = scaler.transform(test_data) . train_data[0] . array([1.953e+01, 1.890e+01, 1.295e+02, 1.217e+03, 1.150e-01, 1.642e-01, 2.197e-01, 1.062e-01, 1.792e-01, 6.552e-02, 1.111e+00, 1.161e+00, 7.237e+00, 1.330e+02, 6.056e-03, 3.203e-02, 5.638e-02, 1.733e-02, 1.884e-02, 4.787e-03, 2.593e+01, 2.624e+01, 1.711e+02, 2.053e+03, 1.495e-01, 4.116e-01, 6.121e-01, 1.980e-01, 2.968e-01, 9.929e-02]) . scaled_train_data[0] . array([ 1.55665013, -0.08374209, 1.56894905, 1.61738288, 1.35230219, 1.15579113, 1.64920637, 1.53999266, -0.05508639, 0.36872483, 2.57200558, -0.10081561, 2.13099893, 1.96746196, -0.29563095, 0.3786633 , 0.72463661, 0.88545529, -0.19263957, 0.37716933, 2.07668666, 0.10014731, 1.96594854, 2.15156304, 0.75699447, 1.00247978, 1.60389716, 1.3188727 , 0.1245053 , 0.85035853]) . 3.2 &#54617;&#49845; . 이제 표준화된 데이터로 Logistic Regression을 학습해 보자. . from sklearn.linear_model import LogisticRegression logit_regressor = LogisticRegression() . logit_regressor.fit(scaled_train_data, train_target) . LogisticRegression() . 3.3 &#50696;&#52769; . Classification을 하는 모델의 경우 예측을 하는 방법은 두가지가 있습니다. . predict . | predict_proba . | predict는 해당 데이터가 어떤 class로 분류할지 바로 알려줍니다. . 반면, predict_proba는 각 class에 속할 확률을 보여줍니다. . train_pred = logit_regressor.predict(scaled_train_data) test_pred = logit_regressor.predict(scaled_test_data) . train_pred[:10] . array([0, 1, 0, 0, 1, 0, 1, 0, 0, 1]) . train_pred_logit = logit_regressor.predict_proba(scaled_train_data) test_pred_logit = logit_regressor.predict_proba(scaled_test_data) . train_pred_logit[:10] . array([[9.99999984e-01, 1.62885674e-08], [1.30750892e-03, 9.98692491e-01], [9.93452400e-01, 6.54760017e-03], [6.39996411e-01, 3.60003589e-01], [5.71378493e-05, 9.99942862e-01], [9.96253495e-01, 3.74650484e-03], [5.02851011e-04, 9.99497149e-01], [9.95986445e-01, 4.01355535e-03], [9.99998296e-01, 1.70356931e-06], [7.13730423e-04, 9.99286270e-01]]) . 각 class에 속할 확률은 다음과 같습니다. . 현재 데이터의 경우 악성과 양성 2개의 클래스가 있기 때문에 2개의 확률이 나타납니다. . 만약 첫 번째 class에 속할 확률이 크다면 데이터는 0번 클래스에 속하게 되는 것..! . train_pred_logit[0] . array([9.99999984e-01, 1.62885674e-08]) . 3.4 &#54217;&#44032; . 데이터의 AUROC를 계산하기 위해서는 1의 클래스로 분류될 확률 하나만 필요합니다. 반면 우리가 갖고 있는 예측값은 0과 1로 분류될 확률을 모두 표시하고 있습니다. 그래서 1에 속할 확률만 남기겠습니다. . train_pred_logit = train_pred_logit[:, 1] test_pred_logit = test_pred_logit[:, 1] . train_pred_logit[0] . 1.628856736535896e-08 . from sklearn.metrics import auc, roc_curve fpr, tpr, threshold = roc_curve(train_target, train_pred_logit) auroc = auc(fpr, tpr) . plt.plot(fpr, tpr) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . print(f&quot;AUROC : {auroc:.4f}&quot;) . AUROC : 0.9971 . J = tpr - fpr idx = np.argmax(J) best_thresh = threshold[idx] print(f&quot;Best Threshold is {best_thresh:.4f}&quot;) print(f&quot;Best Threshold&#39;s sensitivity is {tpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s specificity is {1-fpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s J is {J[idx]:.4f}&quot;) . Best Threshold is 0.5692 Best Threshold&#39;s sensitivity is 0.9961 Best Threshold&#39;s specificity is 0.9790 Best Threshold&#39;s J is 0.9751 . plt.plot(fpr, tpr) plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10)) plt.plot((fpr[idx],fpr[idx]), (fpr[idx], tpr[idx]), color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . plt.scatter(x=scaled_train_data[:,0], y=train_pred_logit) plt.axhline(best_thresh, color=&quot;blue&quot;) plt.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;mean radius&quot;) plt.ylabel(&quot;Probability&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 이제 Threshold로 예측값을 0,1로 변환 후 정확도를 보겠습니다. . train_pred_label = list(map(int, (train_pred_logit &gt; best_thresh))) test_pred_label = list(map(int, (test_pred_logit &gt; best_thresh))) . proba_train_accuracy = accuracy_score(train_target, train_pred_label) proba_test_accuracy = accuracy_score(test_target, test_pred_label) . print(f&quot;Train accuracy is : {proba_train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {proba_test_accuracy:.2f}&quot;) . Train accuracy is : 0.99 Test accuracy is : 0.98 . 이번에는 predict의 결과값으로 정확도를 보겠습니다. . train_accuracy = accuracy_score(train_target, train_pred) test_accuracy = accuracy_score(test_target, test_pred) . print(f&quot;Train accuracy is : {train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {test_accuracy:.2f}&quot;) . Train accuracy is : 0.99 Test accuracy is : 0.97 . predict_proba의 best_threshold로 계산한 결과와 predict로 계산한 결과가 다를 수 있습니다. 이는 두 0과 1로 예측하는 방법이 다르기 때문입니다. 예를 들어서 (0.49, 0.51)의 확률이 있을 때 predict의 경우 class 1의 확률에 속할 확률이 크기 때문에 1로 분류합니다. 하지만 best_threshold가 0.52라면 predict_proba의 경우 class를 0으로 분류하게 됩니다 . 4. &#47560;&#47924;&#47532; . 세개의 모델들의 정확도를 비교해 보겠습니다. . print(f&quot;Linear Regression Test Accuracy: {linear_test_accuracy:.2f}&quot;) print(f&quot;Logistic Regression predict_proba Test Accuracy: {proba_test_accuracy:.2f}&quot;) print(f&quot;Logistic Regression predict Test Accuracy: {test_accuracy:.2f}&quot;) . Linear Regression Test Accuracy: 0.96 Logistic Regression predict_proba Test Accuracy: 0.98 Logistic Regression predict Test Accuracy: 0.97 .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/26/ML.html",
            "relUrl": "/python/2022/04/26/ML.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "(3주차 ML) 3월 24일",
            "content": "&#45336;&#54028;&#51060;&#47196; &#45936;&#51060;&#53552; &#51456;&#48708; . import numpy as np . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = np.column_stack((fish_length, fish_weight)) . fish_data[:5] . array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ]]) . fish_target = np.concatenate((np.ones(35), np.zeros(14))) . fish_target . array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#45936;&#51060;&#53552; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split ## model selection 모듈 아래에 train_test_split 함수 . train_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, stratify = fish_target, random_state = 42) .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/24/ML.html",
            "relUrl": "/python/2022/03/24/ML.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "(3주차 DV) 3월 23일",
            "content": "- How to upload your CSV file online for data analysis . https://evidencen.com/how-to-upload-your-csv-file-online/ . &#45800;&#51068;&#48320;&#49688; . import seaborn as sns import pandas as pd . df1 = pd.read_csv(&#39;https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/Data03.csv&#39;) df1.head() . id type_of_contract type_of_contract2 channel datetime Term payment_type product amount state overdue_count overdue credit rating bank cancellation age Mileage . 0 66758234 | 렌탈 | Normal | 서비스 방문 | 2019-10-20 | 60 | CMS | K1 | 96900 | 계약확정 | 0 | 없음 | 9.0 | 새마을금고 | 정상 | 43.0 | 1862.0 | . 1 66755948 | 렌탈 | Extension_Rental | 서비스 방문 | 2019-10-20 | 60 | 카드이체 | K1 | 102900 | 계약확정 | 0 | 없음 | 2.0 | 현대카드 | 정상 | 62.0 | 2532.0 | . 2 66756657 | 렌탈 | Normal | 홈쇼핑/방송 | 2019-10-20 | 60 | CMS | K1 | 96900 | 계약확정 | 0 | 없음 | 8.0 | 우리은행 | 정상 | 60.0 | 2363.0 | . 3 66423450 | 멤버십 | TAS | 렌탈재계약 | 2019-10-20 | 12 | CMS | K1 | 66900 | 계약확정 | 0 | 없음 | 5.0 | 농협은행 | 정상 | 60.0 | 2449.0 | . 4 66423204 | 멤버십 | TAS | 렌탈재계약 | 2019-10-20 | 12 | CMS | K1 | 66900 | 해약확정 | 12 | 있음 | 8.0 | 농협은행 | 해약 | 51.0 | 1942.0 | . 1 &#48276;&#51452;&#54805; &#48320;&#49688; . df1[&#39;type_of_contract&#39;].value_counts() . 렌탈 46481 멤버십 4819 Name: type_of_contract, dtype: int64 . sns.countplot(data=df1, x=&#39;type_of_contract&#39;) . &lt;AxesSubplot:xlabel=&#39;type_of_contract&#39;, ylabel=&#39;count&#39;&gt; . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 47116 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 53448 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 47716 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 48260 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 49901 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 47116 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 53448 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 47716 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 48260 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 49901 missing from current font. font.set_text(s, 0, flags=flags) . 한글 글씨체 설정이 필요해 보인다. | . df1[&#39;product&#39;].value_counts() . K1 39134 K2 8995 K3 2082 K5 645 K4 327 K6 120 Name: product, dtype: int64 . sns.countplot(data=df1, x=&#39;product&#39;, hue=&#39;type_of_contract2&#39;) . &lt;AxesSubplot:xlabel=&#39;product&#39;, ylabel=&#39;count&#39;&gt; . 범례와 그래프가 겹쳐나오는 문제가 발생 $ to$ matplotlib 옵션으로 해결 가능 | . &#54620;&#44544; &#44648;&#51664; &#54644;&#44208; . import matplotlib.pyplot as plt ## 파이썬 내에서 그래프 출력시 디테일한 옵션 import matplotlib as mpl ## 한글폰트 설정, 글씨체 흐릿한 것을 선명하게 (전체적인 큰 틀의 옵션) . mpl.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) ## 맑은 고딕 . C드라이브 $ to$ Windows $ to$ Fonts $ to$ Malgun Gothic | . sns.countplot(data=df1, x=&#39;type_of_contract&#39;) . &lt;AxesSubplot:xlabel=&#39;type_of_contract&#39;, ylabel=&#39;count&#39;&gt; . &#45936;&#51060;&#53552; &#44536;&#47000;&#54532; &#44217;&#52840;&#47928;&#51228; . - 그래프 사이즈 키우기 . plt.figure(figsize=[10,5]) ## Size 조정 sns.countplot(data=df1, x=&#39;product&#39;, hue=&#39;type_of_contract2&#39;) plt.legend(loc=&#39;right&#39;) ## 범례 오른쪽에 위치 plt.savefig(&#39;img1.png&#39;) ## 이미지 파일 형태로 저장 # plt.savefig(&#39;img1.pdf&#39;) . 2 &#50672;&#49549;&#54805; &#48320;&#49688; . sns.histplot(data=df1, x=&#39;age&#39;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;Count&#39;&gt; . - kde = True 옵션 추가 . plt.title(&#39;계약 유형 별. 고객 연령 분포&#39;) sns.histplot(data=df1, x=&#39;age&#39;, kde = True, hue=&#39;type_of_contract&#39;) ## 확률분포선 (kde=True) plt.show() . 3 &#44536; &#50808; &#52628;&#44032; &#50741;&#49496;&#46308; . - 그래프 축에 있는 글씨 겹침 . sns.countplot(data=df1, x=&#39;bank&#39;) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=[5, 10]) sns.countplot(data=df1, y=&#39;bank&#39;) . &lt;AxesSubplot:xlabel=&#39;count&#39;, ylabel=&#39;bank&#39;&gt; . bank column에 데이터가 굉장히 많다. 빈도수가 높은 상위 10개 데이터만 뽑아서 시각화를 해보자. | . - 빈도가 높은 순으로 정렬 . df1[&#39;bank&#39;].value_counts() ## 빈도수가 높은 순으로 출력 . 국민은행 9901 롯데카드 9518 농협은행 6278 신한은행 3522 우리은행 3386 기업은행 1963 신한카드 1533 하나은행 1446 국민카드 1311 BC카드 1264 새마을금고 964 부산은행 888 삼성카드 884 현대카드 876 대구은행 746 우체국 717 외환은행 586 외환카드 530 경남은행 442 SC제일은행 439 광주은행 347 신협중앙회 341 전북은행 195 씨티은행 162 수협중앙회 160 제주은행 40 유안타증권 27 산업은행 23 현대증권 11 삼성증권 7 하나SK 6 미래에셋증권 5 NH농협카드 4 한국투자증권 4 신한금융투자 4 우리카드 3 대우증권 2 하이투자증권 1 메리츠종합금융증권 1 수협카드 1 상호저축은행 1 SK증권 1 하나대투증권 1 산림조합중앙회 1 대신증권 1 씨티카드 1 Name: bank, dtype: int64 . sns.countplot(data=df1, x=&#39;bank&#39;, order=[&#39;국민은행&#39;, &#39;롯데카드&#39;, &#39;농협은행&#39;, &#39;신한은행&#39;]) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . - 빈도 순 정렬 . order_list = df1[&#39;bank&#39;].value_counts().index.tolist() ## 빈도 수 높은 순으로 인덱스 출력 . sns.countplot(data=df1, x=&#39;bank&#39;, order=order_list) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . - 상위 10개만 시각화 . plt.figure(figsize=[10,5]) ## figure size 조정. sns.countplot(data=df1, x=&#39;bank&#39;, order=order_list[0:10]) plt.savefig(&#39;img10.pdf&#39;) ## 이미지 pdf 형태로 저장 . - 이미지 파일이 저장된 디렉토리 경로 . import os print(os.getcwd()) # print(os.listdir(os.getcwd())) . C: Users 82103 Desktop dino BP2022 _notebooks . os.path.exists(&#39;C:/Users/82103/Desktop/dino/BP2022/_notebooks/img10.pdf&#39;) . True . 따라서 위의 경로(현재 작업 폴더)에 img10.pdf 이미지 파일이 저장되어 있음을 확인할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/23/DV.html",
            "relUrl": "/python/2022/03/23/DV.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "(3주차 ML) 3월 17일",
            "content": "training set / test set . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . len(fish_length), len(fish_weight) . (49, 49) . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1]*35 + [0]*14 . fish_data[0] . [25.4, 242.0] . KNN (&#52395;&#48264;&#51704; &#49884;&#46020;) . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . print(fish_data[4]) . [29.0, 430.0] . print(fish_data[0:5]) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] . print(fish_data[:5]) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] . print(fish_data[44:]) . [[12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] . train_input = fish_data[:35] ## index 0~34 train_target = fish_target[:35] test_input = fish_data[35:] ## index 35~48 test_target = fish_target[35:] . len(train_input), len(train_target) . (35, 35) . len(test_input), len(test_target) . (14, 14) . kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) . 0.0 . 정확도가 0% 이다? | test input에 있는 샘플 14개를 모두 못 맞췄다는 것이다. | WHY ? $ Rightarrow$ 샘플링 편향 | . &#49368;&#54540;&#47553; &#54200;&#54693; . 왜그런가 봤더니 처음에 fish_length와 fish_weight를 도미 35개, 빙어 14개를 쭉 늘어놓고 두 리스트를 합쳤다. | 두 리스트를 합친 fish_data에서 앞에 35개를 훈련 뒤에 14개를 test set으로 잘랐다. | 즉, 훈련세트에는 빙어가 하나도 없고, 테스트셋에는 도미가 하나도 없게 된다. $ Rightarrow$ 미적분 공부하고 확통시험 본 격.. | . | train set과 test set을 나눌 때에는 빙어와 도미 두 class가 잘 섞여있도록 만들어야 한다. | . Numpy . 이제 Numpy를 이용해서 잘 섞어서 train set과 test set으로 나눠보자. | Numpy는 파이썬의 대표적인 배열 library | scikit-learn이나 matplotlib librayr도 넘파이에 크게 의존하고 있고, 입력 데이터가 Numpy로 전달될 거라고 가정하고 있다. predict method 결과값이 array([1])이런 형태로 출력되는 것도 이러한 이유.(사이킷런의 predict 메서드의 반환값을 넘파이 배열로 리턴) | . | 딥러닝 TensorFlow도 Numpy와도 타이트한 관계가 있다. | . . 1차원 배열(벡터), 2차원 배열(행렬), 3차원 배열 | . training set / test set (using Numpy) . input과 target이 함께 섞여서 이동을 해야한다. (섞여야 한다) | 지도학습에서 입력과 타겟이 쌍을 이루고 있게 되는데 따로따로 섞여버리면 정답을 제대로 못주게 되서 엉터리 훈련이 되버린다. | 입력데이터 특성값과 타깂값이 쌍으로 잘 따라서 섞이도록 만들어야 하는것이 중요!!! index를 섞어 분리하는 방법 | . | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr) . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] . print(input_arr.shape) . (49, 2) . - 0~48까지 정수로된 index 만들기 . index = np.arange(49) index . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]) . - index를 섞어준다. . np.random.seed(42) np.random.shuffle(index) . print(index) . [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] . 잘 섞였다... | . input_arr[:5] . array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ]]) . print(input_arr[[1,3]]) . [[ 26.3 290. ] [ 29. 363. ]] . - 랜덤하게 섞인 인덱스 배열에서 앞부분 35개를 훈련셋으로 두고, 뒷부분 14개를 테스트 셋으로 둔다 . print(index) . [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] . train_input = input_arr[index[:35]] train_target = target_arr[index[:35]] . print(input_arr[13], train_input[0]) . [ 32. 340.] [ 32. 340.] . test_input = input_arr[index[35:]] test_target = target_arr[index[35:]] . import matplotlib.pyplot as plt plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(test_input[:,0], test_input[:,1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . KNN (&#46160;&#48264;&#51704; &#49884;&#46020;) . kn = kn.fit(train_input, train_target) . kn.score(test_input, test_target) . 1.0 . 잘 훈련되었다. | . kn.predict(test_input) . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . test_target . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . Summary . Numpy를 이용해서 데이터를 섞어서 만들때, 배열 자체를 섞지 않고 (특성데이터와 타깃테이터가 쌍을 이루어서 섞어야 하므로) . | 배열의 인덱스배열을 만들어서 인덱스를 섞은 후에 . | 섞인 인덱스를 가지고 배열 슬라이싱을 하여 훈련세트와 테스트셋으로 나눈다. . | 이렇게 나눈 것으로 KNN으로 다시 훈련해서 모델을 평가 . | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/19/ML.html",
            "relUrl": "/python/2022/03/19/ML.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "(3주차 DV) 3월 18일",
            "content": "import pandas as pd . pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/train.csv&#39;) . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1455 1456 | 60 | RL | 62.0 | 7917 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | WD | Normal | 175000 | . 1456 1457 | 20 | RL | 85.0 | 13175 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | MnPrv | NaN | 0 | 2 | 2010 | WD | Normal | 210000 | . 1457 1458 | 70 | RL | 66.0 | 9042 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | GdPrv | Shed | 2500 | 5 | 2010 | WD | Normal | 266500 | . 1458 1459 | 20 | RL | 68.0 | 9717 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | 142125 | . 1459 1460 | 20 | RL | 75.0 | 9937 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 6 | 2008 | WD | Normal | 147500 | . 1460 rows × 81 columns . df_train = pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/train.csv&#39;) df_test = pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/test.csv&#39;) . from pandas.core.groupby.generic import ScalarResult import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np from scipy.stats import norm from sklearn.preprocessing import StandardScaler from scipy import stats import warnings warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline . df_train.columns . Index([&#39;Id&#39;, &#39;MSSubClass&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;MasVnrArea&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;KitchenQual&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageYrBlt&#39;, &#39;GarageFinish&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;PoolQC&#39;, &#39;Fence&#39;, &#39;MiscFeature&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) . df_test.columns . Index([&#39;Id&#39;, &#39;MSSubClass&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;MasVnrArea&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;KitchenQual&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageYrBlt&#39;, &#39;GarageFinish&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;PoolQC&#39;, &#39;Fence&#39;, &#39;MiscFeature&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;], dtype=&#39;object&#39;) . df_train[&#39;SalePrice&#39;].describe() ## 부동산 가격의 기술통계량 . count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 . sns.distplot(df_train[&#39;SalePrice&#39;]) ## line : kernel density plot ## 목표변수에 대한 히스토그램과 kernel density plot . print(&quot;Skewness: %f&quot; % df_train[&#39;SalePrice&#39;].skew()) print(&quot;Kurtosis: %f&quot; % df_train[&#39;SalePrice&#39;].kurt()) ## 꼬리가 두터운 정도 (이상치가 많을수록 두꺼움) . Skewness: 1.882876 Kurtosis: 6.536282 . $-2 sim2$ 사이의 값이므로 치우침이 없는 데이터 (by. George &amp; Mallery, 2010) | 첨도가 높으면 (Kurtosis &gt; 3) 이상치가 많이 있다는 것. | . 많은 통계기법들이 정규성을 가정한다. | . positive skewness : 오른쪽 꼬리, 왼쪽에 데이터가 많다. | negative skewness : 왼쪽 꼬리, 오른쪽에 데이터가 많다. | . 왜도, 첨도 읽어보기 . . . var1 = &#39;GrLivArea&#39; # 지상 거실 면적 평방피트 data1 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var1]], axis=1) ## 열로 합치기(axis=1) . data1.plot.scatter(x=var1, y=&#39;SalePrice&#39;, ylim=(0,800000)) . &lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;, ylabel=&#39;SalePrice&#39;&gt; . linear relationship | . var2 = &#39;TotalBsmtSF&#39; # 지하 총 평방 피트 data2 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var2]], axis=1) # 열기준으로 붙임 data2.plot.scatter(x=var2, y=&#39;SalePrice&#39;, ylim=(0,800000)) . &lt;AxesSubplot:xlabel=&#39;TotalBsmtSF&#39;, ylabel=&#39;SalePrice&#39;&gt; . 더 strong 한 linear relationship ( 더 가파르다. ) | . var1 = &#39;GrLivArea&#39; data1 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var1]], axis=1) plt.scatter(var1, y=&#39;SalePrice&#39;, data = data1) . &lt;matplotlib.collections.PathCollection at 0x24f2102bca0&gt; . var2 = &#39;TotalBsmtSF&#39; data2 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var2]], axis=1) plt.scatter(var2, y=&#39;SalePrice&#39;, data = data2) . &lt;matplotlib.collections.PathCollection at 0x24f21535ac0&gt; . data2 . SalePrice OverallQual . 0 208500 | 7 | . 1 181500 | 6 | . 2 223500 | 7 | . 3 140000 | 7 | . 4 250000 | 8 | . ... ... | ... | . 1455 175000 | 6 | . 1456 210000 | 6 | . 1457 266500 | 7 | . 1458 142125 | 5 | . 1459 147500 | 5 | . 1460 rows × 2 columns . var3 = &#39;OverallQual&#39; # 전체 제료 및 마감품질 data3 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var3]], axis=1) # 열로 합치기 f, ax = plt.subplots(figsize=(8,6)) fig = sns.boxplot(x=var3, y=&quot;SalePrice&quot;, data=data3) fig.axis(ymin=0, ymax=800000) . (-0.5, 9.5, 0.0, 800000.0) . var4 = &#39;YearBuilt&#39; # 원래 건설 날짜 data4 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var4]], axis=1) f, ax = plt.subplots(figsize=(16,8)) fig = sns.boxplot(x=var4, y=&quot;SalePrice&quot;, data=data4) fig.axis(ymin=0, ymax=800000) plt.xticks(rotation=90) # x축 눈금 값 90도 회전. .",
            "url": "https://pinkocto.github.io/BP2022/2022/03/18/DV.html",
            "relUrl": "/2022/03/18/DV.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "(2주차 ML) 3월 10일",
            "content": "&#49373;&#49440; &#48516;&#47448; &#47928;&#51228; . &#46020;&#48120;(bream) &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] . import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.xlabel(&#39;length&#39;) # 몸 길이 plt.ylabel(&#39;weight&#39;) # 몸 무게 . Text(0, 0.5, &#39;weight&#39;) . &#48729;&#50612;(smelt) &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . plt.scatter(bream_length, bream_weight, label=&#39;bream&#39;) ## 도미 plt.scatter(smelt_length, smelt_weight, label=&#39;smelt&#39;) ## 빙어 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.legend() plt.show() . 빙어는 길이가 늘어나더라도 무게가 많이 늘지 않는다. $ Rightarrow$ 빙어의 산점도 역시 선형적이지만 무게가 길이에 영향을 덜 받는다. | . binary classification (&#46020;&#48120;, &#48729;&#50612;) &#51456;&#48708; . 다음으로 데이터만 보고 어떤 것이 도미이고 어떤 것이 빙어인지 스스로 구분하기 위해 프로그램을 만들어 보자! | KNN(K-Nearest Neighbors) 방법을 이용할 것. | . - 우선 KNN 알고리즘을 써먹으려면 도미와 빙어 데이터를 하나의 데이터로 합쳐야 한다. . length = bream_length + smelt_length weight = bream_weight + smelt_length . + 연산자가 list 일 경우에는 합쳐지는 역할을 하고, 정수일 때는 우리가 일반적으로 알고있는 덧셈 연산을 한다. | . len(bream_length), len(smelt_length), len(bream_weight), len(smelt_length) . (35, 14, 35, 14) . len(length), len(weight) . (49, 49) . 잘 합쳐진 것 같다. | . - 2차원 리스트로 만들어 보자. (Scikit-learn을 사용하기위해) . ## 이런 식으로 2차원 리스트로 만들것! 길이 무게 [[25.4, 242.0], [26.3, 290.0]. . . . . . . [15.0, 19.9]] . fish_data = [[l,w] for l, w in zip(length, weight)] . - 정답 준비 . 도미(bream)를 1로 놓고, 빙어(smelt)를 0으로 놓자. (0과 1로 분류하는 이진분류) | . fish_target = [1]*35 + [0]*14 . K-&#52572;&#44540;&#51217; &#51060;&#50883; . from sklearn.neighbors import KNeighborsClassifier . kn = KNeighborsClassifier() # class의 instance(객체) 를 만든다. . kn.fit(fish_data, fish_target) ## kn을 모델이라 부름 . KNeighborsClassifier() . 머신러닝 프로그램의 알고리즘이 객체화 된것을 모델이라고 부른다. | 종종 그 알고리즘 자체를 모델이라고도 부름. | . kn.score(fish_data, fish_target) . 1.0 . 100% 다 맞췄다! (100% 정확도 달성!) | . &#49352;&#47196;&#50868; &#49373;&#49440; &#50696;&#52769; . 그래프에 표시된 초록색 삼각형은 어떤 생선일까? | . plt.scatter(bream_length, bream_weight, label=&#39;bream&#39;) plt.scatter(smelt_length, smelt_weight, label=&#39;smelt&#39;) plt.scatter(30, 600, marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.legend() plt.show() . 직관적으로 봤을때 도미(bream) 일 것 같다. | 실제로도 그런지 확인해보자. | . kn.predict([[30, 600]]) ## predict method . array([1]) . predict method 안에 넣을 때도 2차원 배열 데이터를 넣어준다. (사이킷런이 기대하는 것) | n_neighbors=5가 default, 주위에 있는 이웃의 개수(K)만큼 주변 샘플의 class 중 가장 많은 클래스를 정답클래스로 삼는다. | . print(kn._fit_X) . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 9.8] [ 10.5 10.5] [ 10.6 10.6] [ 11. 11. ] [ 11.2 11.2] [ 11.3 11.3] [ 11.8 11.8] [ 11.8 11.8] [ 12. 12. ] [ 12.2 12.2] [ 12.4 12.4] [ 13. 13. ] [ 14.3 14.3] [ 15. 15. ]] . print(kn._y) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . &#47924;&#51312;&#44148; &#46020;&#48120; . Fish 데이터의 총 개수는 49개이다. 이번에는 n_neighbors = 49로 지정 해보자. . kn49 = KNeighborsClassifier(n_neighbors=49) . kn49.fit(fish_data, fish_target) . KNeighborsClassifier(n_neighbors=49) . kn49.score(fish_data, fish_target) ## 점수 . 0.7142857142857143 . score method : 훈련한 모델을 가지고 어떤 데이터를 집어 넣어서 얼마만큼 잘 맞는지를 확인해 보는 것이다. | 분류문제일 경우에는 정확도를 출력 (모델이 어느정도 정확한지를 알아보는 메서드.) | . print(35/49) . 0.7142857142857143 . 이렇게 모델을 만들면 전체 샘플의 다수는 도미 $ to$ 무조건 다 도미 | n_neighbors 매개변수로 주위의 샘플개수를 바꿔볼 수도 있다. 바꾸면 알고리즘의 정확도가 높을수록, 낮을수도 있다. | . &#54869;&#51064; &#47928;&#51228; . kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) for n in range(5, 50): # 최근접 이웃 개수 설정 kn.n_neighbors = n #접수 계산 score = kn.score(fish_data, fish_target) # 100% 정확도에 미치지 못하는 이웃 개수 출력 if score &lt; 1: print(n, score) break . 18 0.9795918367346939 .",
            "url": "https://pinkocto.github.io/BP2022/2022/03/17/ML.html",
            "relUrl": "/2022/03/17/ML.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "tips",
            "content": "1. csv&#54028;&#51068; upload . 참고링크 : https://evidencen.com/how-to-upload-your-csv-file-online/ . import pandas as pd . autompg.csv github notebooks에 upload | autommpg.csv 파일 click | Raw / Blame에서 Raw 버튼 click | 상단의 링크 복사 | . 복사한 주소 : https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/autompg.csv . pd.read_csv(&quot;https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/autompg.csv&quot;) . mpg cyl disp hp wt accler year origin carname . 0 18.0 | 8 | 307.0 | 17 | 3504 | 12.0 | 70 | 1 | chevrolet chevelle malibu | . 1 15.0 | 8 | 350.0 | 35 | 3693 | 11.5 | 70 | 1 | buick skylark 320 | . 2 18.0 | 8 | 318.0 | 29 | 3436 | 11.0 | 70 | 1 | plymouth satellite | . 3 16.0 | 8 | 304.0 | 29 | 3433 | 12.0 | 70 | 1 | amc rebel sst | . 4 17.0 | 8 | 302.0 | 24 | 3449 | 10.5 | 70 | 1 | ford torino | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 82 | 2790 | 15.6 | 82 | 1 | ford mustang gl | . 394 44.0 | 4 | 97.0 | 53 | 2130 | 24.6 | 82 | 2 | vw pickup | . 395 32.0 | 4 | 135.0 | 80 | 2295 | 11.6 | 82 | 1 | dodge rampage | . 396 28.0 | 4 | 120.0 | 75 | 2625 | 18.6 | 82 | 1 | ford ranger | . 397 31.0 | 4 | 119.0 | 78 | 2720 | 19.4 | 82 | 1 | chevy s-10 | . 398 rows × 9 columns . 깃헙에 업로드 한 csv파일을 잘 읽어오는 것을 확인 할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/02/upload.html",
            "relUrl": "/python/2022/03/02/upload.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "(6주차) 2월18일 (3)",
            "content": "- 파일과 경로 . - 텍스트 파일 열기, 쓰기, 읽기 . - Tkinter 파일 다이얼로그를 이용한 예제 . &#54028;&#51068; . 목적 자료를 영구히 보관하기 위해 사용됨 | . | . 종류: 저장된 데이터에 따라 텍스트 파일 일반적인 문자 코드 | . | 이진 파일 사진, 음악, 비디오 | . | . | . 파일의 위치 폴더에 존재 | 폴더는 다른 폴더에 포함되어 있음 | 경로 : 어떤 폴더로부터 그 파일에 이르는 방법 | . | . - &#54028;&#51068; &#44221;&#47196; . * 경로의 종류 . 절대 경로: 루트 폴더로투터 시작 ex) c: users user documents hello.py | . | . 상대 경로: 현재 폴더로부터 시작 | . * 폴더를 표시하는 특수한 기호 . . : 현재 폴더를 의미함 | .. : 현재 폴더의 부모 (즉, 상위폴더를 의미함) | . &gt; &gt;&gt; import os &gt;&gt;&gt; os.getcwd() ## 현재 폴더 경로 &#39;C: USERS USER .... python3.7&#39;&gt; &gt;&gt; os.chdir(&quot;C: Users Users Documents&quot;) ## 현재폴더 경로 변경&gt; &gt;&gt; os.getcwd() ## 현재 폴더 경로 &quot;C: Users Users Documents&quot; ## 바뀐 경로로 출력된 것 확인 . - &#44221;&#47196; &#48516;&#47532; &#47928;&#51088; . Windows에서 경로 분리 문자는 문제는 Escape-Sequence를 나타낼 때 사용됨 | 그냥 문자를 표시하기 위해서는 를 사용 | . | . Unix, Linux 계열에서 경로 분리 문자는 / Windows 위에서 실행되는 파이썬에서 사용 가능 | 앞의 예는 &quot;C:/Users/Users/Documents&quot;로 써도 가능! | . | . Raw 문자열 사용방법 r&quot;문자열&quot;은 문자열 내 모든 특수문자를 무시하고 일반 문자로 취급함 | r&quot;C: Users Users Documents&quot;로 써도 가능! | . | . - &#54028;&#51068; &#50676;&#44592; . open() 이라는 내장 함수를 사용 | . fileVar = open(filename, mode) . open() 함수는 파일 열기를 성공하면 파일을 나타내는 객체를 반환함 _io.TextIoWrapper 클래스 객체임 | . | . . - &#54028;&#51068; &#50676;&#44592; &#47784;&#46300; . 파일열기모드 설명 . r (읽기모드) | 파일을 읽기만 할 때 사용한다 | . w (쓰기모드) | 파일에 내용을 쓸 때 사용하며 기존 파일이 존재하면 내용이 모두 초기화되고 &lt;/br&gt; 주어진 파일이 존재하지 않으면 새로운 파일을 만든다 | . a (추가모드) | 기존 파일의 마지막에 새로운 내용을 추가 시킬 때 사용한다 | . rb, wb | 각각 이진 파일을 읽기 위해 혹은 쓰기 위해 열때 사용한다 | . - &#54028;&#51068;&#50640; &#45936;&#51060;&#53552; &#50416;&#44592; . write() 메서드 사용 | . ofile = open(&quot;snowwhite.txt&quot;, &quot;w&quot;) # 1 ofile.write(&quot;Once upon a time, long, long ago n&quot;) # 2 ofile.write(&quot;a king and queen ruled over n&quot;) ofile.write(&quot;a distant land&quot;) ofile.close() . - &#54028;&#51068;&#51060; &#51316;&#51116;&#54616;&#45716;&#51648; &#44160;&#49324;&#54616;&#44592; . os.path.exist(경로) . 파일이 존재하면 True, 아니면 False를 반환 | . | os.path.isdir(경로) . 지정된 파일이 폴더이면 True, 아니면 False를 반환 | . | os.path.isfile(경로) . 지정된 파일이 일반 파일이면 True, 아니면 False를 반환 | . | . - &#54028;&#51068; &#51088;&#47308; &#51069;&#44592; . 파이썬에는 외부파일을 읽어 들여 프로그램에서 사용할 수 있는 여러가지 방법이 있다. . 전체 데이터를 읽는 메서드 : read(), reaadlines() . | 한 줄을 읽는 메서드 : readline() . | 주어진 길이를 읽는 메서드: read(n) . | . - &#51204;&#52404; &#51069;&#44592; . - 방법1: read() 함수 사용하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ## 읽기 readResult = ifile.read() print(&quot;Read Result:&quot;) print(repr(readResult)) ## repr(): 줄바꿈 문자도 그대로 출력 ifile.close() . Read Result: &#39;Once upon a time, long, long ago na king and queen ruled over na distant land&#39; . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ## 읽기 readResult = ifile.read() print(&quot;Read Result:&quot;) print(readResult) ifile.close() . Read Result: Once upon a time, long, long ago a king and queen ruled over a distant land . read() 는 파일의 내용 전체를 문자열로 돌려준다. | . - 방법2: readlines() 함수 사용하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) readLinesResult = ifile.readlines() print(&quot;Read Lines Result:&quot;) print(readLinesResult) ifile.close() . Read Lines Result: [&#39;Once upon a time, long, long ago n&#39;, &#39;a king and queen ruled over n&#39;, &#39;a distant land&#39;] . readline() 함수는 파일의 모든 줄을 읽어서 각각의 줄을 요소로 갖는 리스트로 돌려준다. | . - 방법2 + 줄바꿈( n) 문자 제거하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) lines = ifile.readlines() for line in lines: line = line.strip() # 줄 끝의 줄 바꿈 문자를 제거 print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . - &#51648;&#51221;&#46108; &#44600;&#51060;&#47564;&#53372; &#51069;&#44592; . file.read(n) : 현재 file pointer로부터 n개의 글자를 읽어서 문자열로 반환 | . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) str1 = ifile.read(4) print(&quot;Read(4) Result:&quot;) print(repr(str1)) str2 = ifile.read(10) print(&quot;Read(10) Result:&quot;) print(repr(str2)) ifile.close() . Read(4) Result: &#39;Once&#39; Read(10) Result: &#39; upon a ti&#39; . - &#54620; &#51460;&#50473; &#51069;&#44592; &#44208;&#44284; . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ifile.readline() ## 1 . &#39;Once upon a time, long, long ago n&#39; . ifile.readline() ## 2 . &#39;a king and queen ruled over n&#39; . ifile.readline() ## 3 . &#39;a distant land&#39; . ifile.readline() ## 4 . &#39;&#39; . line이 끝에 다다르면 빈문자(&#39;&#39;)가 출력된다. | . - &#54028;&#51068;&#50640; &#47336;&#54532; &#49324;&#50857;&#54616;&#44592; . 루프를 사용하여 파일을 한 줄씩 처리하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) line = ifile.readline() while line != &#39;&#39;: # line 처리 print(line) line = ifile.readline() ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) while True: line = ifile.readline() if not line: break print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) lines = ifile.readlines() for line in lines: # line 처리 print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . - &#49707;&#51088;&#44032; &#46308;&#50612;&#44032; &#51080;&#45716; &#54028;&#51068; &#52376;&#47532; . ofile = open(&quot;num.txt&quot;, &quot;w&quot;) ofile.write(&quot;10 20 12 5 n&quot;) ofile.write(&quot;8 9 7 23 n&quot;) ofile.write(&quot;1 8 22 9&quot;) ofile.close() . num.txt 아래와 같은 파일이 있을 떄 파일에 있는 숫자의 합을 구해보자. | . ofile = open(&quot;num.txt&quot;, &quot;r&quot;) print(ofile.read()) ofile.close() . 10 20 12 5 8 9 7 23 1 8 22 9 . ofile = open(&quot;num.txt&quot;, &quot;r&quot;) total = 0 for line in ofile: lineLst = line.split() numList = [eval(x) for x in lineLst] total += sum(numList) print(total) . 134 . 10+20+12+5+8+9+7+23+1+8+22+9 . 134 . 잘 계산된 것을 확인할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/02/18/(3).html",
            "relUrl": "/python/2022/02/18/(3).html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "(6주차) 2월18일 (1)",
            "content": "- 소개 . - 2차원 리스트 처리 . - 2차원 리스트와 함수 . - 예제 . - 다차원 리스트 . &#49548;&#44060; . 테이블이나 행렬은 2차원 리스트로 표현할 수 있다. . - 대한민국 도시들 간 거리 . 서울 부산 대구 광주 . 서울 | 0 | 325 | 237 | 267 | . 부산 | 325 | 0 | 87 | 202 | . 대구 | 237 | 87 | 0 | 172 | . 광주 | 267 | 202 | 172 | 0 | . distance = [[0, 325, 237, 267], [325, 0, 87, 202], [237, 87, 0, 172], [267, 202, 172, 0]] . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52376;&#47532;&#54616;&#44592; . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#54364;&#54788; . matrix_ = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]] . - 각 요소는 두 개의 첨자를 이용하여 표현 . matrix_[0][2] . 3 . matrix_[2][3] . 14 . matrix_[1][1] . 7 . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52488;&#44592;&#54868; . - 사용자 입력 값으로 초기화 . - 무작위 값으로 초기화 . matrix = [] for row in range(3): ## number of rows = 3 matrix.append([]) for col in range(2): ## number of columns = 2 value = eval(input(&quot;value:&quot;)) matrix[row].append(value) . matrix ## 3행 2열의 2차원 리스트 . [[1, 2], [3, 4], [5, 6]] . import random matrix = [] numberOfRows=3 numberOfColumns=2 for row in range(numberOfRows): matrix.append([]) #print(matrix) for col in range(numberOfColumns): matrix[row].append(random.randint(0,99)) #print(matrix) . [[]] [[51]] [[51, 15]] [[51, 15], []] [[51, 15], [47]] [[51, 15], [47, 46]] [[51, 15], [47, 46], []] [[51, 15], [47, 46], [79]] [[51, 15], [47, 46], [79, 98]] . matrix . [[51, 15], [47, 46], [79, 98]] . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52636;&#47141;&#54616;&#44592;, &#49438;&#44592;, &#51221;&#47148; . - 2차원 리스트 출력 . len(matrix), len(matrix[0]) . (3, 2) . for row in range(len(matrix)): for col in range(len(matrix[row])): print(matrix[row][col], end=&#39; &#39;) print() . 51 15 47 46 79 98 . - 2차원 리스트 섞기 . matrix . [[51, 15], [47, 46], [79, 98]] . for row in range(len(matrix)): for col in range(len(matrix[row])): i = random.randint(0, len(matrix)-1) j = random.randint(0, len(matrix[row])-1) matrix[row][col], matrix[i][j] = matrix[i][j], matrix[row][col] . matrix . [[79, 98], [46, 15], [47, 51]] . 리스트 안의 원소들이 잘 섞여진 것을 확인할 수 있다. | . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52376;&#47532; . - 모든 원소들의 합 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . total = 0 for row in matrix: for value in row: total += value print(total) . 79 177 223 238 285 336 . total . 336 . - 각 열의 합 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . for col in range(len(matrix[0])): total = 0 for row in range(len(matrix)): total += matrix[row][col] print(&quot;Sum of column&quot;, col, &quot;is&quot;, total) . Sum of column 0 is 172 Sum of column 1 is 164 . 79+46+47 # 1열 합 . 172 . 98+15+51 # 2열 합 . 164 . 각 열의 합이 잘 계산되었다. | . - 합이 가장 큰 행 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . maxRow = sum(matrix[0]) indexRow = 0 for row in range(1, len(matrix)): if sum(matrix[row]) &gt; maxRow: maxRow = sum(matrix[row]) indexRow = row print(&quot;Row&quot;, indexRow, &quot;has max sum of &quot; , maxRow) . Row 0 has max sum of 177 . - 리스트 정렬 . lst = [[1,2],[1,1],[3,1],[2,5]] lst . [[1, 2], [1, 1], [3, 1], [2, 5]] . lst.sort() lst . [[1, 1], [1, 2], [2, 5], [3, 1]] . 2&#52264;&#50896; &#47532;&#49828;&#53944;&#50752; &#54632;&#49688; . 2차원 리스트도 다른 객체와 마찬가지로 함수에 인수로 전달할 수도 있고, 함수가 반환값으로 반환할 수도 있다. . def getMatrix(): matrix = [] numRows = eval(input(&quot;Number of Rows: &quot;)) numCols = eval(input(&quot;Number of Colunmns: &quot;)) for row in range(numRows): matrix.append([]) for col in range(numCols): value = eval(input(&quot;value:&quot;)) matrix[row].append(value) return matrix . getMatrix() . [[79, 98], [46, 15], [17, 51]] . def accumulate(m): total = 0 for row in m: total += sum(row) return total . accumulate(matrix) . 336 . matrix의 모든 원소의 합은 336으로 위에서 구한 (2차원 리스트 처리: 모든 원소의 합)에서 구한 값과 같다. | . &#50696;&#51228;1 | &#44032;&#51109; &#44032;&#44620;&#50868; &#46160; &#51216;&#51008;? . 여러 점에 대한 좌표가 있다. 이들 점 중에서 가장 가까운 두 점을 찾아보자. . def distance(x1,y1,x2,y2): return((x1-x2)**2 + (y1-y2)**2)**0.5 def nearestPoint(points): p1, p2 = 0, 1 shortestDist = distance(points[p1][0], points[p1][1], points[p2][0], points[p2][1]) for i in range(len(points)): for j in range(i+1, len(points)): d = distance(points[i][0], points[i][1], points[j][0], points[j][1]) if d &lt; shortestDist: shortestDist = d p1, p2 = i, j return p1, p2 . nPoints = eval(input(&quot;점의 수:&quot;)) points = [] for i in range(nPoints): point = [0,0] point[0],point[1] = eval(input(&quot;좌표:&quot;)) points.append(point) p1, p2 = nearestPoint(points) print(&quot;(&quot;, points[p1][0], points[p1][1],&quot;)&quot;,&quot;(&quot;,points[p2][0], points[p2][1],&quot;)&quot;) . ( 0 0 ) ( 0 1 ) . nPoints = eval(input(&quot;점의 수:&quot;)) points = [] for i in range(nPoints): point = [0,0] point[0],point[1] = eval(input(&quot;좌표:&quot;)) points.append(point) p1, p2 = nearestPoint(points) print(&quot;(&quot;, points[p1][0], points[p1][1],&quot;)&quot;,&quot;(&quot;,points[p2][0], points[p2][1],&quot;)&quot;) . ( 2 5 ) ( 3 8 ) . &#50696;&#51228;2 | Sudoku . . [게임 규칙] . 각 열, 각 행에 1~9까지 숫자가 들어가야 한다. | $3 times 3$ 블록에 1~9가지 숫자가 들어가야 한다. | . &#50612;&#46500; Sudoku &#54644;&#44208;&#48169;&#48277;&#51060; &#47582;&#45716;&#51648; &#44160;&#49324; . 두가지 검사방법 . 각 행, 열, 블록이 1~9까지 숫자를 포함하고 있는지 검사 | | 각 셀에 대해 그 셀의 숫자가 행, 열, 블록에서 유일한지 검사 &lt; 이 방법 사용할 것임! | | . | . def isValid(grid): for i in range(9): for j in range(9): if grid[i][j] &lt; 1 or grid[i][j] &gt; 9 or not isValidAt(i,j,grid): return False return True . def isValidAt(i,j,grid): for column in range(9): if column != j and grid[i][column] == grid[i][j]: return False for row in range(9): if row != i and grid[row][j] == grid[i][j]: return False for row in range((i//3)*3, (j//3)*3 + 3): if row != i and column != j and grid[row][column] == grid[i][j]: return False return True . &#45796;&#52264;&#50896; &#47532;&#49828;&#53944; . 일반적으로 $n$개의 첨자 사용 | . m[i][j][k] . .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/02/18/(1).html",
            "relUrl": "/python/2022/02/18/(1).html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "test",
            "content": "import pandas as pd import altair as alt . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . selection = alt.selection_single(); alt.Chart(cars).mark_circle().add_selection( selection ).encode( x=&#39;Horsepower:Q&#39;, y=&#39;Miles_per_Gallon:Q&#39;, color=alt.condition(selection, &#39;Cylinders:O&#39;, alt.value(&#39;grey&#39;)), opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1)) ) . def plot(selection): return alt.Chart(cars).mark_circle().add_selection( selection ).encode( x=&#39;Horsepower:Q&#39;, y=&#39;Miles_per_Gallon:Q&#39;, color=alt.condition(selection, &#39;Cylinders:O&#39;, alt.value(&#39;grey&#39;)), opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1)) ).properties( width=240, height=180 ) . alt.hconcat( plot(alt.selection_single()).properties(title=&#39;Single (Click)&#39;), plot(alt.selection_multi()).properties(title=&#39;Multi (Shift-Click)&#39;), plot(alt.selection_interval()).properties(title=&#39;Interval (Drag)&#39;) ) . alt.hconcat( plot(alt.selection_single(on=&#39;mouseover&#39;)).properties(title=&#39;Single (Mouseover)&#39;), plot(alt.selection_multi(on=&#39;mouseover&#39;)).properties(title=&#39;Multi (Shift-Mouseover)&#39;) ) .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/01/04/test.html",
            "relUrl": "/python/2022/01/04/test.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pinkocto.github.io/BP2022/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pinkocto.github.io/BP2022/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pinkocto.github.io/BP2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pinkocto.github.io/BP2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
{
  
    
        "post0": {
            "title": "Transpose",
            "content": "",
            "url": "https://pinkocto.github.io/BP2022/2022/04/28/Untitled.html",
            "relUrl": "/2022/04/28/Untitled.html",
            "date": " • Apr 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(1주차 ML2) 4월 28일",
            "content": "",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/28/ML.html",
            "relUrl": "/python/2022/04/28/ML.html",
            "date": " • Apr 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "OpenCV",
            "content": "https://www.youtube.com/watch?v=XK3eU9egll8 . &#54872;&#44221; &#49444;&#51221; . Anaconda prompt에서 다음 명령 수행 . pip install opencv-pythoon . import cv2 cv2.__version__ . &#39;4.5.5&#39; . OpenCV (Computer Vision) . 다양한 영상 (이미지) / 동영상 처리에 사용되는 오픈소스 라이브러리 . 1. &#51060;&#48120;&#51648; &#52636;&#47141; . import cv2 img = cv2.imread(&#39;./my_icons/img.jpg&#39;) # 해당 경로의 파일 읽어오기 cv2.imshow(&#39;img&#39;, img) # img 라는 이름의 창에 img를 표시 cv2.waitKey(5000) # 지정된 시간(ms) 동안 사용자 키 입력 대기 print(key) cv2.destroyAllWindows() # 모든 창 닫기 . 98 . &#51069;&#44592; &#50741;&#49496; . cv2.IMREAD_COLOR : 컬러 이미지,. 투명 영역은 무시 (기본값) | cv2.IMREAD_GRAYSCALE : 흑백이미지 | cv2.IMREAD_UNCHANGED : 투명 영엳까지 포함 | import cv2 img_color = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_COLOR) img_gray = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_GRAYSCALE) img_unchanged = cv2.imread(&#39;./my_icons/img.jpg&#39;, cv2.IMREAD_UNCHANGED) cv2.imshow(&#39;img_color&#39;, img_color) cv2.imshow(&#39;img_gray&#39;, img_gray) cv2.imshow(&#39;img_unchanged&#39;, img_unchanged) cv2.waitKey(0) cv2.destroyAllWindows() . Shape . 이미지의 height, width, channel 정보 . import cv2 img = cv2.imread(&#39;./my_icons/img.jpg&#39;) img.shape # 세로, 가로, Channel . (390, 640, 3) . 2. &#46041;&#50689;&#49345; &#52636;&#47141; . &#46041;&#50689;&#49345; &#54028;&#51068; &#52636;&#47141; . import cv2 cap = cv2.VideoCapture(&#39;./my_icons/video.mp4&#39;) while cap.isOpened(): # 동영상 파일이 올바로 열렸는지? ret, frame = cap.read() # ret : 성공 여부, frame : 받아온 이미지 (프레임) if not ret: print(&#39;더 이상 가져올 프레임이 없어요&#39;) break cv2.imshow(&#39;video&#39;, frame) if cv2.waitKey(1) == ord(&#39;q&#39;): print(&#39;사용자 입력에 의해 종료합니다&#39;) break cap.release() # 자원 해제 cv2.destroyAllWindows() # 모든 창 닫기 . 더 이상 가져올 프레임이 없어요 . &#52852;&#47700;&#46972; &#52636;&#47141; . import cv2 cap = cv2.VideoCapture(0) # 0번째 카메라 장치 (Device ID) if not cap.isOpened(): # 카메라가 잘 열리지 않은 경우 exit() # 프로그램 종료 while True: ret, frame = cap.read() if not ret: break cv2.imshow(&#39;camera&#39;, frame) if cv2.waitKey(1) == ord(&#39;q&#39;): # 사용자가 q를 입력하면 break cap.release() cv2.destroyAllWindows() . 3. &#46020;&#54805; &#44536;&#47532;&#44592; . &#48712; &#49828;&#52992;&#52824;&#48513; &#47564;&#46308;&#44592; . import cv2 import numpy as np # 세로 480 X 가로 640, 3 Channel (RGB) 에 해당하는 스케치북 만들기 img = np.zeros((480, 640, 3), dtype = np.uint8) # img[:] = (255, 255, 255) # 전체 공간을 흰색으로 채우기 (B,G,R) # print(img) cv2.imshow(&#39;img&#39;, img) cv2.waitKey(0) cv2.destroyAllWindows() .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/26/opencv.html",
            "relUrl": "/python/2022/04/26/opencv.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(1주차 ML2) 4월 26일",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . import matplotlib.pyplot as plt ## 파이썬 내에서 그래프 출력시 디테일한 옵션 import matplotlib as mpl ## 한글폰트 설정, 글씨체 흐릿한 것을 선명하게 (전체적인 큰 틀의 옵션) . mpl.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) ## 맑은 고딕 . 1. Data . 위스콘신 유방암 데이터(Wisconsin Breast Cancer data)를 분석해보자. . 분석의 목적은 30개의 설명변수를 사용해 진단값이 악성인지 양성인지 예측하는 것입니다. . 1.1 Data Load . from sklearn.datasets import load_breast_cancer . cancer = load_breast_cancer() . cancer[&quot;feature_names&quot;] . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . Variable name Description . radius | 반지름 | . texture | 그레이스케일 값의 표준편차 | . perimeter | 둘레 | . area | 면적 | . smoothness | 반지름의 국소적 변화정도(local variation) | . compactness | $ frac{ text{perimeter}^2}{area}-1.0$ | . concavity | 오목한 정도(severity of concave portions of the contour) | . concave_points | 오목한 점들의 개수(number of concave portions of contour) | . symmetry | 대칭도 | . fractal dimension | 프랙탈 차원($ text{&quot;coastline approximation&quot;} - 1$) | . cancer[&quot;target_names&quot;] . array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;) . malignant : 악성 ($0$) | benign : 양성 ($1$) | . - 데이터와 정답을 확인해보자. . data, target = cancer[&quot;data&quot;], cancer[&quot;target&quot;] . data . array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01, 1.189e-01], [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01, 8.902e-02], [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01, 8.758e-02], ..., [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01, 7.820e-02], [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01, 1.240e-01], [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01, 7.039e-02]]) . target . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]) . 1.2 EDA . df = pd.DataFrame(data, columns=cancer[&quot;feature_names&quot;]) df.describe() . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . count 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | ... | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | 569.000000 | . mean 14.127292 | 19.289649 | 91.969033 | 654.889104 | 0.096360 | 0.104341 | 0.088799 | 0.048919 | 0.181162 | 0.062798 | ... | 16.269190 | 25.677223 | 107.261213 | 880.583128 | 0.132369 | 0.254265 | 0.272188 | 0.114606 | 0.290076 | 0.083946 | . std 3.524049 | 4.301036 | 24.298981 | 351.914129 | 0.014064 | 0.052813 | 0.079720 | 0.038803 | 0.027414 | 0.007060 | ... | 4.833242 | 6.146258 | 33.602542 | 569.356993 | 0.022832 | 0.157336 | 0.208624 | 0.065732 | 0.061867 | 0.018061 | . min 6.981000 | 9.710000 | 43.790000 | 143.500000 | 0.052630 | 0.019380 | 0.000000 | 0.000000 | 0.106000 | 0.049960 | ... | 7.930000 | 12.020000 | 50.410000 | 185.200000 | 0.071170 | 0.027290 | 0.000000 | 0.000000 | 0.156500 | 0.055040 | . 25% 11.700000 | 16.170000 | 75.170000 | 420.300000 | 0.086370 | 0.064920 | 0.029560 | 0.020310 | 0.161900 | 0.057700 | ... | 13.010000 | 21.080000 | 84.110000 | 515.300000 | 0.116600 | 0.147200 | 0.114500 | 0.064930 | 0.250400 | 0.071460 | . 50% 13.370000 | 18.840000 | 86.240000 | 551.100000 | 0.095870 | 0.092630 | 0.061540 | 0.033500 | 0.179200 | 0.061540 | ... | 14.970000 | 25.410000 | 97.660000 | 686.500000 | 0.131300 | 0.211900 | 0.226700 | 0.099930 | 0.282200 | 0.080040 | . 75% 15.780000 | 21.800000 | 104.100000 | 782.700000 | 0.105300 | 0.130400 | 0.130700 | 0.074000 | 0.195700 | 0.066120 | ... | 18.790000 | 29.720000 | 125.400000 | 1084.000000 | 0.146000 | 0.339100 | 0.382900 | 0.161400 | 0.317900 | 0.092080 | . max 28.110000 | 39.280000 | 188.500000 | 2501.000000 | 0.163400 | 0.345400 | 0.426800 | 0.201200 | 0.304000 | 0.097440 | ... | 36.040000 | 49.540000 | 251.200000 | 4254.000000 | 0.222600 | 1.058000 | 1.252000 | 0.291000 | 0.663800 | 0.207500 | . 8 rows × 30 columns . cancer[&quot;feature_names&quot;] . array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;) . 양성과 악성의 비율은 다음과 같다. . pd.Series(cancer[&quot;target&quot;]).value_counts() . 1 357 0 212 dtype: int64 . 종양진단 결과를 나타내는 class 변수의 도수분포로부터 357 명의 관측치가 양성(benign, class=0)에 해당하고, 이보다 적은 212명의 관측치가 악성(malign, class=1)에 해당함을 알 수 있습니다. . sns.countplot(x=target) plt.title(&quot;종양진단 class별 도수분포&quot;) plt.xlabel(&quot;class&quot;) plt.ylim([0, 450]) plt.text(-0.1, 220, &quot;악성&quot;) plt.text(.9, 370, &quot;양성&quot;) plt.show() . sns.boxplot(x=target, y=df[&quot;mean concave points&quot;]) plt.xlabel(&quot;class&quot;) . Text(0.5, 0, &#39;class&#39;) . 악성 종양세포에서 mean_concave_points 값이 훨씬 높은 편임을 알 수 있습니다. | . sns.boxplot(x=target, y=df[&quot;mean radius&quot;]) . &lt;AxesSubplot:ylabel=&#39;mean radius&#39;&gt; . 악성 종양세포에서 mean_radius 값이 훨씬 높은 편임을 알 수 있습니다. | . plt.scatter(x=df[&quot;mean concave points&quot;], y=df[&quot;mean radius&quot;], alpha=.5) plt.xlabel(&quot;mean_concave_points&quot;) plt.ylabel(&quot;mean_radius&quot;) . Text(0, 0.5, &#39;mean_radius&#39;) . 위의 그림은 mean_concave_points와 mean_radius 변수 사이의 강한 양의 상관관계가 있음을 보여줍니다. | . 1.3 Data Split . 데이터를 $7:3$의 비율로 train/test set으로 나누자 . from sklearn.model_selection import train_test_split train_data, test_data, train_target, test_target = train_test_split( data, target, train_size= 0.7, random_state=1001, ) . print(&quot;train data 개수:&quot;, len(train_data)) print(&quot;test data 개수:&quot;, len(test_data)) . train data 개수: 398 test data 개수: 171 . 2. Linear Regression and Categorical Label . Logistic Regression을 학습하기에 앞서 Linear Regression으로 학습할 경우 어떻게 되는지 보자. . from sklearn.linear_model import LinearRegression linear_regressor = LinearRegression() . 2.1 &#54617;&#49845; . linear_regressor.fit(train_data, train_target) . LinearRegression() . 2.2 &#50696;&#52769; . train_pred = linear_regressor.predict(train_data) test_pred = linear_regressor.predict(test_data) . 예측 결과를 보면 $0 sim 1$ 사이를 벗어난 예측값이 보이는데..? | 일단 넘어가자.. | . 2.3 &#49884;&#44033;&#54868; . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5)) preds = [ (&quot;Train&quot;, train_data, train_pred), (&quot;Test&quot;, test_data, test_pred), ] for idx, (name, d, pred) in enumerate(preds): ax = axes[idx] ax.scatter(x=d[:,0], y=pred) ax.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.set_xlabel(&#39;mean_radius&#39;) ax.set_ylabel(&#39;predict&#39;) ax.set_title(f&quot;{name} Data&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 2.4 &#54217;&#44032;&#54616;&#44592; . Linear Regression의 성능을 측정하기 위해서는 우선 예측값을 0과 1로 변환시켜줘야 합니다. . Youden&#39;s Index를 이용해 Best Threshold를 찾은 후, 0과 1로 변화시킨 후 정확도를 비교해보자. . ($ star$)Youden&#39;s J statistic . 참고링크: https://en.wikipedia.org/wiki/Youden%27s_J_statistic . $$ J = text{sensitivity} + text{specificity} - 1$$ . $$ J = frac{ text{True positives}}{ text{True positives}+ text{False negatives}} + frac{ text{True negatives}}{ text{True negatives}+ text{False positives}} - 1$$ . . from sklearn.metrics import auc, roc_curve fpr, tpr, threshold = roc_curve(train_target, train_pred) auroc = auc(fpr, tpr) . fpr . array([0. , 0. , 0. , 0.00699301, 0.00699301, 0.01398601, 0.01398601, 0.02097902, 0.02097902, 0.02797203, 0.02797203, 0.04195804, 0.04195804, 0.06993007, 0.06993007, 0.12587413, 0.12587413, 1. ]) . tpr . array([0. , 0.00392157, 0.70980392, 0.70980392, 0.90980392, 0.90980392, 0.93333333, 0.93333333, 0.96862745, 0.96862745, 0.98431373, 0.98431373, 0.99215686, 0.99215686, 0.99607843, 0.99607843, 1. , 1. ]) . threshold . array([ 2.35725299, 1.35725299, 0.82521489, 0.82466702, 0.70119883, 0.69924511, 0.67546196, 0.67513605, 0.63707749, 0.63401065, 0.61560836, 0.59241528, 0.58134679, 0.50969629, 0.50796669, 0.43013177, 0.42764525, -0.53936311]) . AUROC를 그려보자. . plt.plot(fpr, tpr) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . AUROC 값을 계산하면 다음과 같습니다. . print(f&quot;AUROC : {auroc: .4f}&quot;) . AUROC : 0.9960 . 이제 Best Threshold를 계산해보자. . np.argmax(tpr - fpr) . 10 . 10인 index 에서 Best Threshold를 갖는다는 의미 | . J = tpr - fpr idx = np.argmax(J) best_thresh = threshold[idx] print(f&quot;Best Threshold is {best_thresh:.4f}&quot;) print(f&quot;Best Threshold&#39;s sensitivity is {tpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s specificity is {1-fpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s J is {J[idx]:.4f}&quot;) . Best Threshold is 0.6156 Best Threshold&#39;s sensitivity is 0.9843 Best Threshold&#39;s specificity is 0.9720 Best Threshold&#39;s J is 0.9563 . Best Threshold는 AUROC 그래프에서 직선이 가장 긴 곳입니다. . plt.plot(fpr, tpr) plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10)) plt.plot((fpr[idx], fpr[idx]), (fpr[idx], tpr[idx]), color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) plt.show() . 예측값에서의 Best threshold 의 위치를 그려보자 . fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) preds = [ (&quot;Train&quot;, train_data, train_pred), (&quot;Test&quot;, test_data, test_pred), ] for idx, (name, d, pred) in enumerate(preds): ax = axes[idx] ax.scatter(x=d[:,0], y=pred) ax.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax.set_xlabel(&quot;mean_radius&quot;) ax.set_ylabel(&quot;predict&quot;) ax.set_title(f&quot;{name} Data&quot;) ax.axhline(best_thresh, color=&quot;blue&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 이제 Threshold로 예측값을 $0,1$로 변환 후 정확도를 보자 . train_pred_label = list(map(int, (train_pred &gt; best_thresh))) test_pred_label = list(map(int, (test_pred &gt; best_thresh))) . from sklearn.metrics import accuracy_score linear_train_accuracy = accuracy_score(train_target, train_pred_label) linear_test_accuracy = accuracy_score(test_target, test_pred_label) . print(f&quot;Train accuracy is : {linear_train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {linear_test_accuracy:.2f}&quot;) . Train accuracy is : 0.98 Test accuracy is : 0.96 . 3. Logistic Regression . 이번에는 Logistic Regression을 이용하여 예측해 보자. . 3.1 Scaling . Logistic Regression은 학습하기에 앞서 학습시킬 데이터를 정규화해야 합니다. . Logistic Regressiond에는 exp가 있는데, exp는 값이 클 경우 overflow가 일어날 수 있기 때문입니다. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() . 정규화는 항상 train data를 이용하여 학습하고 valid, test 데이터를 변환해야 합니다. . 모든 데이터를 한번에 학습할 경우 본적이 없는 valiation data의 평균과 분산이 반영되고 이는 overfitting을 일으키는 원인이 됩니다. . scaler.fit(train_data) . StandardScaler() . 학습된 Scaler로 train/test 데이터를 변환합니다. . scaled_train_data = scaler.transform(train_data) scaled_test_data = scaler.transform(test_data) . train_data[0] . array([1.953e+01, 1.890e+01, 1.295e+02, 1.217e+03, 1.150e-01, 1.642e-01, 2.197e-01, 1.062e-01, 1.792e-01, 6.552e-02, 1.111e+00, 1.161e+00, 7.237e+00, 1.330e+02, 6.056e-03, 3.203e-02, 5.638e-02, 1.733e-02, 1.884e-02, 4.787e-03, 2.593e+01, 2.624e+01, 1.711e+02, 2.053e+03, 1.495e-01, 4.116e-01, 6.121e-01, 1.980e-01, 2.968e-01, 9.929e-02]) . scaled_train_data[0] . array([ 1.55665013, -0.08374209, 1.56894905, 1.61738288, 1.35230219, 1.15579113, 1.64920637, 1.53999266, -0.05508639, 0.36872483, 2.57200558, -0.10081561, 2.13099893, 1.96746196, -0.29563095, 0.3786633 , 0.72463661, 0.88545529, -0.19263957, 0.37716933, 2.07668666, 0.10014731, 1.96594854, 2.15156304, 0.75699447, 1.00247978, 1.60389716, 1.3188727 , 0.1245053 , 0.85035853]) . 3.2 &#54617;&#49845; . 이제 표준화된 데이터로 Logistic Regression을 학습해 보자. . from sklearn.linear_model import LogisticRegression logit_regressor = LogisticRegression() . logit_regressor.fit(scaled_train_data, train_target) . LogisticRegression() . 3.3 &#50696;&#52769; . Classification을 하는 모델의 경우 예측을 하는 방법은 두가지가 있습니다. . predict . | predict_proba . | predict는 해당 데이터가 어떤 class로 분류할지 바로 알려줍니다. . 반면, predict_proba는 각 class에 속할 확률을 보여줍니다. . train_pred = logit_regressor.predict(scaled_train_data) test_pred = logit_regressor.predict(scaled_test_data) . train_pred[:10] . array([0, 1, 0, 0, 1, 0, 1, 0, 0, 1]) . train_pred_logit = logit_regressor.predict_proba(scaled_train_data) test_pred_logit = logit_regressor.predict_proba(scaled_test_data) . train_pred_logit[:10] . array([[9.99999984e-01, 1.62885674e-08], [1.30750892e-03, 9.98692491e-01], [9.93452400e-01, 6.54760017e-03], [6.39996411e-01, 3.60003589e-01], [5.71378493e-05, 9.99942862e-01], [9.96253495e-01, 3.74650484e-03], [5.02851011e-04, 9.99497149e-01], [9.95986445e-01, 4.01355535e-03], [9.99998296e-01, 1.70356931e-06], [7.13730423e-04, 9.99286270e-01]]) . 각 class에 속할 확률은 다음과 같습니다. . 현재 데이터의 경우 악성과 양성 2개의 클래스가 있기 때문에 2개의 확률이 나타납니다. . 만약 첫 번째 class에 속할 확률이 크다면 데이터는 0번 클래스에 속하게 되는 것..! . train_pred_logit[0] . array([9.99999984e-01, 1.62885674e-08]) . 3.4 &#54217;&#44032; . 데이터의 AUROC를 계산하기 위해서는 1의 클래스로 분류될 확률 하나만 필요합니다. 반면 우리가 갖고 있는 예측값은 0과 1로 분류될 확률을 모두 표시하고 있습니다. 그래서 1에 속할 확률만 남기겠습니다. . train_pred_logit = train_pred_logit[:, 1] test_pred_logit = test_pred_logit[:, 1] . train_pred_logit[0] . 1.628856736535896e-08 . from sklearn.metrics import auc, roc_curve fpr, tpr, threshold = roc_curve(train_target, train_pred_logit) auroc = auc(fpr, tpr) . plt.plot(fpr, tpr) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . print(f&quot;AUROC : {auroc:.4f}&quot;) . AUROC : 0.9971 . J = tpr - fpr idx = np.argmax(J) best_thresh = threshold[idx] print(f&quot;Best Threshold is {best_thresh:.4f}&quot;) print(f&quot;Best Threshold&#39;s sensitivity is {tpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s specificity is {1-fpr[idx]:.4f}&quot;) print(f&quot;Best Threshold&#39;s J is {J[idx]:.4f}&quot;) . Best Threshold is 0.5692 Best Threshold&#39;s sensitivity is 0.9961 Best Threshold&#39;s specificity is 0.9790 Best Threshold&#39;s J is 0.9751 . plt.plot(fpr, tpr) plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10)) plt.plot((fpr[idx],fpr[idx]), (fpr[idx], tpr[idx]), color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;fpr&quot;) plt.ylabel(&quot;tpr&quot;) . Text(0, 0.5, &#39;tpr&#39;) . plt.scatter(x=scaled_train_data[:,0], y=train_pred_logit) plt.axhline(best_thresh, color=&quot;blue&quot;) plt.axhline(0, color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.axhline(1, color=&quot;red&quot;, linestyle=&quot;--&quot;) plt.xlabel(&quot;mean radius&quot;) plt.ylabel(&quot;Probability&quot;) plt.show() . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . 이제 Threshold로 예측값을 0,1로 변환 후 정확도를 보겠습니다. . train_pred_label = list(map(int, (train_pred_logit &gt; best_thresh))) test_pred_label = list(map(int, (test_pred_logit &gt; best_thresh))) . proba_train_accuracy = accuracy_score(train_target, train_pred_label) proba_test_accuracy = accuracy_score(test_target, test_pred_label) . print(f&quot;Train accuracy is : {proba_train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {proba_test_accuracy:.2f}&quot;) . Train accuracy is : 0.99 Test accuracy is : 0.98 . 이번에는 predict의 결과값으로 정확도를 보겠습니다. . train_accuracy = accuracy_score(train_target, train_pred) test_accuracy = accuracy_score(test_target, test_pred) . print(f&quot;Train accuracy is : {train_accuracy:.2f}&quot;) print(f&quot;Test accuracy is : {test_accuracy:.2f}&quot;) . Train accuracy is : 0.99 Test accuracy is : 0.97 . predict_proba의 best_threshold로 계산한 결과와 predict로 계산한 결과가 다를 수 있습니다. 이는 두 0과 1로 예측하는 방법이 다르기 때문입니다. 예를 들어서 (0.49, 0.51)의 확률이 있을 때 predict의 경우 class 1의 확률에 속할 확률이 크기 때문에 1로 분류합니다. 하지만 best_threshold가 0.52라면 predict_proba의 경우 class를 0으로 분류하게 됩니다 . 4. &#47560;&#47924;&#47532; . 세개의 모델들의 정확도를 비교해 보겠습니다. . print(f&quot;Linear Regression Test Accuracy: {linear_test_accuracy:.2f}&quot;) print(f&quot;Logistic Regression predict_proba Test Accuracy: {proba_test_accuracy:.2f}&quot;) print(f&quot;Logistic Regression predict Test Accuracy: {test_accuracy:.2f}&quot;) . Linear Regression Test Accuracy: 0.96 Logistic Regression predict_proba Test Accuracy: 0.98 Logistic Regression predict Test Accuracy: 0.97 .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/04/26/ML.html",
            "relUrl": "/python/2022/04/26/ML.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(3주차 ML) 3월 24일",
            "content": "&#45336;&#54028;&#51060;&#47196; &#45936;&#51060;&#53552; &#51456;&#48708; . import numpy as np . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = np.column_stack((fish_length, fish_weight)) . fish_data[:5] . array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ]]) . fish_target = np.concatenate((np.ones(35), np.zeros(14))) . fish_target . array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#45936;&#51060;&#53552; &#45208;&#45572;&#44592; . from sklearn.model_selection import train_test_split ## model selection 모듈 아래에 train_test_split 함수 . train_input, test_input, train_target, test_target = train_test_split( fish_data, fish_target, stratify = fish_target, random_state = 42) .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/24/ML.html",
            "relUrl": "/python/2022/03/24/ML.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(3주차 DV) 3월 23일",
            "content": "- How to upload your CSV file online for data analysis . https://evidencen.com/how-to-upload-your-csv-file-online/ . &#45800;&#51068;&#48320;&#49688; . import seaborn as sns import pandas as pd . df1 = pd.read_csv(&#39;https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/Data03.csv&#39;) df1.head() . id type_of_contract type_of_contract2 channel datetime Term payment_type product amount state overdue_count overdue credit rating bank cancellation age Mileage . 0 66758234 | 렌탈 | Normal | 서비스 방문 | 2019-10-20 | 60 | CMS | K1 | 96900 | 계약확정 | 0 | 없음 | 9.0 | 새마을금고 | 정상 | 43.0 | 1862.0 | . 1 66755948 | 렌탈 | Extension_Rental | 서비스 방문 | 2019-10-20 | 60 | 카드이체 | K1 | 102900 | 계약확정 | 0 | 없음 | 2.0 | 현대카드 | 정상 | 62.0 | 2532.0 | . 2 66756657 | 렌탈 | Normal | 홈쇼핑/방송 | 2019-10-20 | 60 | CMS | K1 | 96900 | 계약확정 | 0 | 없음 | 8.0 | 우리은행 | 정상 | 60.0 | 2363.0 | . 3 66423450 | 멤버십 | TAS | 렌탈재계약 | 2019-10-20 | 12 | CMS | K1 | 66900 | 계약확정 | 0 | 없음 | 5.0 | 농협은행 | 정상 | 60.0 | 2449.0 | . 4 66423204 | 멤버십 | TAS | 렌탈재계약 | 2019-10-20 | 12 | CMS | K1 | 66900 | 해약확정 | 12 | 있음 | 8.0 | 농협은행 | 해약 | 51.0 | 1942.0 | . 1 &#48276;&#51452;&#54805; &#48320;&#49688; . df1[&#39;type_of_contract&#39;].value_counts() . 렌탈 46481 멤버십 4819 Name: type_of_contract, dtype: int64 . sns.countplot(data=df1, x=&#39;type_of_contract&#39;) . &lt;AxesSubplot:xlabel=&#39;type_of_contract&#39;, ylabel=&#39;count&#39;&gt; . C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 47116 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 53448 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 47716 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 48260 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:240: RuntimeWarning: Glyph 49901 missing from current font. font.set_text(s, 0.0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 47116 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 53448 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 47716 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 48260 missing from current font. font.set_text(s, 0, flags=flags) C: Users 82103 anaconda3 envs py38r40 lib site-packages matplotlib backends backend_agg.py:203: RuntimeWarning: Glyph 49901 missing from current font. font.set_text(s, 0, flags=flags) . 한글 글씨체 설정이 필요해 보인다. | . df1[&#39;product&#39;].value_counts() . K1 39134 K2 8995 K3 2082 K5 645 K4 327 K6 120 Name: product, dtype: int64 . sns.countplot(data=df1, x=&#39;product&#39;, hue=&#39;type_of_contract2&#39;) . &lt;AxesSubplot:xlabel=&#39;product&#39;, ylabel=&#39;count&#39;&gt; . 범례와 그래프가 겹쳐나오는 문제가 발생 $ to$ matplotlib 옵션으로 해결 가능 | . &#54620;&#44544; &#44648;&#51664; &#54644;&#44208; . import matplotlib.pyplot as plt ## 파이썬 내에서 그래프 출력시 디테일한 옵션 import matplotlib as mpl ## 한글폰트 설정, 글씨체 흐릿한 것을 선명하게 (전체적인 큰 틀의 옵션) . mpl.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) ## 맑은 고딕 . C드라이브 $ to$ Windows $ to$ Fonts $ to$ Malgun Gothic | . sns.countplot(data=df1, x=&#39;type_of_contract&#39;) . &lt;AxesSubplot:xlabel=&#39;type_of_contract&#39;, ylabel=&#39;count&#39;&gt; . &#45936;&#51060;&#53552; &#44536;&#47000;&#54532; &#44217;&#52840;&#47928;&#51228; . - 그래프 사이즈 키우기 . plt.figure(figsize=[10,5]) ## Size 조정 sns.countplot(data=df1, x=&#39;product&#39;, hue=&#39;type_of_contract2&#39;) plt.legend(loc=&#39;right&#39;) ## 범례 오른쪽에 위치 plt.savefig(&#39;img1.png&#39;) ## 이미지 파일 형태로 저장 # plt.savefig(&#39;img1.pdf&#39;) . 2 &#50672;&#49549;&#54805; &#48320;&#49688; . sns.histplot(data=df1, x=&#39;age&#39;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;Count&#39;&gt; . - kde = True 옵션 추가 . plt.title(&#39;계약 유형 별. 고객 연령 분포&#39;) sns.histplot(data=df1, x=&#39;age&#39;, kde = True, hue=&#39;type_of_contract&#39;) ## 확률분포선 (kde=True) plt.show() . 3 &#44536; &#50808; &#52628;&#44032; &#50741;&#49496;&#46308; . - 그래프 축에 있는 글씨 겹침 . sns.countplot(data=df1, x=&#39;bank&#39;) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=[5, 10]) sns.countplot(data=df1, y=&#39;bank&#39;) . &lt;AxesSubplot:xlabel=&#39;count&#39;, ylabel=&#39;bank&#39;&gt; . bank column에 데이터가 굉장히 많다. 빈도수가 높은 상위 10개 데이터만 뽑아서 시각화를 해보자. | . - 빈도가 높은 순으로 정렬 . df1[&#39;bank&#39;].value_counts() ## 빈도수가 높은 순으로 출력 . 국민은행 9901 롯데카드 9518 농협은행 6278 신한은행 3522 우리은행 3386 기업은행 1963 신한카드 1533 하나은행 1446 국민카드 1311 BC카드 1264 새마을금고 964 부산은행 888 삼성카드 884 현대카드 876 대구은행 746 우체국 717 외환은행 586 외환카드 530 경남은행 442 SC제일은행 439 광주은행 347 신협중앙회 341 전북은행 195 씨티은행 162 수협중앙회 160 제주은행 40 유안타증권 27 산업은행 23 현대증권 11 삼성증권 7 하나SK 6 미래에셋증권 5 NH농협카드 4 한국투자증권 4 신한금융투자 4 우리카드 3 대우증권 2 하이투자증권 1 메리츠종합금융증권 1 수협카드 1 상호저축은행 1 SK증권 1 하나대투증권 1 산림조합중앙회 1 대신증권 1 씨티카드 1 Name: bank, dtype: int64 . sns.countplot(data=df1, x=&#39;bank&#39;, order=[&#39;국민은행&#39;, &#39;롯데카드&#39;, &#39;농협은행&#39;, &#39;신한은행&#39;]) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . - 빈도 순 정렬 . order_list = df1[&#39;bank&#39;].value_counts().index.tolist() ## 빈도 수 높은 순으로 인덱스 출력 . sns.countplot(data=df1, x=&#39;bank&#39;, order=order_list) . &lt;AxesSubplot:xlabel=&#39;bank&#39;, ylabel=&#39;count&#39;&gt; . - 상위 10개만 시각화 . plt.figure(figsize=[10,5]) ## figure size 조정. sns.countplot(data=df1, x=&#39;bank&#39;, order=order_list[0:10]) plt.savefig(&#39;img10.pdf&#39;) ## 이미지 pdf 형태로 저장 . - 이미지 파일이 저장된 디렉토리 경로 . import os print(os.getcwd()) # print(os.listdir(os.getcwd())) . C: Users 82103 Desktop dino BP2022 _notebooks . os.path.exists(&#39;C:/Users/82103/Desktop/dino/BP2022/_notebooks/img10.pdf&#39;) . True . 따라서 위의 경로(현재 작업 폴더)에 img10.pdf 이미지 파일이 저장되어 있음을 확인할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/23/DV.html",
            "relUrl": "/python/2022/03/23/DV.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(3주차 ML) 3월 17일",
            "content": "training set / test set . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . len(fish_length), len(fish_weight) . (49, 49) . fish_data = [[l, w] for l, w in zip(fish_length, fish_weight)] fish_target = [1]*35 + [0]*14 . fish_data[0] . [25.4, 242.0] . KNN (&#52395;&#48264;&#51704; &#49884;&#46020;) . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() . print(fish_data[4]) . [29.0, 430.0] . print(fish_data[0:5]) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] . print(fish_data[:5]) . [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] . print(fish_data[44:]) . [[12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]] . train_input = fish_data[:35] ## index 0~34 train_target = fish_target[:35] test_input = fish_data[35:] ## index 35~48 test_target = fish_target[35:] . len(train_input), len(train_target) . (35, 35) . len(test_input), len(test_target) . (14, 14) . kn = kn.fit(train_input, train_target) kn.score(test_input, test_target) . 0.0 . 정확도가 0% 이다? | test input에 있는 샘플 14개를 모두 못 맞췄다는 것이다. | WHY ? $ Rightarrow$ 샘플링 편향 | . &#49368;&#54540;&#47553; &#54200;&#54693; . 왜그런가 봤더니 처음에 fish_length와 fish_weight를 도미 35개, 빙어 14개를 쭉 늘어놓고 두 리스트를 합쳤다. | 두 리스트를 합친 fish_data에서 앞에 35개를 훈련 뒤에 14개를 test set으로 잘랐다. | 즉, 훈련세트에는 빙어가 하나도 없고, 테스트셋에는 도미가 하나도 없게 된다. $ Rightarrow$ 미적분 공부하고 확통시험 본 격.. | . | train set과 test set을 나눌 때에는 빙어와 도미 두 class가 잘 섞여있도록 만들어야 한다. | . Numpy . 이제 Numpy를 이용해서 잘 섞어서 train set과 test set으로 나눠보자. | Numpy는 파이썬의 대표적인 배열 library | scikit-learn이나 matplotlib librayr도 넘파이에 크게 의존하고 있고, 입력 데이터가 Numpy로 전달될 거라고 가정하고 있다. predict method 결과값이 array([1])이런 형태로 출력되는 것도 이러한 이유.(사이킷런의 predict 메서드의 반환값을 넘파이 배열로 리턴) | . | 딥러닝 TensorFlow도 Numpy와도 타이트한 관계가 있다. | . . 1차원 배열(벡터), 2차원 배열(행렬), 3차원 배열 | . training set / test set (using Numpy) . input과 target이 함께 섞여서 이동을 해야한다. (섞여야 한다) | 지도학습에서 입력과 타겟이 쌍을 이루고 있게 되는데 따로따로 섞여버리면 정답을 제대로 못주게 되서 엉터리 훈련이 되버린다. | 입력데이터 특성값과 타깂값이 쌍으로 잘 따라서 섞이도록 만들어야 하는것이 중요!!! index를 섞어 분리하는 방법 | . | . import numpy as np . input_arr = np.array(fish_data) target_arr = np.array(fish_target) . print(input_arr) . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 6.7] [ 10.5 7.5] [ 10.6 7. ] [ 11. 9.7] [ 11.2 9.8] [ 11.3 8.7] [ 11.8 10. ] [ 11.8 9.9] [ 12. 9.8] [ 12.2 12.2] [ 12.4 13.4] [ 13. 12.2] [ 14.3 19.7] [ 15. 19.9]] . print(input_arr.shape) . (49, 2) . - 0~48까지 정수로된 index 만들기 . index = np.arange(49) index . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]) . - index를 섞어준다. . np.random.seed(42) np.random.shuffle(index) . print(index) . [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] . 잘 섞였다... | . input_arr[:5] . array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ]]) . print(input_arr[[1,3]]) . [[ 26.3 290. ] [ 29. 363. ]] . - 랜덤하게 섞인 인덱스 배열에서 앞부분 35개를 훈련셋으로 두고, 뒷부분 14개를 테스트 셋으로 둔다 . print(index) . [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] . train_input = input_arr[index[:35]] train_target = target_arr[index[:35]] . print(input_arr[13], train_input[0]) . [ 32. 340.] [ 32. 340.] . test_input = input_arr[index[35:]] test_target = target_arr[index[35:]] . import matplotlib.pyplot as plt plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(test_input[:,0], test_input[:,1]) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . KNN (&#46160;&#48264;&#51704; &#49884;&#46020;) . kn = kn.fit(train_input, train_target) . kn.score(test_input, test_target) . 1.0 . 잘 훈련되었다. | . kn.predict(test_input) . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . test_target . array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) . Summary . Numpy를 이용해서 데이터를 섞어서 만들때, 배열 자체를 섞지 않고 (특성데이터와 타깃테이터가 쌍을 이루어서 섞어야 하므로) . | 배열의 인덱스배열을 만들어서 인덱스를 섞은 후에 . | 섞인 인덱스를 가지고 배열 슬라이싱을 하여 훈련세트와 테스트셋으로 나눈다. . | 이렇게 나눈 것으로 KNN으로 다시 훈련해서 모델을 평가 . | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/19/ML.html",
            "relUrl": "/python/2022/03/19/ML.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(3주차 DV) 3월 18일",
            "content": "import pandas as pd . pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/train.csv&#39;) . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1455 1456 | 60 | RL | 62.0 | 7917 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | WD | Normal | 175000 | . 1456 1457 | 20 | RL | 85.0 | 13175 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | MnPrv | NaN | 0 | 2 | 2010 | WD | Normal | 210000 | . 1457 1458 | 70 | RL | 66.0 | 9042 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | GdPrv | Shed | 2500 | 5 | 2010 | WD | Normal | 266500 | . 1458 1459 | 20 | RL | 68.0 | 9717 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | 142125 | . 1459 1460 | 20 | RL | 75.0 | 9937 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 6 | 2008 | WD | Normal | 147500 | . 1460 rows × 81 columns . df_train = pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/train.csv&#39;) df_test = pd.read_csv(&#39;C:/Users/82103/Desktop/2022-1/머신러닝/data/test.csv&#39;) . from pandas.core.groupby.generic import ScalarResult import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np from scipy.stats import norm from sklearn.preprocessing import StandardScaler from scipy import stats import warnings warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline . df_train.columns . Index([&#39;Id&#39;, &#39;MSSubClass&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;MasVnrArea&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;KitchenQual&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageYrBlt&#39;, &#39;GarageFinish&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;PoolQC&#39;, &#39;Fence&#39;, &#39;MiscFeature&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) . df_test.columns . Index([&#39;Id&#39;, &#39;MSSubClass&#39;, &#39;MSZoning&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;MasVnrArea&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;KitchenQual&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Functional&#39;, &#39;Fireplaces&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageYrBlt&#39;, &#39;GarageFinish&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;PoolQC&#39;, &#39;Fence&#39;, &#39;MiscFeature&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;], dtype=&#39;object&#39;) . df_train[&#39;SalePrice&#39;].describe() ## 부동산 가격의 기술통계량 . count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64 . sns.distplot(df_train[&#39;SalePrice&#39;]) ## line : kernel density plot ## 목표변수에 대한 히스토그램과 kernel density plot . print(&quot;Skewness: %f&quot; % df_train[&#39;SalePrice&#39;].skew()) print(&quot;Kurtosis: %f&quot; % df_train[&#39;SalePrice&#39;].kurt()) ## 꼬리가 두터운 정도 (이상치가 많을수록 두꺼움) . Skewness: 1.882876 Kurtosis: 6.536282 . $-2 sim2$ 사이의 값이므로 치우침이 없는 데이터 (by. George &amp; Mallery, 2010) | 첨도가 높으면 (Kurtosis &gt; 3) 이상치가 많이 있다는 것. | . 많은 통계기법들이 정규성을 가정한다. | . positive skewness : 오른쪽 꼬리, 왼쪽에 데이터가 많다. | negative skewness : 왼쪽 꼬리, 오른쪽에 데이터가 많다. | . 왜도, 첨도 읽어보기 . . . var1 = &#39;GrLivArea&#39; # 지상 거실 면적 평방피트 data1 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var1]], axis=1) ## 열로 합치기(axis=1) . data1.plot.scatter(x=var1, y=&#39;SalePrice&#39;, ylim=(0,800000)) . &lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;, ylabel=&#39;SalePrice&#39;&gt; . linear relationship | . var2 = &#39;TotalBsmtSF&#39; # 지하 총 평방 피트 data2 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var2]], axis=1) # 열기준으로 붙임 data2.plot.scatter(x=var2, y=&#39;SalePrice&#39;, ylim=(0,800000)) . &lt;AxesSubplot:xlabel=&#39;TotalBsmtSF&#39;, ylabel=&#39;SalePrice&#39;&gt; . 더 strong 한 linear relationship ( 더 가파르다. ) | . var1 = &#39;GrLivArea&#39; data1 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var1]], axis=1) plt.scatter(var1, y=&#39;SalePrice&#39;, data = data1) . &lt;matplotlib.collections.PathCollection at 0x24f2102bca0&gt; . var2 = &#39;TotalBsmtSF&#39; data2 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var2]], axis=1) plt.scatter(var2, y=&#39;SalePrice&#39;, data = data2) . &lt;matplotlib.collections.PathCollection at 0x24f21535ac0&gt; . data2 . SalePrice OverallQual . 0 208500 | 7 | . 1 181500 | 6 | . 2 223500 | 7 | . 3 140000 | 7 | . 4 250000 | 8 | . ... ... | ... | . 1455 175000 | 6 | . 1456 210000 | 6 | . 1457 266500 | 7 | . 1458 142125 | 5 | . 1459 147500 | 5 | . 1460 rows × 2 columns . var3 = &#39;OverallQual&#39; # 전체 제료 및 마감품질 data3 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var3]], axis=1) # 열로 합치기 f, ax = plt.subplots(figsize=(8,6)) fig = sns.boxplot(x=var3, y=&quot;SalePrice&quot;, data=data3) fig.axis(ymin=0, ymax=800000) . (-0.5, 9.5, 0.0, 800000.0) . var4 = &#39;YearBuilt&#39; # 원래 건설 날짜 data4 = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var4]], axis=1) f, ax = plt.subplots(figsize=(16,8)) fig = sns.boxplot(x=var4, y=&quot;SalePrice&quot;, data=data4) fig.axis(ymin=0, ymax=800000) plt.xticks(rotation=90) # x축 눈금 값 90도 회전. .",
            "url": "https://pinkocto.github.io/BP2022/2022/03/18/DV.html",
            "relUrl": "/2022/03/18/DV.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(2주차 ML) 3월 10일",
            "content": "&#49373;&#49440; &#48516;&#47448; &#47928;&#51228; . &#46020;&#48120;(bream) &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] . import matplotlib.pyplot as plt plt.scatter(bream_length, bream_weight) plt.xlabel(&#39;length&#39;) # 몸 길이 plt.ylabel(&#39;weight&#39;) # 몸 무게 . Text(0, 0.5, &#39;weight&#39;) . &#48729;&#50612;(smelt) &#45936;&#51060;&#53552; &#51456;&#48708;&#54616;&#44592; . smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . plt.scatter(bream_length, bream_weight, label=&#39;bream&#39;) ## 도미 plt.scatter(smelt_length, smelt_weight, label=&#39;smelt&#39;) ## 빙어 plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.legend() plt.show() . 빙어는 길이가 늘어나더라도 무게가 많이 늘지 않는다. $ Rightarrow$ 빙어의 산점도 역시 선형적이지만 무게가 길이에 영향을 덜 받는다. | . binary classification (&#46020;&#48120;, &#48729;&#50612;) &#51456;&#48708; . 다음으로 데이터만 보고 어떤 것이 도미이고 어떤 것이 빙어인지 스스로 구분하기 위해 프로그램을 만들어 보자! | KNN(K-Nearest Neighbors) 방법을 이용할 것. | . - 우선 KNN 알고리즘을 써먹으려면 도미와 빙어 데이터를 하나의 데이터로 합쳐야 한다. . length = bream_length + smelt_length weight = bream_weight + smelt_length . + 연산자가 list 일 경우에는 합쳐지는 역할을 하고, 정수일 때는 우리가 일반적으로 알고있는 덧셈 연산을 한다. | . len(bream_length), len(smelt_length), len(bream_weight), len(smelt_length) . (35, 14, 35, 14) . len(length), len(weight) . (49, 49) . 잘 합쳐진 것 같다. | . - 2차원 리스트로 만들어 보자. (Scikit-learn을 사용하기위해) . ## 이런 식으로 2차원 리스트로 만들것! 길이 무게 [[25.4, 242.0], [26.3, 290.0]. . . . . . . [15.0, 19.9]] . fish_data = [[l,w] for l, w in zip(length, weight)] . - 정답 준비 . 도미(bream)를 1로 놓고, 빙어(smelt)를 0으로 놓자. (0과 1로 분류하는 이진분류) | . fish_target = [1]*35 + [0]*14 . K-&#52572;&#44540;&#51217; &#51060;&#50883; . from sklearn.neighbors import KNeighborsClassifier . kn = KNeighborsClassifier() # class의 instance(객체) 를 만든다. . kn.fit(fish_data, fish_target) ## kn을 모델이라 부름 . KNeighborsClassifier() . 머신러닝 프로그램의 알고리즘이 객체화 된것을 모델이라고 부른다. | 종종 그 알고리즘 자체를 모델이라고도 부름. | . kn.score(fish_data, fish_target) . 1.0 . 100% 다 맞췄다! (100% 정확도 달성!) | . &#49352;&#47196;&#50868; &#49373;&#49440; &#50696;&#52769; . 그래프에 표시된 초록색 삼각형은 어떤 생선일까? | . plt.scatter(bream_length, bream_weight, label=&#39;bream&#39;) plt.scatter(smelt_length, smelt_weight, label=&#39;smelt&#39;) plt.scatter(30, 600, marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.legend() plt.show() . 직관적으로 봤을때 도미(bream) 일 것 같다. | 실제로도 그런지 확인해보자. | . kn.predict([[30, 600]]) ## predict method . array([1]) . predict method 안에 넣을 때도 2차원 배열 데이터를 넣어준다. (사이킷런이 기대하는 것) | n_neighbors=5가 default, 주위에 있는 이웃의 개수(K)만큼 주변 샘플의 class 중 가장 많은 클래스를 정답클래스로 삼는다. | . print(kn._fit_X) . [[ 25.4 242. ] [ 26.3 290. ] [ 26.5 340. ] [ 29. 363. ] [ 29. 430. ] [ 29.7 450. ] [ 29.7 500. ] [ 30. 390. ] [ 30. 450. ] [ 30.7 500. ] [ 31. 475. ] [ 31. 500. ] [ 31.5 500. ] [ 32. 340. ] [ 32. 600. ] [ 32. 600. ] [ 33. 700. ] [ 33. 700. ] [ 33.5 610. ] [ 33.5 650. ] [ 34. 575. ] [ 34. 685. ] [ 34.5 620. ] [ 35. 680. ] [ 35. 700. ] [ 35. 725. ] [ 35. 720. ] [ 36. 714. ] [ 36. 850. ] [ 37. 1000. ] [ 38.5 920. ] [ 38.5 955. ] [ 39.5 925. ] [ 41. 975. ] [ 41. 950. ] [ 9.8 9.8] [ 10.5 10.5] [ 10.6 10.6] [ 11. 11. ] [ 11.2 11.2] [ 11.3 11.3] [ 11.8 11.8] [ 11.8 11.8] [ 12. 12. ] [ 12.2 12.2] [ 12.4 12.4] [ 13. 13. ] [ 14.3 14.3] [ 15. 15. ]] . print(kn._y) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] . &#47924;&#51312;&#44148; &#46020;&#48120; . Fish 데이터의 총 개수는 49개이다. 이번에는 n_neighbors = 49로 지정 해보자. . kn49 = KNeighborsClassifier(n_neighbors=49) . kn49.fit(fish_data, fish_target) . KNeighborsClassifier(n_neighbors=49) . kn49.score(fish_data, fish_target) ## 점수 . 0.7142857142857143 . score method : 훈련한 모델을 가지고 어떤 데이터를 집어 넣어서 얼마만큼 잘 맞는지를 확인해 보는 것이다. | 분류문제일 경우에는 정확도를 출력 (모델이 어느정도 정확한지를 알아보는 메서드.) | . print(35/49) . 0.7142857142857143 . 이렇게 모델을 만들면 전체 샘플의 다수는 도미 $ to$ 무조건 다 도미 | n_neighbors 매개변수로 주위의 샘플개수를 바꿔볼 수도 있다. 바꾸면 알고리즘의 정확도가 높을수록, 낮을수도 있다. | . &#54869;&#51064; &#47928;&#51228; . kn = KNeighborsClassifier() kn.fit(fish_data, fish_target) for n in range(5, 50): # 최근접 이웃 개수 설정 kn.n_neighbors = n #접수 계산 score = kn.score(fish_data, fish_target) # 100% 정확도에 미치지 못하는 이웃 개수 출력 if score &lt; 1: print(n, score) break . 18 0.9795918367346939 .",
            "url": "https://pinkocto.github.io/BP2022/2022/03/17/ML.html",
            "relUrl": "/2022/03/17/ML.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "tips",
            "content": "1. csv&#54028;&#51068; upload . 참고링크 : https://evidencen.com/how-to-upload-your-csv-file-online/ . import pandas as pd . autompg.csv github notebooks에 upload | autommpg.csv 파일 click | Raw / Blame에서 Raw 버튼 click | 상단의 링크 복사 | . 복사한 주소 : https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/autompg.csv . pd.read_csv(&quot;https://raw.githubusercontent.com/pinkocto/BP2022/master/_notebooks/autompg.csv&quot;) . mpg cyl disp hp wt accler year origin carname . 0 18.0 | 8 | 307.0 | 17 | 3504 | 12.0 | 70 | 1 | chevrolet chevelle malibu | . 1 15.0 | 8 | 350.0 | 35 | 3693 | 11.5 | 70 | 1 | buick skylark 320 | . 2 18.0 | 8 | 318.0 | 29 | 3436 | 11.0 | 70 | 1 | plymouth satellite | . 3 16.0 | 8 | 304.0 | 29 | 3433 | 12.0 | 70 | 1 | amc rebel sst | . 4 17.0 | 8 | 302.0 | 24 | 3449 | 10.5 | 70 | 1 | ford torino | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 393 27.0 | 4 | 140.0 | 82 | 2790 | 15.6 | 82 | 1 | ford mustang gl | . 394 44.0 | 4 | 97.0 | 53 | 2130 | 24.6 | 82 | 2 | vw pickup | . 395 32.0 | 4 | 135.0 | 80 | 2295 | 11.6 | 82 | 1 | dodge rampage | . 396 28.0 | 4 | 120.0 | 75 | 2625 | 18.6 | 82 | 1 | ford ranger | . 397 31.0 | 4 | 119.0 | 78 | 2720 | 19.4 | 82 | 1 | chevy s-10 | . 398 rows × 9 columns . 깃헙에 업로드 한 csv파일을 잘 읽어오는 것을 확인 할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/03/02/upload.html",
            "relUrl": "/python/2022/03/02/upload.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "(6주차) 2월18일 (3)",
            "content": "- 파일과 경로 . - 텍스트 파일 열기, 쓰기, 읽기 . - Tkinter 파일 다이얼로그를 이용한 예제 . &#54028;&#51068; . 목적 자료를 영구히 보관하기 위해 사용됨 | . | . 종류: 저장된 데이터에 따라 텍스트 파일 일반적인 문자 코드 | . | 이진 파일 사진, 음악, 비디오 | . | . | . 파일의 위치 폴더에 존재 | 폴더는 다른 폴더에 포함되어 있음 | 경로 : 어떤 폴더로부터 그 파일에 이르는 방법 | . | . - &#54028;&#51068; &#44221;&#47196; . * 경로의 종류 . 절대 경로: 루트 폴더로투터 시작 ex) c: users user documents hello.py | . | . 상대 경로: 현재 폴더로부터 시작 | . * 폴더를 표시하는 특수한 기호 . . : 현재 폴더를 의미함 | .. : 현재 폴더의 부모 (즉, 상위폴더를 의미함) | . &gt; &gt;&gt; import os &gt;&gt;&gt; os.getcwd() ## 현재 폴더 경로 &#39;C: USERS USER .... python3.7&#39;&gt; &gt;&gt; os.chdir(&quot;C: Users Users Documents&quot;) ## 현재폴더 경로 변경&gt; &gt;&gt; os.getcwd() ## 현재 폴더 경로 &quot;C: Users Users Documents&quot; ## 바뀐 경로로 출력된 것 확인 . - &#44221;&#47196; &#48516;&#47532; &#47928;&#51088; . Windows에서 경로 분리 문자는 문제는 Escape-Sequence를 나타낼 때 사용됨 | 그냥 문자를 표시하기 위해서는 를 사용 | . | . Unix, Linux 계열에서 경로 분리 문자는 / Windows 위에서 실행되는 파이썬에서 사용 가능 | 앞의 예는 &quot;C:/Users/Users/Documents&quot;로 써도 가능! | . | . Raw 문자열 사용방법 r&quot;문자열&quot;은 문자열 내 모든 특수문자를 무시하고 일반 문자로 취급함 | r&quot;C: Users Users Documents&quot;로 써도 가능! | . | . - &#54028;&#51068; &#50676;&#44592; . open() 이라는 내장 함수를 사용 | . fileVar = open(filename, mode) . open() 함수는 파일 열기를 성공하면 파일을 나타내는 객체를 반환함 _io.TextIoWrapper 클래스 객체임 | . | . . - &#54028;&#51068; &#50676;&#44592; &#47784;&#46300; . 파일열기모드 설명 . r (읽기모드) | 파일을 읽기만 할 때 사용한다 | . w (쓰기모드) | 파일에 내용을 쓸 때 사용하며 기존 파일이 존재하면 내용이 모두 초기화되고 &lt;/br&gt; 주어진 파일이 존재하지 않으면 새로운 파일을 만든다 | . a (추가모드) | 기존 파일의 마지막에 새로운 내용을 추가 시킬 때 사용한다 | . rb, wb | 각각 이진 파일을 읽기 위해 혹은 쓰기 위해 열때 사용한다 | . - &#54028;&#51068;&#50640; &#45936;&#51060;&#53552; &#50416;&#44592; . write() 메서드 사용 | . ofile = open(&quot;snowwhite.txt&quot;, &quot;w&quot;) # 1 ofile.write(&quot;Once upon a time, long, long ago n&quot;) # 2 ofile.write(&quot;a king and queen ruled over n&quot;) ofile.write(&quot;a distant land&quot;) ofile.close() . - &#54028;&#51068;&#51060; &#51316;&#51116;&#54616;&#45716;&#51648; &#44160;&#49324;&#54616;&#44592; . os.path.exist(경로) . 파일이 존재하면 True, 아니면 False를 반환 | . | os.path.isdir(경로) . 지정된 파일이 폴더이면 True, 아니면 False를 반환 | . | os.path.isfile(경로) . 지정된 파일이 일반 파일이면 True, 아니면 False를 반환 | . | . - &#54028;&#51068; &#51088;&#47308; &#51069;&#44592; . 파이썬에는 외부파일을 읽어 들여 프로그램에서 사용할 수 있는 여러가지 방법이 있다. . 전체 데이터를 읽는 메서드 : read(), reaadlines() . | 한 줄을 읽는 메서드 : readline() . | 주어진 길이를 읽는 메서드: read(n) . | . - &#51204;&#52404; &#51069;&#44592; . - 방법1: read() 함수 사용하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ## 읽기 readResult = ifile.read() print(&quot;Read Result:&quot;) print(repr(readResult)) ## repr(): 줄바꿈 문자도 그대로 출력 ifile.close() . Read Result: &#39;Once upon a time, long, long ago na king and queen ruled over na distant land&#39; . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ## 읽기 readResult = ifile.read() print(&quot;Read Result:&quot;) print(readResult) ifile.close() . Read Result: Once upon a time, long, long ago a king and queen ruled over a distant land . read() 는 파일의 내용 전체를 문자열로 돌려준다. | . - 방법2: readlines() 함수 사용하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) readLinesResult = ifile.readlines() print(&quot;Read Lines Result:&quot;) print(readLinesResult) ifile.close() . Read Lines Result: [&#39;Once upon a time, long, long ago n&#39;, &#39;a king and queen ruled over n&#39;, &#39;a distant land&#39;] . readline() 함수는 파일의 모든 줄을 읽어서 각각의 줄을 요소로 갖는 리스트로 돌려준다. | . - 방법2 + 줄바꿈( n) 문자 제거하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) lines = ifile.readlines() for line in lines: line = line.strip() # 줄 끝의 줄 바꿈 문자를 제거 print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . - &#51648;&#51221;&#46108; &#44600;&#51060;&#47564;&#53372; &#51069;&#44592; . file.read(n) : 현재 file pointer로부터 n개의 글자를 읽어서 문자열로 반환 | . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) str1 = ifile.read(4) print(&quot;Read(4) Result:&quot;) print(repr(str1)) str2 = ifile.read(10) print(&quot;Read(10) Result:&quot;) print(repr(str2)) ifile.close() . Read(4) Result: &#39;Once&#39; Read(10) Result: &#39; upon a ti&#39; . - &#54620; &#51460;&#50473; &#51069;&#44592; &#44208;&#44284; . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) ifile.readline() ## 1 . &#39;Once upon a time, long, long ago n&#39; . ifile.readline() ## 2 . &#39;a king and queen ruled over n&#39; . ifile.readline() ## 3 . &#39;a distant land&#39; . ifile.readline() ## 4 . &#39;&#39; . line이 끝에 다다르면 빈문자(&#39;&#39;)가 출력된다. | . - &#54028;&#51068;&#50640; &#47336;&#54532; &#49324;&#50857;&#54616;&#44592; . 루프를 사용하여 파일을 한 줄씩 처리하기 . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) line = ifile.readline() while line != &#39;&#39;: # line 처리 print(line) line = ifile.readline() ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) while True: line = ifile.readline() if not line: break print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . ifile = open(&quot;snowwhite.txt&quot;, &quot;r&quot;) lines = ifile.readlines() for line in lines: # line 처리 print(line) ifile.close() . Once upon a time, long, long ago a king and queen ruled over a distant land . - &#49707;&#51088;&#44032; &#46308;&#50612;&#44032; &#51080;&#45716; &#54028;&#51068; &#52376;&#47532; . ofile = open(&quot;num.txt&quot;, &quot;w&quot;) ofile.write(&quot;10 20 12 5 n&quot;) ofile.write(&quot;8 9 7 23 n&quot;) ofile.write(&quot;1 8 22 9&quot;) ofile.close() . num.txt 아래와 같은 파일이 있을 떄 파일에 있는 숫자의 합을 구해보자. | . ofile = open(&quot;num.txt&quot;, &quot;r&quot;) print(ofile.read()) ofile.close() . 10 20 12 5 8 9 7 23 1 8 22 9 . ofile = open(&quot;num.txt&quot;, &quot;r&quot;) total = 0 for line in ofile: lineLst = line.split() numList = [eval(x) for x in lineLst] total += sum(numList) print(total) . 134 . 10+20+12+5+8+9+7+23+1+8+22+9 . 134 . 잘 계산된 것을 확인할 수 있다. | .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/02/18/(3).html",
            "relUrl": "/python/2022/02/18/(3).html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "(6주차) 2월18일 (1)",
            "content": "- 소개 . - 2차원 리스트 처리 . - 2차원 리스트와 함수 . - 예제 . - 다차원 리스트 . &#49548;&#44060; . 테이블이나 행렬은 2차원 리스트로 표현할 수 있다. . - 대한민국 도시들 간 거리 . 서울 부산 대구 광주 . 서울 | 0 | 325 | 237 | 267 | . 부산 | 325 | 0 | 87 | 202 | . 대구 | 237 | 87 | 0 | 172 | . 광주 | 267 | 202 | 172 | 0 | . distance = [[0, 325, 237, 267], [325, 0, 87, 202], [237, 87, 0, 172], [267, 202, 172, 0]] . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52376;&#47532;&#54616;&#44592; . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#54364;&#54788; . matrix_ = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]] . - 각 요소는 두 개의 첨자를 이용하여 표현 . matrix_[0][2] . 3 . matrix_[2][3] . 14 . matrix_[1][1] . 7 . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52488;&#44592;&#54868; . - 사용자 입력 값으로 초기화 . - 무작위 값으로 초기화 . matrix = [] for row in range(3): ## number of rows = 3 matrix.append([]) for col in range(2): ## number of columns = 2 value = eval(input(&quot;value:&quot;)) matrix[row].append(value) . matrix ## 3행 2열의 2차원 리스트 . [[1, 2], [3, 4], [5, 6]] . import random matrix = [] numberOfRows=3 numberOfColumns=2 for row in range(numberOfRows): matrix.append([]) #print(matrix) for col in range(numberOfColumns): matrix[row].append(random.randint(0,99)) #print(matrix) . [[]] [[51]] [[51, 15]] [[51, 15], []] [[51, 15], [47]] [[51, 15], [47, 46]] [[51, 15], [47, 46], []] [[51, 15], [47, 46], [79]] [[51, 15], [47, 46], [79, 98]] . matrix . [[51, 15], [47, 46], [79, 98]] . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52636;&#47141;&#54616;&#44592;, &#49438;&#44592;, &#51221;&#47148; . - 2차원 리스트 출력 . len(matrix), len(matrix[0]) . (3, 2) . for row in range(len(matrix)): for col in range(len(matrix[row])): print(matrix[row][col], end=&#39; &#39;) print() . 51 15 47 46 79 98 . - 2차원 리스트 섞기 . matrix . [[51, 15], [47, 46], [79, 98]] . for row in range(len(matrix)): for col in range(len(matrix[row])): i = random.randint(0, len(matrix)-1) j = random.randint(0, len(matrix[row])-1) matrix[row][col], matrix[i][j] = matrix[i][j], matrix[row][col] . matrix . [[79, 98], [46, 15], [47, 51]] . 리스트 안의 원소들이 잘 섞여진 것을 확인할 수 있다. | . 2&#52264;&#50896; &#47532;&#49828;&#53944; &#52376;&#47532; . - 모든 원소들의 합 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . total = 0 for row in matrix: for value in row: total += value print(total) . 79 177 223 238 285 336 . total . 336 . - 각 열의 합 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . for col in range(len(matrix[0])): total = 0 for row in range(len(matrix)): total += matrix[row][col] print(&quot;Sum of column&quot;, col, &quot;is&quot;, total) . Sum of column 0 is 172 Sum of column 1 is 164 . 79+46+47 # 1열 합 . 172 . 98+15+51 # 2열 합 . 164 . 각 열의 합이 잘 계산되었다. | . - 합이 가장 큰 행 구하기 . matrix . [[79, 98], [46, 15], [47, 51]] . maxRow = sum(matrix[0]) indexRow = 0 for row in range(1, len(matrix)): if sum(matrix[row]) &gt; maxRow: maxRow = sum(matrix[row]) indexRow = row print(&quot;Row&quot;, indexRow, &quot;has max sum of &quot; , maxRow) . Row 0 has max sum of 177 . - 리스트 정렬 . lst = [[1,2],[1,1],[3,1],[2,5]] lst . [[1, 2], [1, 1], [3, 1], [2, 5]] . lst.sort() lst . [[1, 1], [1, 2], [2, 5], [3, 1]] . 2&#52264;&#50896; &#47532;&#49828;&#53944;&#50752; &#54632;&#49688; . 2차원 리스트도 다른 객체와 마찬가지로 함수에 인수로 전달할 수도 있고, 함수가 반환값으로 반환할 수도 있다. . def getMatrix(): matrix = [] numRows = eval(input(&quot;Number of Rows: &quot;)) numCols = eval(input(&quot;Number of Colunmns: &quot;)) for row in range(numRows): matrix.append([]) for col in range(numCols): value = eval(input(&quot;value:&quot;)) matrix[row].append(value) return matrix . getMatrix() . [[79, 98], [46, 15], [17, 51]] . def accumulate(m): total = 0 for row in m: total += sum(row) return total . accumulate(matrix) . 336 . matrix의 모든 원소의 합은 336으로 위에서 구한 (2차원 리스트 처리: 모든 원소의 합)에서 구한 값과 같다. | . &#50696;&#51228;1 | &#44032;&#51109; &#44032;&#44620;&#50868; &#46160; &#51216;&#51008;? . 여러 점에 대한 좌표가 있다. 이들 점 중에서 가장 가까운 두 점을 찾아보자. . def distance(x1,y1,x2,y2): return((x1-x2)**2 + (y1-y2)**2)**0.5 def nearestPoint(points): p1, p2 = 0, 1 shortestDist = distance(points[p1][0], points[p1][1], points[p2][0], points[p2][1]) for i in range(len(points)): for j in range(i+1, len(points)): d = distance(points[i][0], points[i][1], points[j][0], points[j][1]) if d &lt; shortestDist: shortestDist = d p1, p2 = i, j return p1, p2 . nPoints = eval(input(&quot;점의 수:&quot;)) points = [] for i in range(nPoints): point = [0,0] point[0],point[1] = eval(input(&quot;좌표:&quot;)) points.append(point) p1, p2 = nearestPoint(points) print(&quot;(&quot;, points[p1][0], points[p1][1],&quot;)&quot;,&quot;(&quot;,points[p2][0], points[p2][1],&quot;)&quot;) . ( 0 0 ) ( 0 1 ) . nPoints = eval(input(&quot;점의 수:&quot;)) points = [] for i in range(nPoints): point = [0,0] point[0],point[1] = eval(input(&quot;좌표:&quot;)) points.append(point) p1, p2 = nearestPoint(points) print(&quot;(&quot;, points[p1][0], points[p1][1],&quot;)&quot;,&quot;(&quot;,points[p2][0], points[p2][1],&quot;)&quot;) . ( 2 5 ) ( 3 8 ) . &#50696;&#51228;2 | Sudoku . . [게임 규칙] . 각 열, 각 행에 1~9까지 숫자가 들어가야 한다. | $3 times 3$ 블록에 1~9가지 숫자가 들어가야 한다. | . &#50612;&#46500; Sudoku &#54644;&#44208;&#48169;&#48277;&#51060; &#47582;&#45716;&#51648; &#44160;&#49324; . 두가지 검사방법 . 각 행, 열, 블록이 1~9까지 숫자를 포함하고 있는지 검사 | | 각 셀에 대해 그 셀의 숫자가 행, 열, 블록에서 유일한지 검사 &lt; 이 방법 사용할 것임! | | . | . def isValid(grid): for i in range(9): for j in range(9): if grid[i][j] &lt; 1 or grid[i][j] &gt; 9 or not isValidAt(i,j,grid): return False return True . def isValidAt(i,j,grid): for column in range(9): if column != j and grid[i][column] == grid[i][j]: return False for row in range(9): if row != i and grid[row][j] == grid[i][j]: return False for row in range((i//3)*3, (j//3)*3 + 3): if row != i and column != j and grid[row][column] == grid[i][j]: return False return True . &#45796;&#52264;&#50896; &#47532;&#49828;&#53944; . 일반적으로 $n$개의 첨자 사용 | . m[i][j][k] . .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/02/18/(1).html",
            "relUrl": "/python/2022/02/18/(1).html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "test",
            "content": "import pandas as pd import altair as alt . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . selection = alt.selection_single(); alt.Chart(cars).mark_circle().add_selection( selection ).encode( x=&#39;Horsepower:Q&#39;, y=&#39;Miles_per_Gallon:Q&#39;, color=alt.condition(selection, &#39;Cylinders:O&#39;, alt.value(&#39;grey&#39;)), opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1)) ) . def plot(selection): return alt.Chart(cars).mark_circle().add_selection( selection ).encode( x=&#39;Horsepower:Q&#39;, y=&#39;Miles_per_Gallon:Q&#39;, color=alt.condition(selection, &#39;Cylinders:O&#39;, alt.value(&#39;grey&#39;)), opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1)) ).properties( width=240, height=180 ) . alt.hconcat( plot(alt.selection_single()).properties(title=&#39;Single (Click)&#39;), plot(alt.selection_multi()).properties(title=&#39;Multi (Shift-Click)&#39;), plot(alt.selection_interval()).properties(title=&#39;Interval (Drag)&#39;) ) . alt.hconcat( plot(alt.selection_single(on=&#39;mouseover&#39;)).properties(title=&#39;Single (Mouseover)&#39;), plot(alt.selection_multi(on=&#39;mouseover&#39;)).properties(title=&#39;Multi (Shift-Mouseover)&#39;) ) . selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_24452/3629722415.py in &lt;module&gt; 4 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], 5 init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, -&gt; 6 bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} 7 ) 8 NameError: name &#39;genres&#39; is not defined .",
            "url": "https://pinkocto.github.io/BP2022/python/2022/01/04/test.html",
            "relUrl": "/python/2022/01/04/test.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pinkocto.github.io/BP2022/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pinkocto.github.io/BP2022/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pinkocto.github.io/BP2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pinkocto.github.io/BP2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}